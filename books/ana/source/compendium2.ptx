<?xml version="1.0" encoding="UTF-8" ?>
<!-- Generated by Pandoc using pretext.lua -->
<pretext>
<article>


	<figure>
	<image source="compendium.png"/>
		<caption>image</caption>
</figure>


	<section xml:id="introduction">
		<title>Introduction</title>


		<subsection xml:id="polynomial-form-and-evaluation-speed">
			<title>Polynomial form and evaluation speed</title>

			<p>
				To explore the relationship between computational time and polynomial form, consider three forms of the same polynomial <me>\begin{aligned}
{2}
f_1(x) &amp; = x^5 - 15x^4 + 85x^3 - 225x^2 + 274x -120 &amp;&amp; \quad\text{(normal)}\\
%
f_2(x) &amp; = (x - 1)(x - 2)(x - 3)(x - 4)(x - 5) &amp;&amp; \quad\text{(factored)}\\
%
f_3(x) &amp; = -120 + x(274 + x(-225 + x(85 + x(-15 + x)))) &amp;&amp; \quad\text{(nested)}\end{aligned}</me> We can evaluate each function at 1000 randomly generated points (to be used for all functions and all replicates) and, using an R package called microbenchmark, repeat this experiment 1000 times for each form. Results of this experiment are given in Figure <xref ref="fig::polys" />.
			</p>

			<figure>
	<image source="1_basics/polynomial_evaluation_plot.pdf"/>
				<caption>Median computing times (measured execution time in microseconds) are 607.1064 for <m>f_1</m>, 511.8887 for <m>f_2</m>, and 516.9337 for <m>f_3</m>.</caption>
</figure>

			<p>
				With respect to class, you might ask yourself why we have emphasized nesting, but not factoring, when it comes to working with polynomials.
			</p>

		</subsection>
	</section>

	<section xml:id="root-finding-and-fixed-point-problems">
		<title>Root-finding and fixed-point problems</title>


		<subsection xml:id="fixed-point-iteration">
			<title>Fixed-point iteration</title>

			<p>
				Consider the function <me>g(x) = 2^{-x} = e^{-\ln(2)x}</me> and the associated fixed-point problem <m>g(x) = x</m>. The result obtained by cobwebbing (graphically) is shown in the figure below, with the iterates marked in ticks on the bottom axis. Beyond <m>n=4</m> things get pretty crowded near the fixed point.
			</p>

			<figure>
	<image source="1_basics/fixedpoint.pdf"/>
				<caption>A fixed-point problem.</caption>
</figure>

			<p>
				As tabular output we have the first few steps as well. Note that <m>p_n</m> are approximates to the root <m>p</m>.
			</p>

<!-- div attr= class="center"-->
			<table>
				<caption></caption>
				<tabular>
				<row header="yes">
					<cell halign="center"><m>n</m></cell>
					<cell halign="center"><m>p_n</m></cell>
				</row>
				<row class="odd">
					<cell halign="center">0</cell>
					<cell halign="center">0.5000000</cell>
				</row>
				<row class="even">
					<cell halign="center">1</cell>
					<cell halign="center">0.7071068</cell>
				</row>
				<row class="odd">
					<cell halign="center">2</cell>
					<cell halign="center">0.6125473</cell>
				</row>
				<row class="even">
					<cell halign="center">3</cell>
					<cell halign="center">0.6540409</cell>
				</row>
				<row class="odd">
					<cell halign="center">4</cell>
					<cell halign="center">0.6354978</cell>
				</row>
				</tabular>
			</table><!--</div attr= class="center">-->

			<p>
				Now, for a given degree of accuracy, how many iterations do we actually need? Consider <me>2^{-x}=x\text{ on }\left[\frac{1}{3}, 1\right]</me> The bounds are given by <me>|p_n-p|\leq k^n \max(p_0-a, b - p_0)</me> using the initial guess, and by <me>|p_n-p|\leq\frac{k^n}{1-k}|p_1-p_0|</me> using the initial guess and first iteration. We will look at a few applications of the bounds to the problem above. First notice that <m>g'(x) = -\ln(2)2^{-x}</m> and <m>|g'(x)| \leq \ln(2)2^{-{1}/{3}} &lt; k = 0.551</m>, where <m>k = 0.551</m> is a bound on the magnitude of <m>g'(x)</m>.
			</p>

			<p>
				The worst possible initial guess would be at one of the endpoints, so we will start there (this maximizes the term <m>\max(p_0-a, b - p_0)</m>, which in this case we actually want to do in order to generate a conservative bound). Taking <m>D</m> as the desired accuracy (i.e., an accuracy within <m>10^{-D}</m>), this gives, <me>\begin{aligned}
k^n \max(p_0-a, b - p_0) &amp; &lt; 10^{-D}\\
(0.551)^n \left(\frac{2}{3}\right) &amp; &lt; 10^{-D}\\
(0.551)^n &amp; &lt; \left(\frac{3}{2}\right)10^{-D}\\
n \log(0.551) &amp; &lt; \log\left(\frac{3}{2}\right) - D\\
n &amp; &gt; \frac{\log\left(\frac{3}{2}\right) - D}{\log(0.551)}\end{aligned}</me> In the last line, the inequality has been reversed since we are dividing by a negative. With <m>D=4</m> this gives <m>n &gt; 14.77277</m> which requires <m>N=15</m> steps.
			</p>

			<p>
				For the second bound, we actually need <m>p_1</m> in addition to <m>p_0</m>. From <m>p_0 = \frac{2}{3}</m>, we have <m>p_1 = 2^{-1/3}</m> (so <m>|p_1-p_0| = |2^{-1/3} - \frac{1}{3}| ~\approx~ 0.4604</m>). Similarly, from <m>p_0=1</m>, we have <m>p_1 = \frac{1}{2}</m> (so <m>|p_1-p_0| = |\frac{1}{2} - 1| = 0.5</m>). We will use the second of these which is larger in value. <me>\begin{aligned}
\frac{k^n}{1-k}|p_1-p_0| &amp; &lt; 10^{-D}\\
\frac{(0.551)^n}{1-0.551}(0.5) &amp; &lt; 10^{-D}\\
(0.551)^n &amp; &lt; \left(\frac{1-0.551}{0.5}\right) 10^{-D}\\
n\log(0.551) &amp; &lt; \log\left(\frac{1-0.551}{0.5}\right) -D\\
n &amp; &gt; \frac{\log\left(\frac{1-0.551}{0.5}\right) -D}{\log(0.551)}\\\end{aligned}</me> For consistency, with <m>D=4</m> this gives <m>n &gt; 15.63357</m> which requires <m>N=16</m> steps. We have to do at least <m>16</m> steps to ensure we are within the bound, though we may satisfy this much more quickly. Notice that this is quite a bit more work than our bound for the Bisection method required.
			</p>

			<p>
				Consider the root-finding problem <m>f(x) = x^4-3x^2-3</m>. It has many different, but equivalent, fixed-point problems. <me>\begin{aligned}
g_1(x) &amp; = x- f(x) = x\\
g_2(x) &amp; = x+f(x) = x\\
g_3(x) &amp; = \sqrt{\frac{3}{x^2-3}} = x\\
g_4(x) &amp; = x - \frac{x^4-3x^2-3}{4x^3-6x}\end{aligned}</me>
			</p>

<!-- div attr= class="center"-->
			<figure>
	<image source="1_basics/bad.pdf"/>
				<caption>image</caption>
</figure><!--</div attr= class="center">-->

			<p>
				A better formulation is given by <me>g_5(x) = \sqrt[4]{3x^2+3}</me>
			</p>

<!-- div attr= class="center"-->
			<figure>
	<image source="1_basics/good.pdf"/>
				<caption>image</caption>
</figure><!--</div attr= class="center">-->

			<p>
				Consider the root-finding problem <m>3x^2-e^x = 0</m>. It has a few different, but equivalent, fixed-point problems. <me>\begin{aligned}
g_1(x) &amp; = \pm\sqrt{\dfrac{e^x}{3}} = x\\
g_2(x) &amp; = \ln(3x^2) = x\\
g_3(x) &amp; = 3x^2 - e^x + x = x\end{aligned}</me> Unfortunately these differ with regard to theory of fixed-point iterations. In particular, keep in mind
			</p>

			<dl>
				<dt>for existence:</dt>
					<dd>			<p>
				the trapping region <m>g\in[a,b]</m> for <m>x\in[a,b]</m>
			</p></dd>
				<dt>for uniqueness:</dt>
					<dd>			<p>
				the slope criterion <m>0&lt;|g'(x)|&lt;1</m>
			</p></dd>
			</dl>

			<p>
				The first function, <m>g_1(x)</m> is able to capture two of the three points of interest, with nicely trapped solutions and small derivatives on those intervals. It does fail to capture the third (rightmost) root, where the slope exceeds one (see Fig. <xref ref="fig::g1" />). Though our intervals cannot include <m>x=0</m>, <m>g_2(x)</m> works well for the largest root, where the derivative is small and the appropriate trapping region can be constructed (see Fig. <xref ref="fig::g2" />). The simplest formulation algebraically is <m>g_3(x)</m>, yet this has by far the worst characteristics with respect to the fixed-point theory for existence and uniqueness (see Fig. <xref ref="fig::g3" />). It is difficult to constrict suitable trapping regions and there are only narrow ranges for which the derivative is appropriately bounded in magnitude.
			</p>

			<figure>
	<image source="1_basics/g1.png"/>
				<caption>Two functions that give rise to fixed-point problems. Dotted corresponds to the negative value of the square root and the solid line corresponds to the positive value. Left: intersections of each with the black <m>1:1</m> line indicate fixed points. Right: values between <m>-1</m> and <m>1</m> indicate that a fixed point, if it exists, is unique.</caption>
</figure>

			<figure>
	<image source="1_basics/g1p.png"/>
				<caption>Two functions that give rise to fixed-point problems. Dotted corresponds to the negative value of the square root and the solid line corresponds to the positive value. Left: intersections of each with the black <m>1:1</m> line indicate fixed points. Right: values between <m>-1</m> and <m>1</m> indicate that a fixed point, if it exists, is unique.</caption>
</figure>

			<figure>
	<image source="1_basics/g2.png"/>
				<caption>Left: intersections with the black <m>1:1</m> line indicate fixed points. Right: values between <m>-1</m> and <m>1</m> indicate that a fixed point, if it exists, is unique. Notice that the smaller the value of the fixed point, the steeper the function <m>g_2(x)</m>.</caption>
</figure>

			<figure>
	<image source="1_basics/g2p.png"/>
				<caption>Left: intersections with the black <m>1:1</m> line indicate fixed points. Right: values between <m>-1</m> and <m>1</m> indicate that a fixed point, if it exists, is unique. Notice that the smaller the value of the fixed point, the steeper the function <m>g_2(x)</m>.</caption>
</figure>

			<figure>
	<image source="1_basics/g3.png"/>
				<caption>Left: intersections with the black <m>1:1</m> line indicate fixed points. Right: values between <m>-1</m> and <m>1</m> indicate that a fixed point, if it exists, is unique.</caption>
</figure>

			<figure>
	<image source="1_basics/g3p.png"/>
				<caption>Left: intersections with the black <m>1:1</m> line indicate fixed points. Right: values between <m>-1</m> and <m>1</m> indicate that a fixed point, if it exists, is unique.</caption>
</figure>

			<p>
				Consider the fixed-point problems <m>\sin(x) = x</m> (below, left) and <m>\cos(x) = x</m> (below, right).
			</p>

			<figure>
	<image source="1_basics/fixpt_trig.pdf"/>
				<caption>A trigonometric fixed-point problem.</caption>
</figure>

			<p>
				A total of <m>20</m> iterations of each problem are shown in the color figure above. Time is indicated in two ways: on the graph early steps are in dark blue and fade to yellow. Along the axis earlier approximations are marked by tall (also dark blue) tick marks and later iterations are marked by shorter (also yellow) tick marks. The scaling of the above-axis tick marks is nothing quantitative or necessarily related to the values of the approximation itself, it is just meant to show the progression of the iterations.
			</p>

			<p>
				For two problems starting from <m>p_0=1</m>, we see the iterations approaching the fixed point visible on each panel of the graph. Solutions to the first fixed-point problem (left) converge very slowly, while those to the second (right) converge very quickly. Details of the approaches to the fixed points are interesting as well. Approximations converging to the true solution of <m>p=0</m> for the first problem decrease monotonically, while those converging to the true solution of the second problem oscillate.
			</p>


			<paragraphs xml:id="challenge">
				<title>Challenge</title>

				<p>
					For each of the above, state an associated rootfinding problem, an interval on which you could apply the bisection method, and the number of iterations it would take to approximate the root to an accuracy of <m>10^{-6}</m> with bisection.
				</p>

			</paragraphs>
		</subsection>
	</section>

	<section xml:id="interpolation-and-approximation">
		<title>Interpolation and approximation</title>


		<subsection xml:id="polynomial-interpolation">
			<title>Polynomial interpolation</title>


			<subsubsection xml:id="lagrange-interpolation">
				<title>Lagrange interpolation</title>

				<p>
					Below are <m>L_k(x) = L_{7, k}(x)</m> for <m>8</m> equally spaced points <m>x_0 = -1, \dots, x_7 = 1</m> on <m>[-1, 1]</m>.
				</p>

				<figure>
	<image source="2_interpolation/Li.pdf"/>
					<caption>image</caption>
</figure>

				<p>
					Considering the function <me>f(x) = x^3e^{-1.1x}\sin(x)</me> we show the approximation <me>P(x) = P_7(x) = \sum_{k=0}^7 f(x_k)L_k(x)</me> (along with the function) on the left and the error on the right.
				</p>

				<figure>
	<image source="2_interpolation/fP.pdf"/>
					<caption>image</caption>
</figure>

			</subsubsection>

			<subsubsection xml:id="cubic-spline-interpolation">
				<title>Cubic spline interpolation</title>

				<dl>
					<dt>Note:</dt>
						<dd>				<p>
					Before you begin, please note that there is a typo in the clamped spline example in the book (Example 2, page 148, eighth edition), the correct solution will be given below.
				</p></dd>
					<dt>Intro:</dt>
						<dd>				<p>
					When computing a cubic spline with <m>n+1</m> grid points, keep in mind that we have <m>n</m> intervals, <m>n</m> spline segments, and an <m>(n+1)\times (n+1)</m> matrix. For example, if the data is known at the grid points <m>x_0, x_1, x_2, x_3, x_4</m> we have <m>5</m> points, <m>4</m> intervals, <m>4</m> spline segments, and a <m>5\times5</m> matrix. The spacings between grid points, defined <m>h_j = x_{j+1} - x_j</m>, are used below.
				</p></dd>
					<dt>General approach:</dt>
						<dd>				<p>
					We can verify the conditions of the spline by definition, but ultimately the way to implement this, even for small datasets, is to use matrix algebra. Let’s use the vector <m>\mathbf{r}</m> (as in <m>\textbf{A}\textbf{x} = \textbf{r}</m>, rather than <m>\textbf{A}\textbf{x} = \textbf{b}</m>) for the right-hand side to avoid likely confusion between entries of the right-hand side vector and the coefficients <m>b_j</m> of the splines. The first and last rows of <m>\mathbf{A}</m> and <m>\mathbf{r}</m> depend on the choice of boundary conditions, but the interior rows are illustrated below. On the interior rows of the main diagonal, <m>\mathbf{A}</m> has entries <me>2(h_0+h_1), 2(h_1 + h_2), \cdots, 2(h_{n-3} + h_{n-2}), 2(h_{n-2} + h_{n-1})</me> On the interior rows of the sub-diagonal (below main), <m>\mathbf{A}</m> has entries <me>h_0, h_1, \cdots, h_{n-3}, h_{n-2}</me> On the interior rows of the super-diagonal (above main), <m>\mathbf{A}</m> has entries <me>h_1, h_2, \cdots, h_{n-2}, h_{n-1}</me>
				</p>

				<p>
					<me>\begin{aligned}
\mathbf{A} &amp; = \setcounter{MaxMatrixCols}{20}
\begin{bmatrix}
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
h_0 &amp; 2(h_0+h_1) &amp; h_1 &amp; 0 &amp; \cdot &amp; \cdot &amp; \cdot&amp; \cdot &amp; \cdot &amp;\cdot &amp; 0\\
\cdot &amp; \ddots &amp; \ddots &amp; \ddots &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
0 &amp; \cdot &amp; \cdot &amp; 0 &amp; h_{j-1} &amp; 2(h_{j-1}+h_{j}) &amp; h_{j} &amp; 0 &amp; \cdot &amp; \cdot &amp; 0\\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
0 &amp; \cdot &amp; \cdot &amp; \cdot&amp; \cdot &amp; \cdot &amp;\cdot &amp; 0 &amp; h_{n-2} &amp; 2(h_{n-2}+h_{n-1}) &amp; h_{n-1}\\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
\end{bmatrix}\\
\mathbf{x} &amp; = 
\left[\begin{array}{c}
c_0 \\
c_1 \\
\vdots \\
c_{n-1} \\
c_n \\
\end{array}\right]\\
\mathbf{r} &amp; = 
\left[\begin{array}{c}
\cdots \\
3\left(\dfrac{a_2-a_1}{h_1} - \dfrac{a_1-a_0}{h_0}\right) \\
\vdots \\
3\left(\dfrac{a_n-a_{n-1}}{h_{n-1}} - \dfrac{a_{n-1}-a_{n-2}}{h_{n-2}}\right) \\
\cdots \\
\end{array}\right]\end{aligned}</me>
				</p></dd>
					<dt>Natural BCs:</dt>
						<dd>				<p>
					The interior rows of <m>\mathbf{A}</m> are as described above, but the first row begins with <m>1, 0, \dots</m> and the last row ends with <m>\dots, 0, 1</m>. To satisfy the boundary condition, the first and last rows of of <m>\mathbf{r}</m> are exactly <m>0</m>. <me>\begin{aligned}
\mathbf{A} &amp; = \setcounter{MaxMatrixCols}{20}
\begin{bmatrix}
1 &amp; 0 &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\vdots &amp; \vdots \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; \cdot \\
\cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp; \cdot &amp;0 &amp; 1 \\
\end{bmatrix}\\
\mathbf{r} &amp; = 
\left[\begin{array}{c}
0 \\
\vdots \\
\vdots \\
\vdots\\
0 \\
\end{array}\right]\end{aligned}</me>
				</p></dd>
					<dt>Clamped BCs:</dt>
						<dd>				<p>
					The interior rows of <m>\mathbf{A}</m> are as described above, but the first row begins with <m>2h_0, h_0, 0, \dots</m> and the last row ends with <m>\dots, 0, h_{n-1}, 2h_{n-1}</m>. To satisfy the boundary condition, the first and last rows of of <m>\mathbf{r}</m> are exactly <m>3\left(\dfrac{a_1-a_0}{h_0} -f'(a)\right)</m> and <m>3\left(f'(b) - \dfrac{a_n-a_{n-1}}{h_{n-1}}\right)</m>, respectively.
				</p>

				<p>
					<me>\begin{aligned}
\mathbf{A} &amp; = %\setcounter{MaxMatrixCols}{20}
\begin{bmatrix}
2h_0 &amp; h_0 &amp; 0 &amp;\cdot &amp; \cdot &amp; \cdot &amp;\cdot &amp; 0 \\
\cdot &amp; \ddots &amp; \ddots &amp; \ddots &amp; \cdots &amp; \cdots &amp;\cdots &amp; \vdots \\
%\vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\vdots &amp; \vdots \\
\vdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \ddots &amp; \ddots &amp;\ddots &amp; \cdot \\
\cdot &amp; \cdots &amp; \cdot &amp; \cdot &amp; \cdot &amp; 0 &amp;h_{n-1} &amp; 2h_{n-1} \\
\end{bmatrix}\\
\mathbf{r} &amp; = 
\left[\begin{array}{c}
3\left(\dfrac{a_1-a_0}{h_0} -f'(a)\right) \\
\vdots \\
\vdots \\
\vdots\\
3\left(f'(b) - \dfrac{a_n-a_{n-1}}{h_{n-1}}\right) \\
\end{array}\right]\end{aligned}</me>
				</p></dd>
					<dt>Parameterizing the spline:</dt>
						<dd>				<p>
					Once the <m>c_j</m>’s have been solved, the <m>b_j</m>’s and <m>d_j</m>’s can be specified <em>in reverse order</em> from <m>j=n-1,n-2,\dots,0</m> (see Alg. 3.4 (Step 6) or Alg. 3.5 (Step 7)). <me>\begin{aligned}
b_j &amp; = \dfrac{a_{j+1} - a_j}{h_j} - \dfrac{h_j(c_{j+1} + 2c_j)}{3}\\
d_j &amp; = \dfrac{c_{j+1} - c_j}{3h_j}\end{aligned}</me> Notice that <m>c_n</m> (from <m>\mathbf{x}</m>) is used in the calculation of <m>d_{n-1}</m>, but is never actually used in a spline. Regardless of the boundary condition, the calculated coefficients will appear in the spline of the form <me>S(t) = \left\{\begin{alignedat}{2}
&amp;\phantom{a_j + b_j(x-x_j) + } \cdots \phantom{(x-x_j)^2 + d_j(x-x_j)^3}, &amp;&amp;\cdots\\
S_j(t) &amp;= a_j + b_j(x-x_j) + c_j(x-x_j)^2 + d_j(x-x_j)^3, \quad &amp;&amp;x_j\leq x &lt; x_{j+1}\\
&amp;\phantom{a_j + b_j(x-x_j) + } \cdots \phantom{(x-x_j)^2 + d_j(x-x_j)^3}, &amp;&amp;\cdots\\
\end{alignedat}\right.</me> Note that <m>x_0=a</m> and <m>x_n=b</m>.
				</p></dd>
					<dt>Keep reading:</dt>
						<dd>				<p>
					(next page, please)
				</p></dd>
					<dt>Example</dt>
						<dd>				<p>
					Consider the data <m>x_0 = 0, x_1=1, x_2, =2, x_3=3</m> and <m>f(x) = e^x</m>.
				</p>

				<dl>
					<dt>A natural spline:</dt>
						<dd>				<p>
					See Example 1 on page 143.
				</p>

<!-- div attr= class="center"-->
				<table>
					<caption></caption>
					<tabular>
					<row header="yes">
						<cell halign="center"><m>j</m></cell>
						<cell halign="center"><m>a_j</m></cell>
						<cell halign="center"><m>b_j</m></cell>
						<cell halign="center"><m>c_j</m></cell>
						<cell halign="center"><m>d_j</m></cell>
					</row>
					<row class="odd">
						<cell halign="center">0</cell>
						<cell halign="center">1.000000</cell>
						<cell halign="center">1.465998</cell>
						<cell halign="center">0.0000000</cell>
						<cell halign="center">0.2522842</cell>
					</row>
					<row class="even">
						<cell halign="center">1</cell>
						<cell halign="center">2.718282</cell>
						<cell halign="center">2.222850</cell>
						<cell halign="center">0.7568526</cell>
						<cell halign="center">1.6910714</cell>
					</row>
					<row class="odd">
						<cell halign="center">2</cell>
						<cell halign="center">7.389056</cell>
						<cell halign="center">8.809770</cell>
						<cell halign="center">5.8300668</cell>
						<cell halign="center">-1.9433556</cell>
					</row>
					</tabular>
				</table>

				<figure>
	<image source="2_interpolation/natural.pdf"/>
					<caption>image</caption>
</figure><!--</div attr= class="center">--></dd>
					<dt>A clamped spline:</dt>
						<dd>				<p>
					See Example 2 on page 148.
				</p>

<!-- div attr= class="center"-->
				<table>
					<caption></caption>
					<tabular>
					<row header="yes">
						<cell halign="center"><m>j</m></cell>
						<cell halign="center"><m>a_j</m></cell>
						<cell halign="center"><m>b_j</m></cell>
						<cell halign="center"><m>c_j</m></cell>
						<cell halign="center"><m>d_j</m></cell>
					</row>
					<row class="odd">
						<cell halign="center">0</cell>
						<cell halign="center">1.000000</cell>
						<cell halign="center">1.000000</cell>
						<cell halign="center">0.4446825</cell>
						<cell halign="center">0.2735993</cell>
					</row>
					<row class="even">
						<cell halign="center">1</cell>
						<cell halign="center">2.718282</cell>
						<cell halign="center">2.710163</cell>
						<cell halign="center">1.2654805</cell>
						<cell halign="center">0.6951308</cell>
					</row>
					<row class="odd">
						<cell halign="center">2</cell>
						<cell halign="center">7.389056</cell>
						<cell halign="center">7.326516</cell>
						<cell halign="center">3.3508729</cell>
						<cell halign="center">2.0190916</cell>
					</row>
					</tabular>
				</table>

				<figure>
	<image source="2_interpolation/clamped.pdf"/>
					<caption>image</caption>
</figure><!--</div attr= class="center">--></dd>
				</dl></dd>
				</dl>

			</subsubsection>
		</subsection>

		<subsection xml:id="discrete-least-squares-approximation">
			<title>Discrete least squares approximation</title>

			<p>
				Given the data in the vehicle speed problems (see sections 3.3, 3.4 homework), we can plot a few functions representing the position data (see Figure <xref ref="fig::speed_plots" />). The Hermite (black) and spline (red) interpolations exactly pass through the position data points. On the other hand, the linear approximation (blue) does not exactly pass through a single of these points.
			</p>

			<figure>
	<image source="3_approximation/speed1.pdf"/>
				<caption>Data and interpolating and approximating polynomials.</caption>
</figure>

			<figure>
	<image source="3_approximation/speed2.pdf"/>
				<caption>Speed data and predicted speed by interpolating and approximating polynomials (<term>left:</term> feet per second and <term>right:</term> miles per hour).</caption>
</figure>

			<figure>
	<image source="3_approximation/speed2.pdf"/>
				<caption>Speed data and predicted speed by interpolating and approximating polynomials (<term>left:</term> feet per second and <term>right:</term> miles per hour).</caption>
</figure>

			<p>
				There are a variety of factors to weigh when choosing a method for representing data by a function.
			</p>

		</subsection>

		<subsection xml:id="discrete-case-i.e.-8.1-pg.-486" class="unnumbered">
			<title>Discrete case (i.e., 8.1, pg. 486)</title>

			<p>
				Consider the last term <me>\sum_{i=1}^m\Big(P_n(x_i)\Big)^2 = \sum_{i=1}^m \left(\sum_{j=0}^n a_j x_i^j\right)^2</me> in the definition of the error <m>E = E_2(a_0, a_1, \dots, a_n)</m> for discrete least squares approximation. We have the following (though I do not find this particularly illuminating and instead prefer to differentiate before addressing the sums), where it might initially be helpful to remember that <m>x_i^0 = 1</m> and <m>x_i^1 = x_i</m> (e.g., <m>a_0 x_i^0 = a_0\cdot1 =a_0</m> and <m>a_1 x_i^1 = a_1x_i</m>), <me>\begin{aligned}
 \sum_{i=1}^m \left(\sum_{j=0}^n a_j x_i^j\right)^2 &amp; = \sum_{i=1}^m (a_0 + a_1x_i+ \dots + a_n x_i^n)(a_0 + a_1x_i + \dots + a_n x_i^n)\\
 &amp; = \sum_{i=1}^m \Big[a_0 (a_0 + \dots + a_n x_i^n) + \dots + a_jx_i^j(a_0 + \dots + a_n x_i^n) + \dots + a_n x_i^n(a_0 + \dots + a_n x_i^n)\Big]\\
 &amp; = \sum_{i=1}^m \sum_{j=0}^n a_jx_i^j(a_0 + a_1x_i + \dots + a_n x_i^n)\\
 &amp; = \sum_{j=0}^n a_j \sum_{i=1}^m x_i^j(a_0 + a_1x_i + \dots + a_n x_i^n)\\
 &amp; = \sum_{j=0}^n a_j \sum_{i=1}^m x_i^j \sum_{k=0}^n a_k x_i^k\\
 &amp; = \sum_{j=0}^n a_j \sum_{i=1}^m \sum_{k=0}^n a_k x_i^{j+k}\\
% &amp; = \sum_{j=0}^n a_j \sum_{k=0}^n a_k \sum_{i=1}^m x_i^{j+k}\\
% &amp; = \sum_{i=1}^m \sum_{j=0}^n a_jx_i^j \sum_{k=0}^n a_kx_i^k\\
 % &amp; = \sum_{i=1}^m a_0 a_0 + (a_0a_1 + a_1a_0)x_i + (a_0a_2+a_1a_1+a_2a_0)x_i^2 + \dots\\
% &amp; = \sum_{i=1}^m a_0 a_0 + (a_0a_1 + a_1a_0)x_i + (a_0a_2+a_1a_1+a_2a_0)x_i^2 + \dots\\
 &amp; = \sum_{j=0}^n\sum_{k=0}^n a_ja_k\Big(\sum_{i=1}^m x_i^{j+k}\Big)
 \end{aligned}</me> Our goal is the partial derivative of this term with respect to <m>a_j</m>. Notice the following, where key steps are moving differentiation under the sum, applying the product rule, and rearranging the sum, <me>\begin{aligned}
\dfrac{\partial}{\partial a_j} \left(\sum_{i=1}^m \left(\sum_{j=0}^n a_j x_i^j\right)^2 \right) &amp; = \dfrac{\partial}{\partial a_j}\Big(\sum_{i=1}^m (a_0 + a_1x_i + \dots + a_n x_i^n)(a_0 + a_1x_i + \dots + a_n x_i^n)\Big)\\
&amp; = \sum_{i=1}^m \dfrac{\partial}{\partial a_j}\Big((a_0 + a_1x_i + \dots + a_n x_i^n)(a_0 + a_1x_i + \dots + a_n x_i^n)\Big)\\
&amp; = \sum_{i=1}^m \left(x_i^j(a_0 + a_1x_i + \dots + a_n x_i^n) + (a_0 + a_1x_i + \dots + a_n x_i^n) x_i^j\right)\\
&amp; = \sum_{i=1}^m 2\left(x_i^j(a_0 + a_1x_i + \dots + a_n x_i^n)\right)\\
&amp; = \sum_{i=1}^m 2\left(x_i^j\sum_{k=0}^n(a_kx_i^k)\right)\\
&amp; = 2\sum_{k=0}^n a_k \sum_{i=1}^m x_i^{j+k}\\
%&amp; = \int_a^b 2x_i^j(a_0 + a_1x_i + \dots + a_n x_i^n)\,dx_i\\
%&amp; = 2\int_a^b x_i^j\sum_{k=0}^n a_k x_i^k\,dx_i\\
%&amp; = 2\int_a^b \sum_{k=0}^n a_k x_i^{j+k}\,dx_i\\
%&amp; = 2\sum_{k=0}^n a_k \int_a^b x_i^{j+k}\,dx_i\end{aligned}</me> Related to this, to find the least squares polynomial approximation to <m>\{(x_i, y_i)\}_{i=1}^m</m> by <m>P_n(x)~=~\sum\limits_{k=0}^n a_k x^k</m>, we solve <me>D^TD\vec{a} =D^T\vec{y}</me> for the vector of unknown coefficients <m>\vec{a}</m> where <me>D = \left[\begin{array}{cccc} (x_1)^0 &amp; (x_1)^1 &amp; \cdots &amp; (x_1)^n\\
(x_2)^0 &amp; (x_2)^1 &amp; \cdots &amp; (x_2)^n\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
(x_m)^0 &amp; (x_m)^1 &amp; \cdots &amp; (x_m)^n
\end{array}\right]</me>
			</p>

		</subsection>

		<subsection xml:id="continuous-case-i.e.-8.2-pg.-495" class="unnumbered">
			<title>Continuous case (i.e., 8.2, pg. 495)</title>

			<p>
				Consider the last term <me>\int_a^b \left(\sum_{k=0}^n a_k x^k\right)^2</me> in the definition of the error <m>E = E_2(a_0, a_1, \dots, a_n)</m> for least squares polynomial approximation. We have, <me>\begin{aligned}
\int_a^b \left(\sum_{k=0}^n a_k x^k\right)^2\,dx %&amp; = \int_a^b \left(\sum_{k=0}^n a_k x^k\right)^2\,dx\\
&amp; = \int_a^b (a_0 + a_1x + \dots + a_n x^n)(a_0 + a_1x + \dots + a_n x^n)\,dx\\\end{aligned}</me> Our goal is the partial derivative of this term with respect to <m>a_j</m>. Notice the following, where key steps are moving differentiation under the integral, applying the product rule, and rearranging the sum, <me>\begin{aligned}
\dfrac{\partial}{\partial a_j}\int_a^b \left(\sum_{k=0}^n a_k x^k\right)^2\,dx &amp; = \dfrac{\partial}{\partial a_j} \int_a^b (a_0 + a_1x + \dots + a_n x^n)(a_0 + a_1x + \dots + a_n x^n)\,dx\\
&amp; = \int_a^b \dfrac{\partial}{\partial a_j} \Big((a_0 + a_1x + \dots + a_n x^n)(a_0 + a_1x + \dots + a_n x^n)\Big)\,dx\\
&amp; = \int_a^b \left(x^j(a_0 + a_1x + \dots + a_n x^n) + (a_0 + a_1x + \dots + a_n x^n) x^j\right)\,dx\\
&amp; = \int_a^b 2x^j(a_0 + a_1x + \dots + a_n x^n)\,dx\\
&amp; = 2\int_a^b x^j\sum_{k=0}^n a_k x^k\,dx\\
&amp; = 2\int_a^b \sum_{k=0}^n a_k x^{j+k}\,dx\\
&amp; = 2\sum_{k=0}^n a_k \int_a^b x^{j+k}\,dx\end{aligned}</me>
			</p>

		</subsection>

		<subsection xml:id="least-squares-function-approximation">
			<title>Least squares function approximation</title>


			<subsubsection xml:id="legendre-polynomials">
				<title>Legendre polynomials</title>

				<p>
					The standard monomials for function approximation, shown as the familiar unlabeled gray curves in Figure <xref ref="fig::leg" />, are <m>\phi_i(x) = x^i</m> for <m>i=0, 1, \dots, n</m>. These lack the property of orthogonality that is achieved by the Legendre polynomials, also shown Figure <xref ref="fig::leg" />, but in black and labeled. The first few Legendre polynomials are
				</p>

				<figure>
	<image source="3_approximation/legendre_funs.pdf"/>
					<caption>In gray the intuitive monomials <m>\phi_i(x) = x^i</m> with the Legendre polynomials <m>P_i(x)</m> in black.</caption>
</figure>

				<p>
					<me>\begin{aligned}
P_0(x) &amp; = 1\\
P_1(x) &amp; = x\\
P_2(x) &amp; = x^2-\dfrac{1}{3}\\
P_3(x) &amp; = x^3-\dfrac{3}{5}x\\
 &amp; \dots\\
P_n(x) &amp; = (x-B_k)P_{n-1}(x)-C_kP_{x-2}(x)\end{aligned}</me>
				</p>

				<p>
					where <me>\begin{aligned}
B_k &amp; = \dfrac{\int_{-1}^1 x \cdot 1 \cdot [P_{k-1}(x)]^2\,dx}{\int_{-1}^1 1 \cdot [P_{k-1}(x)]^2\,dx}\\
C_k &amp; = \dfrac{\int_{-1}^1 x \cdot 1 \cdot [P_{k-1}(x) P_{k-2}(x)]\,dx}{\int_{-1}^1 1 \cdot [P_{k-2}(x)]^2\,dx}\end{aligned}</me>
				</p>

			</subsubsection>

			<subsubsection xml:id="chebyshev-polynomials">
				<title>Chebyshev polynomials</title>

				<p>
					The standard monomials for function approximation, shown as the familiar unlabeled gray curves in Figure <xref ref="fig::cheb" />, are <m>\phi_i(x) = x^i</m> for <m>i=0, 1, \dots, n</m>. These lack the property of orthogonality that is achieved by the Chebyshev polynomials, also shown Figure <xref ref="fig::cheb" />, but in black and labeled. The first few Chebyshev polynomials are
				</p>

				<figure>
	<image source="3_approximation/cheb_funs.pdf"/>
					<caption>In gray the intuitive monomials <m>\phi_i(x) = x^i</m> with the Chebyshev polynomials <m>T_i(x)</m> in black.</caption>
</figure>

				<p>
					<me>\begin{aligned}
T_0(x) &amp; = 1\\
T_1(x) &amp; = x\\
T_2(x) &amp; = 2x^2-1\\
T_3(x) &amp; = 4x^3-3x\\
 &amp; \dots\\
T_n(x) &amp; = 2xT_{n-1}(x) - T_{n-2}(x)\end{aligned}</me>
				</p>

				<p>
					Since the alternate definition of the Chebyshev polynomials is <m>T_n(x) = \cos(n\arccos(x))</m> for <m>n\geq0</m>, it should seem reasonable that these functions are bounded in magnitude by one, as illustrated in Figure <xref ref="fig::cheb" />. The monic Chebyshev polynomials (not illustrated, but handy for Chebyshev economization of power series), with leading coefficient <m>1</m>, are defined by <me>\tilde T_n(x) = \dfrac{1}{2^{n-1}}T_n(x)</me> For these polynomials the extrema, which occur <em>at</em> the same points, are those of <m>T_n(x)</m> but reduced in value by a factor of <m>\dfrac{1}{2^{n-1}}</m>, that is, the extrema are, for <m>k=0, 1, \dots, n</m> <me>\tilde T'_n(\bar x_k) = \dfrac{(-1)^k}{2^{n-1}}</me>
				</p>

			</subsubsection>

			<subsubsection xml:id="chebyshev-points-and-lagrange-nodes">
				<title>Chebyshev points and Lagrange nodes</title>

				<p>
					By a theorem, we have that the optimal choice of nodes for polynomial approximation by a degree <m>n</m> polynomial is given by the zeros of of the <m>(n+1)^\text{st}</m> Chebyshev polynomial <m>T_{n+1}(x)</m>. For example, as given in Figure <xref ref="fig::cheb_pts" />, we take the approximation of <m>f(x) = e^x</m> on <m>[0, 1]</m> with three points. We expect to to take, <m>\tilde x_0 = 0</m>, <m>\tilde x_1 = 0.5</m>, and <m>\tilde x_2 = 1.0</m>, but achieve better performance with Chebyshev nodes <m>\bar x_i</m>. The nodes for interpolation are given by the zeros of the Chebyshev polynomial <m>T_3(x)</m>, which in turn are given by <m>\bar x_k = \cos\left(\dfrac{2k-1}{2n}\pi\right)</m> for <m>k=1, 2, 3</m>. <me>\begin{aligned}
\bar x_1 &amp;= \cos\left(\dfrac{2(1)-1}{2(3)}\cdot\pi\right) = \cos\left(\dfrac{1\cdot\pi}{6}\right)= \dfrac{\sqrt{3}}{2} ~\approx~ 0.866\\
\bar x_2 &amp;= \cos\left(\dfrac{2(2)-1}{2(3)}\cdot\pi\right) = \cos\left(\dfrac{3\cdot\pi}{6}\right)= 0\\
\bar x_3 &amp;= \cos\left(\dfrac{2(3)-1}{2(3)}\cdot\pi\right) = \cos\left(\dfrac{5\cdot\pi}{6}\right)= -\dfrac{\sqrt{3}}{2} ~\approx~ -0.866 \end{aligned}</me>
				</p>

				<p>
					From <m>\tilde x_i \in [a,b]</m> we can compute <m>\bar x_i \in [-1, 1]</m> by <me>x_i = \dfrac{2\tilde x_i - a - b}{b-a}</me> and in the reverse we can use <me>\tilde x_i = \dfrac{1}{2}\left((b-a)\bar x_i + a + b\right)</me> As an example,
				</p>

<!-- div attr= class="center"-->
				<table>
					<caption></caption>
					<tabular>
					<row header="yes">
						<cell halign="center"><m>i</m></cell>
						<cell halign="left"><m>\phantom{-}\bar x_i</m></cell>
						<cell halign="left"><m>\tilde x_i</m></cell>
					</row>
					<row class="odd">
						<cell halign="center"></cell>
						<cell halign="left"></cell>
						<cell halign="left"></cell>
					</row>
					<row class="even">
						<cell halign="center">1</cell>
						<cell halign="left"><m>\phantom{-}0.866</m></cell>
						<cell halign="left">0.933</cell>
					</row>
					<row class="odd">
						<cell halign="center">2</cell>
						<cell halign="left"><m>\phantom{-}0.0</m></cell>
						<cell halign="left">0.500</cell>
					</row>
					<row class="even">
						<cell halign="center">3</cell>
						<cell halign="left"><m>-0.866</m></cell>
						<cell halign="left">0.067</cell>
					</row>
					</tabular>
				</table><!--</div attr= class="center">-->

				<p>
					The näively chosen nodes that include the endpoints (red) are illustrated in the left-hand panel of Figure <xref ref="fig::cheb_pts" />, and as shown give a reasonable approximate. Yet, the approximation is improved by use of the Chebyshev points (blue).
				</p>

				<figure>
	<image source="3_approximation/cheb_pts_lagr.pdf"/>
					<caption>Lagrange interpolation of <m>f(x) = e^x</m> on <m>[0,1]</m>, but with Chebyshev points instead of equispaced points.</caption>
</figure>

				<figure>
	<image source="3_approximation/cheb_pts_lagr_err.pdf"/>
					<caption>Lagrange interpolation of <m>f(x) = e^x</m> on <m>[0,1]</m>, but with Chebyshev points instead of equispaced points.</caption>
</figure>

				<p>
					Notice, from Figure <xref ref="fig::cheb_zeros" />, that the zeros accumulate or cluster near the boundary of the interval. This helps to <sq>clamp down</sq> the polynomial interpolation near the boundary.
				</p>

				<figure>
	<image source="3_approximation/cheb_zeros.pdf"/>
					<caption>Location of zeros to <m>T_n(x)</m> in the interval <m>[0, 1]</m> for Chebyshev functions of orders from <m>n=2, 3, \dots, 15</m>.</caption>
</figure>

			</subsubsection>

			<subsubsection xml:id="chebyshev-economization">
				<title>Chebyshev economization</title>

				<p>
					As we will soon see, the <sq>best</sq> reduced order polynomial approximation <m>P_{n-1}</m> to a polynomial <m>P_n(x)</m> is given by <me>P_{n-1}(x) = P_{n}(x) - a_n\tilde T_n(x)</me> where <m>\tilde T_n(x)</m> is the <m>n^\text{th}</m> order monic Chebyshev polynomial and <m>a_n</m> is the coefficient of the highest order term in <m>P_n(x)</m>. To economize the polynomial approximation to <m>f(x) = e^x</m> given by a <m>4^\text{th}</m> order Maclaurin polynomial, we can subtract <m>\dfrac{1}{24}\tilde T_4(x)</m> from the original <m>P_4(x)</m> (note that <m>a_4=\dfrac{1}{24}</m> in the original polynomial representation). This alters coefficients of all powers of <m>x</m> of lower orders that are present in <m>T_4(x)</m> and eliminates the <m>4^\text{th}</m> order term entirely. Notice in Figure <xref ref="fig::cheb_ord" /> that the maximum error for the third order function is only slightly worse for <m>x=1.0</m> than for the fourth order function.
				</p>

				<figure>
	<image source="3_approximation/cheb_order_lagr.pdf"/>
					<caption>Left: Full- and reduced-order approximations of <m>f(x) = e^x</m> on <m>[0, 1]</m> by <m>P_3(x)~=~P_4(x)~-~a_4 \tilde T_4(x)</m>. Right: Error in approximations of <m>f(x) = e^x</m> by full- and reduced-order polynomials.</caption>
</figure>

				<figure>
	<image source="3_approximation/cheb_order_lagr_err.pdf"/>
					<caption>Left: Full- and reduced-order approximations of <m>f(x) = e^x</m> on <m>[0, 1]</m> by <m>P_3(x)~=~P_4(x)~-~a_4 \tilde T_4(x)</m>. Right: Error in approximations of <m>f(x) = e^x</m> by full- and reduced-order polynomials.</caption>
</figure>

			</subsubsection>
		</subsection>
	</section>

	<section xml:id="numerical-calculus">
		<title>Numerical calculus</title>


		<subsection xml:id="numerical-differentiation">
			<title>Numerical differentiation</title>

			<p>
				Consider the function <m>f(x) = e^x</m> at a point <m>x_0=1</m> and approximations to its derivative <m>f'(x_0)~\approx~ 2.71281</m>. Approximations can be made by to somewhat straightforward approximations: the first-order forward difference and the second-order forward and centered differences. The error in these approximations is illustrated below, with rounding to simulate calculations by single-precision and double-precision numbers.
			</p>

			<figure>
	<image source="4_differentiation/step_size_8.pdf"/>
				<caption>Points indicate calculated <m>\log_{10}\left(\text{abs. error}\right)</m> for first-order forward (black), second-order forward (red), and centered (gray) difference approximations. Line illustrates predicted reduction in error as step size shrinks. <term>Left:</term> 8 digit calculation (simulated single-precision), <term>Right:</term> 16 digit calculation (simulated double-precision)</caption>
</figure>

			<figure>
	<image source="4_differentiation/step_size_16.pdf"/>
				<caption>Points indicate calculated <m>\log_{10}\left(\text{abs. error}\right)</m> for first-order forward (black), second-order forward (red), and centered (gray) difference approximations. Line illustrates predicted reduction in error as step size shrinks. <term>Left:</term> 8 digit calculation (simulated single-precision), <term>Right:</term> 16 digit calculation (simulated double-precision)</caption>
</figure>

			<figure>
	<image source="4_differentiation/step_size_roundoff_8.pdf"/>
				<caption>As in Figure <xref ref="fig::plots" />, but with smaller step sizes included. Note that error <em>increases</em> due to roundoff for both methods, but especially for the centered method as the step size is reduced below <m>h = 0.01</m>.</caption>
</figure>

			<figure>
	<image source="4_differentiation/step_size_roundoff_16.pdf"/>
				<caption>As in Figure <xref ref="fig::plots" />, but with smaller step sizes included. Note that error <em>increases</em> due to roundoff for both methods, but especially for the centered method as the step size is reduced below <m>h = 0.01</m>.</caption>
</figure>

		</subsection>

		<subsection xml:id="numerical-integration-monte-carlo">
			<title>Numerical integration (Monte Carlo)</title>

			<p>
				<em>This section may be imperfect, but reflects the main idea.</em>
			</p>

			<p>
				Though not emphasized here, the techniques that follow are most useful in higher dimensional integrals. We can integrate <m>f(x) = x^2</m> using the Fundamental Theorem of Calculus, <me>\int_0^2 x^2\,dx = \left.\left(\dfrac{x^3}{3}\right)\right|_0^2 = \dfrac{2^3}{3} - \dfrac{0^3}{3} = \dfrac{8}{3}</me> From the Mean Value Theorem for Integrals we have that <me>\hat{f} = \dfrac{1}{2-0}\int_0^2 x^2\,dx = \dfrac{1}{2}\cdot \dfrac{8}{3} = \dfrac{4}{3}</me> The shaded region on the left panel (Fig <xref ref="fig::calc" />) shows the area under the curve. The area on the right is the same size, but bounded by a constant function whose value is <m>\hat{f}</m>.
			</p>

			<figure>
	<image source="5_integration/intro.pdf"/>
				<caption>Left: Traditional calculus interpretation of the area under the curve. Right: Average value of a function defined by equal area.</caption>
</figure>

			<p>
				One scheme of Monte Carlo integration uses function values at randomly chosen points to calculate areas which are then averaged. Here <me>\int_a^b f(x)\,dx \approx \dfrac{1}{N}\sum_{i=1}^{N} \underbrace{(b-a)f(x_i)}_\text{\(i\)th area} = (b-a)\underbrace{\dfrac{1}{N}\sum_{i=1}^{N} f(x_i)}_\text{average height}</me> Fig <xref ref="fig::average" /> (left panel) shows 5 of the 100 random rectangle regions. The shaded area on the right panel corresponds to the region whose height is the average of the random function values. Notice that this height very closely compares with the average value of the function, thus the areas of the regions are similar.
			</p>

			<figure>
	<image source="5_integration/rectangles.pdf"/>
				<caption>Dashed line indicates <m>\hat{f}</m> from traditional calculus. Left: 5 sample rectangles (shaded). Right: Average area and function height by random sampling.</caption>
</figure>

			<p>
				An alternate scheme (Fig <xref ref="fig::compare" />, left) plots points in the plane and considers the fraction of points that fall below the curve (i.e., ‘the acceptance region‘). This fraction is multiplied by the total area of the region (easier to calculate since the shape is likely square). Though this is easier to program than the notation might suggest, consider the indicator function <me>I(i) = \begin{cases}1, \quad y_i &lt; f(x_i)\\0, \quad\text{otherwise}\end{cases}</me> and take <me>\int_a^b f(x)\,dx \approx(f_{max}-f_{min})(b-a)\dfrac{1}{N} \sum_{i=1}^N I(i)</me>
			</p>

			<figure>
	<image source="5_integration/rejection.pdf"/>
				<caption>Left: Accepted points fall below the graph of the function. Right: Comparison of rectangle method and rejection method for ten different trials of each.</caption>
</figure>

			<figure>
	<image source="5_integration/Ns.pdf"/>
				<caption>Left: Accepted points fall below the graph of the function. Right: Comparison of rectangle method and rejection method for ten different trials of each.</caption>
</figure>
		</subsection>

	</section>



</article>
</pretext>
