<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2022-01-06T11:08:13-06:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Mathematical expectation of discrete random variables</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    useLabelIds: true,
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl', 'sfrac']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore",
    processHtmlClass: "has_am",
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
/* support for the sfrac command in MathJax (Beveled fraction) */
  startup: {
    ready() {
      //
      // Creating a simple "sfrac" package on-the-fly
      //
      const Configuration = MathJax._.input.tex.Configuration.Configuration;
      const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
      
      new CommandMap('sfrac', {
        sfrac: 'SFrac'
        }, {
        SFrac(parser, name) {
        const num = parser.ParseArg(name);
        const den = parser.ParseArg(name);
        const frac = parser.create('node', 'mfrac', [num, den], {bevelled: true});
        parser.Push(frac);
        }
      });
      //
      // Create the package for the overridden macros
      //
      Configuration.create('sfrac', {
        handler: {macro: ['sfrac']}
      });
      
    MathJax.startup.defaultReady();
    }
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg">miniversion=0.6</script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="pretext-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" id="latex-macros" class="hidden-content" style="display:none">\(
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://www.math.uco.edu" target="_blank"><img src="external/images/cover.png" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="ana.html"><span class="title">Applied Numerical Analysis:</span> <span class="subtitle"></span></a></h1>
<p class="byline">Sean M. Laverty</p>
</div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap-approximation.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap-approximation.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="section-probability-densities.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap-approximation.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap-approximation.html" title="Up">Up</a><a class="next-button button toolbar-item" href="section-probability-densities.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="front-colophon.html" data-scroll="front-colophon">Colophon</a></li>
<li><a href="author-bio-SML.html" data-scroll="author-bio-SML">Author Biography</a></li>
<li><a href="dedication.html" data-scroll="dedication">Dedication</a></li>
<li><a href="acknowledgement.html" data-scroll="acknowledgement">Acknowledgements</a></li>
<li><a href="preface.html" data-scroll="preface">Preface</a></li>
<li><a href="contributors.html" data-scroll="contributors">Contributors to the 0\(^\mathrm{th}\) Edition</a></li>
</ul>
</li>
<li class="link">
<a href="chap-basics.html" data-scroll="chap-basics"><span class="codenumber">1</span> <span class="title">Counting</span></a><ul><li><a href="section-R-intro.html" data-scroll="section-R-intro">Introduction to R programming</a></li></ul>
</li>
<li class="link">
<a href="chap-rootfinding.html" data-scroll="chap-rootfinding"><span class="codenumber">2</span> <span class="title">Root-finding and fixed-point problems</span></a><ul><li><a href="section-continuous-expectation.html" data-scroll="section-continuous-expectation">Expectation of continuous random variables</a></li></ul>
</li>
<li class="link">
<a href="chap-interpolation.html" data-scroll="chap-interpolation"><span class="codenumber">3</span> <span class="title">Interpolation</span></a><ul>
<li><a href="sec-multi-cont.html" data-scroll="sec-multi-cont">Multivariate continuous densities</a></li>
<li><a href="section-special-densities.html" data-scroll="section-special-densities">Special probability densities</a></li>
</ul>
</li>
<li class="link">
<a href="chap-approximation.html" data-scroll="chap-approximation"><span class="codenumber">4</span> <span class="title">Approximation</span></a><ul>
<li><a href="section-discrete-expectation.html" data-scroll="section-discrete-expectation" class="active">Mathematical expectation of discrete random variables</a></li>
<li><a href="section-probability-densities.html" data-scroll="section-probability-densities">Probability densities</a></li>
</ul>
</li>
<li class="link">
<a href="chap-calculus.html" data-scroll="chap-calculus"><span class="codenumber">5</span> <span class="title">Numerical calculus</span></a><ul>
<li><a href="section-special-distributions.html" data-scroll="section-special-distributions">Special probability distributions</a></li>
<li><a href="section-probability-distributions.html" data-scroll="section-probability-distributions">Probability distributions</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-def.html" data-scroll="appendix-def"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thm.html" data-scroll="appendix-thm"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-ex.html" data-scroll="appendix-ex"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-gfdl.html" data-scroll="appendix-gfdl"><span class="codenumber">D</span> <span class="title">GNU Free Documentation License</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
<li class="link"><a href="back-colophon.html" data-scroll="back-colophon"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="section-discrete-expectation"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">4.1</span> <span class="title">Mathematical expectation of discrete random variables</span>
</h2>
<section class="introduction" id="introduction-14"><p id="p-149">This is the start of Chapter 4 in Freund's Mathematical Statistics. In the first pass we will study the major topics of this chapter with a focus on those applying to discrete random variables.</p></section><section class="subsection" id="sub-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.1.1</span> <span class="title">Expected value</span>
</h3>
<p id="p-150">See section 4.1 - 4.2. Recommended problems: (pg 136) 7, 9, 10, 11, (pg 161) 60</p>
<article class="definition definition-like" id="def-expected-value-4-1"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.1</span><span class="period">.</span><span class="space"> </span><span class="title">expected value.</span>
</h4>
<p id="p-151">If \(X\) is a discrete random variable and \(\displaystyle f(x)\) is the value of its probability distribution at \(X\text{,}\) the <dfn class="terminology">expected value</dfn> of \(X\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[X] = \sum_x x\cdot f(x)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-expected-value-function-random-variable-4-1"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.2</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of a function of a random variable.</span>
</h4>
<p id="p-152">If \(X\) is a discrete random variable and \(\displaystyle f(x)\) is the value of its probability distribution at \(X\text{,}\) the expected value of \(\displaystyle
g(X)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X)] = \sum_x g(x)\cdot f(x)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-2"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.3</span><span class="period">.</span><span class="space"> </span><span class="title">expectation of a linear function.</span>
</h4>
<p id="p-153">If \(\displaystyle a\) and \(\displaystyle b\) are constants, then \(\displaystyle E[aX +b] = aE[X]+b\text{.}\)</p></article><article class="corollary theorem-like" id="cor-4-1"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.1.4</span><span class="period">.</span>
</h4>
<p id="p-154">If a is a constant, then \(\displaystyle E[aX] = aE[X]\text{.}\)</p></article><article class="corollary theorem-like" id="cor-4-2"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.1.5</span><span class="period">.</span>
</h4>
<p id="p-155">If b is a constant, then \(\displaystyle E[b] = b\text{.}\)</p></article><article class="theorem theorem-like" id="thm-expected-value-joint-random-variables-4-4"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.6</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of joint random variables.</span>
</h4>
<p id="p-156">If \(X\) and \(Y\) are discrete random variables and \(\displaystyle f(x, y)\) is the value of their joint probability distribution at \(\displaystyle (x, y)\text{,}\) the expected value of \(\displaystyle g(X, Y)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X, Y)] = \sum_x \sum_y g(x, y)\cdot f(x,y)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-5"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.7</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of a linear combination of random variables.</span>
</h4>
<p id="p-157">If \(\displaystyle c_1, c_2, \dots, c_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E\left[\sum_{i=1}^n c_i g_i(X_1, X_2, \dots, X_k)\right] =
\sum_{i=1}^n c_i E\left[g_i(X_1, X_2, \dots, X_k)\right]
\end{equation*}
</div></article></section><section class="exercises" id="exercises-11"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.1.2</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-18"><h4 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.9.</span>
</h4>
<p id="p-158">Suppose that \(X\) takes on values \(0, 1, 2, 3\) with probabilities \(\dfrac{1}{125}, \dfrac{12}{125}, \dfrac{48}{125},
\dfrac{64}{125}\)</p>
<ol class="lower-alpha">
<li id="li-18"><p id="p-159">Find \(E[X]\) and \(E[X^2]\)</p></li>
<li id="li-19"><p id="p-160">Determine the value of \(E[(3X + 2)^2]\)</p></li>
</ol></article></section><section class="subsection" id="sub-moments"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.1.3</span> <span class="title">Expected value</span>
</h3>
<p id="p-161">See section 4.3. Recommended problems: 4.3 (pg 146) 20, 22, 23, 31, 33, 34, 40, (pg 162) 69, 73, 75</p>
<article class="definition definition-like" id="def-moments-about-origin-4-2"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.8</span><span class="period">.</span><span class="space"> </span><span class="title">moments about the origin.</span>
</h4>
<p id="p-162">The \(\displaystyle r^\text{th}\) <dfn class="terminology">moment about the origin</dfn> of a random variable \(X\text{,}\) denoted by \(\displaystyle \mu_r'\text{,}\) is the expected value of \(\displaystyle
(X)^r\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_r'=E[(X)^r] = \sum_x x^r\cdot f(x)
\end{equation*}
</div>
<p class="continuation">for \(\displaystyle r =
0,1,2, \dots\) when \(X\) is discrete.</p></article><article class="definition definition-like" id="def-mean-4-3"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.9</span><span class="period">.</span><span class="space"> </span><span class="title">mean of a discrete random variable.</span>
</h4>\(\displaystyle \mu_1'\) is called the <dfn class="terminology">mean</dfn> of the distribution of \(X\text{,}\) or simply the <dfn class="terminology">mean</dfn> of \(X\text{;}\) and it is denoted by \(\displaystyle
\mu\text{.}\)</article><article class="definition definition-like" id="def-moments-about-mean-4-4"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.10</span><span class="period">.</span><span class="space"> </span><span class="title">moments about the mean.</span>
</h4>
<p id="p-163">The \(\displaystyle r^\text{th}\) <dfn class="terminology">moment about the mean</dfn> of a random variable \(X\text{,}\) denoted by \(\displaystyle \mu_r\text{,}\) is the expected value of \(\displaystyle
(X-\mu)^r\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_r=E[(X-\mu)^r] = \sum_x (x-\mu)^r\cdot f(x)
\end{equation*}
</div>
<p class="continuation">for \(\displaystyle r =
0,1,2, \dots\) when \(X\) is discrete.</p></article><p id="p-164">Now, you could imagine in some cases the moments being difficult to calculate as sums. We sometimes take the approach of building what are called moment-generating functions. These are mathematical functions whose purpose is to generate the moments of a distribution that we might need.</p></section><section class="exercises" id="exercises-12"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.1.4</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-19"><h4 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.18.</span>
</h4>
<p id="p-165">Find \(\mu\text{,}\) \(\mu_2'\text{,}\) and \(\sigma^2\) for the random variable \(X\) that has probability distribution \(f(x) = 0.5 \text{
for } x= \pm 2\text{.}\)</p></article></section><section class="subsection" id="sub-mgf"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.1.5</span> <span class="title">Moment-generating functions</span>
</h3>
<p id="p-166">See section 4.5.</p>
<article class="definition definition-like" id="def-mgf-4-6"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.11</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function.</span>
</h4>
<p id="p-167">The <dfn class="terminology">moment-generating function</dfn> of a random variable \(X\text{,}\) where it exists, is given by</p>
<div class="displaymath">
\begin{equation*}
\displaystyle
M_X(t) = E[e^{tX}] = \sum_x e^{tx}\cdot f(x)
\end{equation*}
</div>
<p class="continuation">when \(X\) is discrete.</p></article><p id="p-168">Notce that a moment-generating function \(\displaystyle M_X(t)\) itself is a function of the variable \(\displaystyle t\) not \(X\text{.}\) As it turns out, we are most interested in values of the function at or near \(\displaystyle t=0\text{.}\)</p>
<article class="example example-like" id="ex-maclaurin"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-maclaurin"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.1.12</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function via Taylor series.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-maclaurin"><article class="example example-like"><p id="p-169">Recall that the Maclaurin series (Taylor series around zero) for \(\displaystyle e^{tx}\) is</p>
<div class="displaymath">
\begin{equation*}
e^{tx} = 1 + tx +
\frac{1}{2!}\left(tx\right)^2 + \frac{1}{3!}\left(tx\right)^3 + \cdots +
\frac{1}{r!}\left(tx\right)^r + \cdots
\end{equation*}
</div>
<p class="continuation">This means (in the discrete case), that</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
M_X(t) \amp = \sum_x e^{tx} f(x)\\
\amp = \sum_x \left(1 + tx + \frac{1}{2!}(tx)^2 +
\frac{1}{3!}\left(tx\right)^3 + \cdots + \frac{1}{r!}\left(tx\right)^r +
\cdots\right) f(x)\\
\amp = \sum_x (f(x) + txf(x) + \frac{t^2}{2!}x^2f(x) +
\frac{t^3}{3!}x^3f(x) + \cdots + \frac{t^r}{r!}x^rf(x) + \cdots)\\
\amp = \sum_xf(x) + t\sum_x xf(x) + \frac{t^2}{2!}\sum_x x^2f(x) +
\frac{t^3}{3!}\sum_x x^3f(x) + \cdots + \frac{t^r}{r!}\sum_x x^rf(x) +
\cdots
\end{aligned}
\end{equation*}
</div>
<p class="continuation">Looking closely, at</p>
<div class="displaymath">
\begin{equation*}
M_X(t) = \sum_xf(x) + \left(\sum_x xf(x)\right)t
+ \left(\sum_x x^2f(x)\right)\frac{t^2}{2!} + \left(\sum_x
x^3f(x)\right)\frac{t^3}{3!} + \cdots + \left(\sum_x
x^rf(x)\right)\frac{t^r}{r!} + \cdots
\end{equation*}
</div>
<p class="continuation">coefficients of the terms \(\displaystyle \dfrac{t^r}{r!}\) are the moments about the origin \(\displaystyle \mu_r' = \sum_x x^r f(x)\)</p></article></div>
<article class="example example-like" id="ex-three-cards-mgf"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-three-cards-mgf"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.1.13</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function for three cards.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-three-cards-mgf"><article class="example example-like"><p id="p-170">Recall the probability distribution \(f(x) = P(X = x) =
\dfrac{{3\choose x}}{8} \text{ for } x = 0, 1, 2, 3\) (this was used earlier to determine the probabilities of \(x\) heads on three flips of a coin).</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
M_X(t) \amp = \sum_x e^{tx} f(x)\\
\amp = 1 \cdot \frac{1}{8} + e^{t} \cdot \frac{3}{8}+ e^{2t} \cdot
\frac{3}{8} + e^{3t} \cdot \frac{1}{8}\\
\amp = \frac{1}{8} \left(1 + 3e^{t} + 3e^{2t} + e^{3t}\right)\\
M_X(t) \amp = \frac{1}{8} (1+e^t)^3\\
\end{aligned}
\end{equation*}
</div></article></div>
<article class="theorem theorem-like" id="thm-4-9"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.14</span><span class="period">.</span><span class="space"> </span><span class="title">moments via differentiation.</span>
</h4>
<p id="p-171">The \(r^\text{th}\) moment about the origin, \(\mu_r'\text{,}\) can be written</p>
<div class="displaymath">
\begin{equation*}
\displaystyle \mu_r' = \dfrac{d^rM_X(t)}{dt^r}\Big|_{t=0}
\end{equation*}
</div></article><article class="example example-like" id="ex-three-cards-mgf2"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-three-cards-mgf2"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.1.15</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function for three cards, via differentiation.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-three-cards-mgf2"><article class="example example-like"><p id="p-172">Referencing Example <a href="" class="xref" data-knowl="./knowl/ex-three-cards-mgf.html" title="Example 4.1.13: moment-generating function for three cards">Example 4.1.13</a>, the mean of the random variable, given by \(\mu_1'\text{,}\) whose MGF is \(M_X(t) =
\frac{1}{8} (1+e^t)^3\) is found as follows</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
\mu_1' \amp = \left(\dfrac{d}{dt}\left(\frac{1}{8}
(1+e^t)^3\right)\right)\Big|_{t=0}\\
\amp = \left(\frac{3}{8} (1+e^t)^2e^t\right)\Big|_{t=0}\\
\amp = \left(\frac{3}{8} (2)^2\right)\\
\mu_1' \amp = \frac{3}{2}
\end{aligned}
\end{equation*}
</div></article></div>
<p id="p-173">So, <em class="emphasis">given</em> a moment-generating function, a relatively simple application of calculus allows us to replace a more tedious calculation of the moment from its definition.</p>
<article class="theorem theorem-like" id="thm-4-10"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.16</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function of functions of a random variable.</span>
</h4>
<p id="p-174">If \(\displaystyle a\) and \(\displaystyle b\) are constants, then</p>
<ol class="decimal">
<li id="li-20"><p id="p-derived-li-20">\(\displaystyle M_{X+a}(t) = E[e^{(X+a)t}] = e^{at} \cdot
M_X(t)\text{;}\)</p></li>
<li id="li-21"><p id="p-derived-li-21">\(\displaystyle M_{bX}(t) = E[e^{bXt}] = M_X(bt)\text{;}\)</p></li>
<li id="li-22"><p id="p-derived-li-22">\(\displaystyle M_{\frac{X+a}{b}}(t) =
E\left[e^{\left(\frac{X+a}{b}\right)t}\right] = e^{(a/b)t} \cdot
M_X\left(\frac{t}{b}\right)\text{;}\)</p></li>
</ol></article><p id="p-175">The rules in <a href="" class="xref" data-knowl="./knowl/thm-4-10.html" title="Theorem 4.1.16: moment-generating function of functions of a random variable">Theorem 4.1.16</a> allow us to calculate moment-generating functions of simple functions of a random variable.</p></section><section class="exercises" id="exercises-13"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.1.6</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-20"><h4 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.33.</span>
</h4>
<p id="p-176">Find the moment-generating function of the discrete random variable \(X\) that has the probability distribution</p>
<div class="displaymath">
\begin{equation*}
f(x) =
2\left(\dfrac{1}{3}\right)^x \text{ for } x = 1, 2, 3, \dots
\end{equation*}
</div>
<p class="continuation">and use it to determine the values of \(\mu_1'\) and \(\mu_2'\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-21"><h4 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.40.</span>
</h4>
<p id="p-177">Given the moment-generating function \(X_X(t) = e^{3t+8t^2}\text{,}\) find the moment generating function of the random variable \(Z =
\dfrac{1}{4}\left(X-3\right)\) and use it to determine the mean and variance of \(Z\text{.}\)</p></article></section><section class="subsection" id="sub-product-moments"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.1.7</span> <span class="title">Product moments</span>
</h3>
<p id="p-178">See section 4.6.</p>
<article class="definition definition-like" id="def-product-moments-origin-4-7"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.17</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the origin.</span>
</h4>
<p id="p-179">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the origin</dfn> of the random variables \(X\) and \(Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}\text{,}\) is the expected value of \(\displaystyle X^rY^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}'=E[X^rY^s] = \sum_x\sum_y x^r y^s\cdot f(x, y)
\end{equation*}
</div>
<p class="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2,
\dots\) when \(X\) and Y are discrete.</p></article><p id="p-180">Special cases of product moments are \(\displaystyle \mu_{1,0}' =
E[X^1Y^0] = E[X] = \mu_X\) and \(\displaystyle \mu_{0,1}' = E[X^0Y^1]
= E[Y] = \mu_Y\text{.}\)</p>
<p id="p-181">As complicated as the definitions of the product moments may be, they lead to a way to define and calculate the very important concept of covariance.</p>
<ul class="disc">
<li id="li-23"><p id="p-derived-li-23">If we have a high probability of large \(X\) paired with large \(Y\) and small \(\displaystyle
X\) paired with small \(Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \gt 0\)</p></li>
<li id="li-24"><p id="p-derived-li-24">If we have a high probability of large \(X\) paired with small \(Y\) and small \(\displaystyle
X\) paired with large \(Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \lt 0\)</p></li>
</ul>
<article class="definition definition-like" id="def-product-moments-mean-4-8"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.18</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the mean.</span>
</h4>
<p id="p-182">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the mean</dfn> of the random variables \(X\) and \(Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}'\text{,}\) is the expected value of \(\displaystyle (X-\mu_X)^r(Y-\mu_Y)^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}=E[(X-\mu_X)^r(Y-\mu_Y)^s] = \sum_x\sum_y (x-\mu_X)^r
(y-\mu_Y)^s\cdot f(x, y)
\end{equation*}
</div>
<p class="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2, \dots\) when \(X\) and Y are discrete.</p></article><article class="definition definition-like" id="def-covariance-4-9"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.19</span><span class="period">.</span><span class="space"> </span><span class="title">covariance.</span>
</h4>
<p id="p-183">\(\displaystyle \mu_{1,1}\) is called the <dfn class="terminology">covariance</dfn> of \(X\) and \(Y\text{,}\) and it is denoted by \(\displaystyle \sigma_{XY}\) or \(\displaystyle
\operatorname{cov}(X, Y)\text{,}\) or \(\displaystyle C(X, Y)\text{.}\)</p></article><article class="theorem theorem-like" id="thm-4-11"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.20</span><span class="period">.</span><span class="space"> </span><span class="title">covariance from moments about the origin.</span>
</h4>
<p id="p-184">\(\displaystyle \sigma_{XY} = \mu_{1,1}' - \mu_X \mu_Y\)</p></article><article class="theorem theorem-like" id="thm-4-12"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.21</span><span class="period">.</span><span class="space"> </span><span class="title">independence and covariance.</span>
</h4>
<p id="p-185">If \(X\) and \(Y\) are independent, then</p>
<div class="displaymath">
\begin{equation*}
\displaystyle E[XY] = E[X]\cdot E[Y]
\end{equation*}
</div>
<p class="continuation">and \(\displaystyle \sigma_{XY} = 0\text{.}\)</p></article><article class="remark remark-like" id="rmrk-4-12"><h4 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">4.1.22</span><span class="period">.</span>
</h4>In terms of moments,<div class="displaymath">
\begin{equation*}
\displaystyle\mu_{1,1}' = \mu_{1,0}'\cdot \mu_{0,1}'
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-13"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.23</span><span class="period">.</span><span class="space"> </span><span class="title">product moments of independent random variables.</span>
</h4>
<p id="p-186">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent, then \(\displaystyle E[X_1X_2\cdots X_n] = E[X_1]\cdot
E[X_2]\cdot\cdots\cdot E[X_n]\text{.}\)</p></article><p id="p-187">Independence means covariance is zero, but covariances of zero does not mean independence.</p></section><section class="exercises" id="exercises-14"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.1.8</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-22"><h4 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.41.</span>
</h4>
<p id="p-188">If \(X\) and \(Y\) have the joint probability distribution \(f(x, y) = \dfrac{1}{4}\) for \((-3, -5)\text{,}\)  \((-1, -1)\text{,}\) \((1, 1)\text{,}\)  \((3, 5)\text{,}\) find \(\operatorname{cov}(X, Y)\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-23"><h4 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.45.</span>
</h4>
<p id="p-189">If \(X\) and \(Y\) have the joint probability distribution \(f(-1, 0) = 0\text{,}\)  \(f(-1, 1) = \dfrac{1}{4}\text{,}\) \(f(0, 0) = \dfrac{1}{6}\text{,}\) \(f(1, 0) = \dfrac{1}{12}\text{,}\) \(f(1,
1) = \dfrac{1}{2}\) show that</p>
<ol class="lower-alpha">
<li id="li-25"><p id="p-190">\(\operatorname{cov}(X, Y) = 0\text{;}\)</p></li>
<li id="li-26"><p id="p-191">the two random variables are not independent.</p></li>
</ol></article></section><section class="subsection" id="sub-moments-linear-combinations"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.1.9</span> <span class="title">Moments of linear combinations of random variables</span>
</h3>
<section class="introduction" id="introduction-15"><p id="p-192">See section 4.7. Recommended problems: 4.7-8 (pg 158) 48, 49, 57</p></section><article class="theorem theorem-like" id="thm-4-14"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.24</span><span class="period">.</span><span class="space"> </span><span class="title">variance.</span>
</h4>
<p id="p-193">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle
a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E[Y] = \sum_{i=1}^n
a_iE[X_i]
\end{equation*}
</div>
<p class="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n a_i^2
\operatorname{var}[X_i] + 2 \mathop{\sum \sum}_{i \lt j} a_i
a_j\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.1.25</span><span class="period">.</span><span class="space"> </span><span class="title">variance of independent random variables.</span>
</h4>
<p id="p-194">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n
a_i^2\operatorname{var}[X_i]
\end{equation*}
</div></article><article class="example example-like" id="ex-var-covar"><a href="" data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-var-covar"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.1.26</span><span class="period">.</span><span class="space"> </span><span class="title">covariances of linear combinations.</span>
</h4></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-var-covar"><article class="example example-like"><p id="p-195">Consider three random variables \(X\text{,}\) \(Y\text{,}\) and \(Z\) with \(\mu_X = 2\text{,}\) with \(\mu_Y = -3\text{,}\) with \(\mu_Z = 4\text{;}\) with \(\sigma_X^2 = 1\text{,}\) \(\sigma_Y^2 = 5\text{,}\) \(\sigma_Z^2 =
2\text{;}\) and \(\operatorname{cov}(X, Y) = -2\text{,}\) \(\operatorname{cov}(X, Z) =
-1\text{,}\) and \(\operatorname{cov}(Y, Z) = 1\text{.}\)</p>
<p id="p-196">Find \(\mu_W\) and \(\operatorname{var}(W) = \sigma_W^2\) for \(W = 3X-Y+2Z\text{.}\)</p>
<a href="" data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-6" id="solution-6"><span class="type">Solution</span><span class="period">.</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-6"><div class="solution solution-like">
<p id="p-197">First, \(\mu_W = (3)\mu_X + (-1)\mu_Y + (2)\mu_Z = 17\text{.}\)</p>
<p id="p-198">We could apply the theorem directly, but we can do this more directly with linear algebra. The idea is that we can picture the linear combination \(W = 3X-Y+2Z\) as</p>
<div class="displaymath">
\begin{equation*}
W =
\left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}X \\Y\\
Z\end{array}\right] = (3)X + (-1)Y + (2)Z
\end{equation*}
</div>
<p id="p-199">Let \(a\) be the row vector \(a = \left[\begin{array}{ccc}3 \amp -1
\amp 2\end{array}\right]\text{,}\) its transpose be the column vector \(a^T\text{,}\) and the matrix \(\Sigma\) be defined as follows,</p>
<div class="displaymath">
\begin{equation*}
\Sigma = \left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]
\end{equation*}
</div>
<p id="p-200">This approach can be justified by expanding the sums in <a href="" class="xref" data-knowl="./knowl/thm-4-14.html" title="Theorem 4.1.24: variance">Theorem 4.1.24</a> with a sum of 2 random variables.</p>
<p id="p-201">We can calculate the variance of \(W\) by \(\operatorname{var}(W)
= a\Sigma a^T\text{.}\) Specifically,</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]\cdot\left[\begin{array}{c}3 \\-1\\
2\end{array}\right]
\end{equation*}
</div>
<p id="p-202">Notice \(Sigma\) is symmetric and that the covariances lie in order along the main diagonal and the variances off-diagonal.</p>
<p id="p-203">Multiplying the square matrix and column vector first, we have</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}3 \\-9\\
0\end{array}\right]
\end{equation*}
</div>
<p id="p-204">And finally, \(a \Sigma a^T = 18\text{.}\)</p>
</div></div></article></div>
<article class="theorem theorem-like" id="thm-4-15"><h4 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.1.27</span><span class="period">.</span><span class="space"> </span><span class="title">covariance of two linear combinations.</span>
</h4>
<p id="p-205">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_i X_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n,
b_1, b_2, \dots, b_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1,
Y_2] = \sum_{i=1}^n a_i b_i\operatorname{var}[X_i] + \mathop{\sum
\sum}_{i \lt j} (a_ib_j + a_jb_i)\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3b"><h4 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.1.28</span><span class="period">.</span>
</h4>
<p id="p-206">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_iX_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1, Y_2] =
\sum_{i=1}^n a_i b_i\operatorname{var}[X_i]
\end{equation*}
</div></article> The same logic used in <a href="" class="xref" data-knowl="./knowl/ex-var-covar.html" title="Example 4.1.26: covariances of linear combinations">Example 4.1.26</a> allows us to compute the covariance between two linear combinations of random variables directly also. Instead of calculating \(a\Sigma a^T\) we will calculate \(a \Sigma b^T\) where \(b\) is the vector of coefficients of the second linear combination.</section><section class="exercises" id="exercises-15"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.1.10</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-24"><h4 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.48.</span>
</h4>
<p id="p-207">If \(X_1\text{,}\) \(X_2\text{,}\) \(X_3\) are independent and have the means \(4, 9, 3\) and the variances \(3, 7, 5\text{,}\) find the mean and variance of  show that</p>
<ol class="lower-alpha">
<li id="li-27"><p id="p-208">\(Y = 2X_1 - 3X_2 + 4X_3\text{;}\)</p></li>
<li id="li-28"><p id="p-209">\(Z = X_1 + 2X_2 -X_3\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-25"><h4 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.49.</span>
</h4>
<p id="p-210">Repeat both parts of the previous exercise after dropping the assumption of independence and using instead that \(\operatorname{cov}(X_1, X_2) = 1\text{,}\) \(\operatorname{cov}(X_2, X_3)
= -2\text{,}\) \(\operatorname{cov}(X_1, X_3) = -3\text{.}\)</p></article></section><section class="subsection" id="sub-conditional-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.1.11</span> <span class="title">Conditional expectation</span>
</h3>
<p id="p-211">See section 4.8.</p>
<article class="definition definition-like" id="def-conditional-expectation-4-10"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.29</span><span class="period">.</span><span class="space"> </span><span class="title">conditional expectation.</span>
</h4>
<p id="p-212">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle u(X)\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
E[u(X)|y] = \sum_x u(x)\cdot f(x|y)
\end{equation*}
</div>
<p class="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle v(Y)\) given \(\displaystyle X
= x\) is</p>
<div class="displaymath">
\begin{equation*}
E[v(Y)|x] = \sum_y v(y)\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="def-conditional-mean"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.30</span><span class="period">.</span><span class="space"> </span><span class="title">conditional mean.</span>
</h4>
<p id="p-213">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle u(X) = X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\mu_{X|y} = E[X|y] = \sum_x x\cdot f(x|y)
\end{equation*}
</div>
<p class="continuation">and the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle v(Y) = Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle \mu_{Y|x} = E[Y|x] = \sum_y y\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="def-conditional-variance"><h4 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.1.31</span><span class="period">.</span><span class="space"> </span><span class="title">conditional variance.</span>
</h4>
<p id="p-214">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional variance</dfn> of \(X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\sigma^2_{X|y} = E[(X-\mu_{X|y})^2|y] = E[X^2]-\mu^2_{X|y}
\end{equation*}
</div>
<p class="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle\sigma^2_{Y|x} = E[(Y-\mu_{Y|x})^2|y] =
E[Y^2]-\mu^2_{Y|x}
\end{equation*}
</div></article></section></section></div></main>
</div>
</body>
</html>
