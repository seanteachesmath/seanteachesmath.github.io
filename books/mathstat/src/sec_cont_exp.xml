<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="section-continuous-expectation">
<title>Expectation of continuous random variables</title>

<introduction>

<p> This is the start of Chapter 4 in Freund's Mathematical Statistics.
Now we will study the major topics of this chapter with a
focus on those applying to continuous random variables. </p>
</introduction>

<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->

<subsection xml:id="sub-expectation">
<title>Expected value</title>
<p>See section 4.1 - 4.2. Recommended problems: (pg 136) 7, 9, 10, 11,
(pg 161) 60
</p>


<definition xml:id="def-expected-value-4-1-cont">
<title>expected value (continuous)</title>
<statement>
<p>
If <m>X</m> is a continuous random variable and <m>f(x)</m> is the value
of its probability density at <m>x</m>, the <term>expected value</term>
of <m>X</m> is given by
<me> E[X] = \int x\cdot f(x)\,dx</me>
</p>
</statement>
</definition>

<example xml:id="ex-cont-exp">
<title>expected value (continuous)</title>
<statement>
<p>
Consider the probability density given below <me>f(x) = \begin{cases}x,
\amp \quad 0 \lt x \lt 1\\
2-x, \amp \quad 1 \le x \lt 2\\
0, \amp \quad \text{otherwise}\\
\end{cases}</me>
Find <m>E[X]</m>.
</p>

<solution>
<me>
\begin{aligned}[t]
E[X] \amp = \int_{-\infty}^\infty x\cdot f(x)\,dx\\
 \amp = \int_{-\infty}^0 0\,dx + \int_{0}^1 \underbrace{x\cdot
x}_{x^2}\,dx + \int_{1}^2 \underbrace{x\cdot(2-x)}_{2x-x^2}\,dx +
\int_2^{\infty} 0\,dx\\
 \amp = 0 + \int_{0}^1 x^2\,dx + \int_{1}^2 2x-x^2\,dx + 0\\
 \amp = \Big(\dfrac{x^3}{3}\Big)\Big|0^1 +
\Big(x^2-\dfrac{x^3}{3}\Big)\Big|1^2\\
 \amp = \Big(\dfrac{1}{3} - 0\Big) + \Big(\Big(4-\dfrac{8}{3}\Big) -
\Big(1-\dfrac{1}{3}\Big)\Big)\\ 
E[X] \amp = 1
\end{aligned}
</me>
</solution>

</statement>
</example>

<theorem
xml:id="thm-expected-value-function-random-variable-4-1-cont">
<title>expected value of a function of a random variable
(continuous)</title>
<statement>
<p>
If <m>X</m> is a continuous random variable and <m>f(x)</m> is the value
of its probability density at <m>x</m>, the expected value of
<m>g(X)</m> is given by
<me>E[g(X)] = \int g(x)\cdot f(x)\,dx</me>
</p>
</statement>
</theorem>

<p>
Now would be a good time to review <xref ref="thm-4-2"/>, <xref
ref="cor-4-1"/>, <xref ref="cor-4-2"/>, and <xref ref="thm-4-5"/>, all
of which apply here as well.

Recall, certain choices of <m>g(x)</m> in <xref
ref="thm-expected-value-function-random-variable-4-1-cont"/> give us the
mean, variance, and other special moments.
</p>

<p> While we are at it, consider <xref ref="thm-4-2"/> which suggested
that <m>E[aX+b] = aE[X]+b</m>. Defining expectation by integration gives
a more convenient means of proving this statement.
<proof xml:id="proof-thm-4-2">
<me>
\begin{aligned}[t]
E[aX+b] \amp = \int_{-\infty}^\infty (ax+b)\cdot f(x)\,dx\\
\amp = \int_{-\infty}^\infty ax\cdot f(x)\,dx + \int_{-\infty}^\infty
b\cdot f(x)\,dx\\
\amp = a\int_{-\infty}^\infty x\cdot f(x)\,dx + b\int_{-\infty}^\infty
\cdot f(x)\,dx\\
\amp = aE[x] + b
\end{aligned}
</me>
</proof>
Notice that the second of these integrals is <m>1</m> since <m>f(x)</m>
is a probability density function.
</p>
<!-->
<theorem xml:id="thm-continuous-th4-2">
<p> If <m>a</m> and <m>b</m> are constants, then
<m>E[aX +b] = aE[X]+b</m>.
</p>
</theorem>

<corollary xml:id="continuous-cor4-1">
<p> If a is a constant, then
<m>E[aX] = aE[X]</m>.
</p>
</corollary>

<corollary xml:id="continuous-cor4-2">
<p>If b is a constant, then
<m>E[b] = b</m>.
</p>
</corollary>-->

</subsection>
<!-- See section 4.1 - 4.2. Recommended problems: (pg 136) disc: 9, (pg
161) 60, cont: 7, 10, 11 -->
<exercises>
 <exercise>
 <title>Problem 4.7</title>
 <statement>
 <p>Find the expected value of the random variable <m>Y</m> whose probability density function is given by <me>f(y) = \begin{cases} \dfrac{1}{8}(y+1), \amp \quad 2 \lt y \lt 4\\
 0, \amp \quad \text{elsewhere}\end{cases}</me>
 </p>
 </statement>
 </exercise>
 
  <exercise>
 <title>Problem 4.10</title>
 <statement>
 <p>If the probability density of <m>X</m> is given by <me>f(x) = \begin{cases} \dfrac{1}{x\ln(3)}, \amp \quad 1 \lt x \lt 3\\
 0, \amp \quad \text{elsewhere}\end{cases}</me>
  <ol>
<li><p> find <m>E[X]</m>, <m>E[X^2]</m>, <m>E[X^3]</m>.</p></li>
<li><p> determine the value of <m>E[X^3 + 2X^2-3X+1]</m>.</p></li>
</ol>
 </p>
 </statement>
 </exercise>
 
</exercises>


<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->

<subsection xml:id="sub-moments-cont">
<title> Moments</title>
<p>See section 4.3. Recommended problems: 4.3 (pg 146) 20, 22, 23, 31,
33, 34, 40, (pg 162) 69, 73, 75
</p>

<definition xml:id="def-moments-about-origin-4-2-cont">
<title>moments about the origin (continuous)</title>
<statement>
<p>
The <m>r^\text{th}</m> <term>moment about the origin</term> of a random
variable <m>X</m>, denoted by <m>\mu_r'</m>, is the expected value of
<m>(X)^r</m>; symbolically
<me>\mu_r'=E[(X)^r] = \int_{-\infty}^\infty x^r\cdot f(x)\,dx</me> for
<m>r = 0,1,2,
\dots</m> when <m>X</m> is continuous.
</p>
</statement>
</definition>

<!--><definition xml:id="def-continuous-mean-4-3">
<m>\mu_1'</m> is called the <term>mean</term> of the density of
<m>X</m>, or simply the <term>mean</term> of <m>X</m>; and it is denoted
by <m>\mu</m>.
</definition>-->


<definition xml:id="def-moments-about-mean-4-4-cont">
<title>moments about the mean (continuous)</title>
<statement>
<p>
The <m>\displaystyle r^\text{th}</m> <term>moment about the
mean</term> of a random variable <m>X</m>, denoted by
<m>\displaystyle \mu_r</m>, is the expected value of <m>\displaystyle
(X-\mu)^r</m>; symbolically
<me>\mu_r=E[(X-\mu)^r] = \int_{-\infty}^\infty (x-\mu)^r\cdot
f(x)\,dx</me> for
<m>\displaystyle r =
0,1,2, \dots</m> when <m>X</m> is discrete.
</p>
</statement>
</definition>

<example xml:id="ex-poly">
<title> <m>8^{\text{th}}</m>-order polynomial density</title>
<statement>
<p>
Consider <me>f(x) = \begin{cases}630x^4(1-x)^4, \amp \quad 0 \lt x \lt
1\\ 0, \amp \quad
\text{otherwise}\end{cases}</me>
Find <m>\mu</m> and <m>\sigma^2</m>.
</p>
<hint>
<p>
It is recommended that you expand the term <m>(1-x)^4</m>.  Doing
that, and carefully integrating, you should find <m>\mu =
\sfrac{1}{2}</m>. Additionally, by finding <m>\mu_2' = E[X^2] =
\sfrac{3}{11}</m>, you will be able to use <me>\sigma^2 = \mu_2' -
(\mu)^2</me> to find the variance is <m>\sigma^2 = \sfrac{1}{44}</m>.
</p>
</hint>
</statement>
</example>
</subsection>


<!-- Recommended problems: 4.3 (pg 146) disc: 23; cont: 20, 22, 31, -->
<exercises>
 <exercise>
 <title>Problem 4.20</title>
 <statement>
 <p>Find <m>\mu_r'</m> and <m>\sigma^2</m> for the random variable
<m>X</m> with probability density given by <me>f(x) = \begin{cases}
\dfrac{1}{x\ln(3)}, \amp \quad 1 \lt x \lt 3\\
 0, \amp \quad \text{elsewhere}\end{cases}</me>
 </p>
 <hint> Use the definition of <m>\mu_r' = E[X^r]</m> to begin.
 </hint>
 </statement>
 </exercise>
 
<!-->  <exercise> 
 <title>Problem 4.22</title>
 <statement>
 <p>Find <m>\mu</m>, <m>\mu_2'</m>, and <m>\sigma^2</m> for the
random
variable <m>X</m> that has probability distribution <m>f(x) = 0.5 \text{
for } x= \pm 2</m>.
 </p>
 </statement>
 </exercise>-->
 
</exercises>
<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->


<subsection xml:id="sub-chebyshev">
<title>Chebyshev's Theorem</title>
<introduction>
<p>See section 4.4.</p>
</introduction>

<theorem xml:id="thm-chebyshev">
<title>Chebyshev's Theorem</title>
<statement>
<p>If <m>\mu</m> and <m>\sigma</m> are the mean and the standard
deviation of a random variable <m>X</m>, then for any positive constant
<m>k</m> the probability is <em>at least</em> <m>1-\dfrac{1}{k^2}</m>
that <m>X</m> will take on a value within <m>k</m> standard deviations
of
the mean; symbolically <me>P(|x-\mu| \lt k\sigma) \ge 1 -
\dfrac{1}{k^2}, \sigma \ne 0</me>
</p>
</statement></theorem>

<p> <xref ref="thm-chebyshev"/> gives a lower bound on the probability
that the value of the random variable is within a certain distance
(specifically <m>\pm k\sigma</m>) of the mean.</p>

</subsection>

<exercises>
  <exercise>
 <title>Problem 4.31</title>
 <statement>
 <p>What is the smallest value of <m>k</m> Chebyshev's theorem for which
the probability that a random variable wil take on a value between
<m>\mu -k\sigma</m> and <m>\mu k+\sigma</m> is 
<ol>
<li><p>at least <m>0.95</m>?</p></li>
<li><p>at least <m>0.99</m>?</p></li>
</ol>
 </p>
 </statement>
 </exercise>
</exercises>

<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->


<subsection xml:id="sub-mgf-cont">
<title>Moment-generating functions</title>
<p>See section 4.5.</p>

<definition xml:id="def-mgf-4-6-cont">
<title>moment-generating function (continuous)</title>

<statement>
<p> The <term>moment-generating function</term> of a random variable
<m>X</m>, where it exists, is given by <me>\displaystyle
M_X(t) = E[e^{tX}] = \int_{-\infty}^\infty e^{tx}\cdot f(x)\,dx</me>
when <m>X</m> is discrete.
</p>
</statement></definition>

<p>
Notce that a moment-generating function <m>\displaystyle M_X(t)</m>
itself is a function of the variable <m>\displaystyle t</m> not
<m>X</m>. As it turns out, we are most interested in
values
of the function at or near <m>t=0</m>.
</p>

<p>
Now would be a good time to review <xref ref="thm-4-9"/> and <xref
ref="thm-4-10"/> which apply here in the continuous case as well.
</p>

<example xml:id="ex-exp-MGF">
<title>moment-generating function by definition</title>
<statement>
<p>
Find the moment-generating function of <me>f(x) = \begin{cases}e^{-x}, \amp \quad 0 \lt x
\\ 0, \amp \quad
\text{otherwise}\end{cases}</me>
</p>
<solution>
<me>
\begin{aligned}[t]
M_X(t) \amp = E[e^{tX}]\\
		\amp = \int_{0}^\infty e^{tx}\cdot e^{-x}\,dx\\
		\amp = \int_{0}^\infty e^{-(1-t)x}\,dx\\
		\amp =
\lim_{A\to\infty}\Big(\Big(-\dfrac{1}{1-t}e^{-(1-t)x}\Big)\Big|_0^A\Big)
\\
		\amp =
\lim_{A\to\infty}\Big(\Big(\underbrace{-\dfrac{1}{1-t}e^{-(1-t)A}}_{\text{needs }t \lt 1}\Big) - \Big(-\dfrac{1}{1-t}e^{-(1-t)0}\Big)\Big)\\
M_X(t) \amp = 0 + \dfrac{1}{1-t}
\end{aligned}
</me>
For the first integral to converge we require <m>t \lt 1</m>.
</solution>
</statement>
</example>

<p>
With the result from <xref ref="ex-exp-MGF"/>, assuming <m>|t| \lt 1</m>
we can expand around <m>t=0</m> to find <me>M_X(t) = \dfrac{1}{1-t} = 1
+ t + t^2 + \dots + t^r +
\dots</me> Remembering that <m>\mu_r'</m> is the coefficient of
<m>\dfrac{t^r}{r!}</m>, we can rewrite the previous expansion as
<me>M_X(t) = \dfrac{1}{1-t} = 1 + 1\Big(\dfrac{t^1}{1!}\Big) +
2!\Big(\dfrac{t^2}{2!}\Big) + \dots + r!\Big(\dfrac{t^r}{r!}\Big) +
\dots</me> which shows that <m>\mu_r' = r!</m>.
</p>

<p>
Above the primary difficulty once the moment-generating function has
been calculated is probably in the expansion of the Maclaurin series.
One alternative to this requires calculating the integrals that define
the necessary expectations <me>\mu_r' = E[X^r] = \int_{-\infty}^\infty
x^r e^{-x}\,dx</me> by integration by parts or tabular integration. 
Instead, given the moment-generating function, we could apply <xref
ref="thm-4-9"/>.</p>


<!--><theorem xml:id="thm-4-9">
<title>moments via differentiation</title>
<statement>
<p>
The <m>r^\text{th}</m> moment about the origin, <m> \mu_r'</m>, can be
written
<me>\displaystyle \mu_r' = \dfrac{d^rM_X(t)}{dt^r}\Big|_{t=0}</me>
</p>
</statement></theorem>-->


<example xml:id="ex-exp-mgf-by-diff">
<title>moments of an exponential by differentiation</title>
<statement>
<p>
Recall the moment-generating function from <xref ref="ex-exp-MGF"/> was
<m>M_X(t) = \dfrac{1}{1-t}</m> for <m>t \lt 1</m>. Find <m>\mu_1'</m>
and <m>\mu_2'</m>.
</p>
<solution>
By theorem <me>
\begin{aligned}[t]
 \mu_1' \amp = \dfrac{d}{dt}\Big(M_X(t)\Big)\Big|_{t=0}\\
 \amp = \dfrac{d}{dt}\Big(\dfrac{1}{1-t}\Big)\Big|_{t=0}\\
 \amp = \Big(-(1-t)^{-2}(-1)\Big)\Big|_{t=0}\\
 \mu_1'  \amp = \Big(-(1-0)^{-2}(-1)\Big) = 1\\
  \end{aligned}
</me>
To calculate <m>\mu_2'</m> we take the second derivative of
<m>M_X(t)</m>.  Starting from <m>\dfrac{dM_X(t)}{dt} = (1-t)^{-2}</m>, we find <me>
\begin{aligned}[t]
 \mu_2' \amp = \dfrac{d}{dt}\Big(\dfrac{dM_X(t)}{dt}\Big)\Big|_{t=0}\\
 \amp = \dfrac{d}{dt}\Big((1-t)^{-2}\Big)\Big|_{t=0}\\
 \amp = \Big(-2(1-t)^{-3}(-1)\Big)\Big|_{t=0}\\
 \mu_2'  \amp = \Big(-2(1-0)^{-3}(-1)\Big) = 2\\
  \end{aligned}
</me>
</solution>
</statement>
</example>


<!--><p> So, <em>given</em> a moment-generating function, a relatively
simple application of calculus allows us to replace a more tedious
calculation of the moment from its definition.
</p>

<theorem xml:id="thm-4-10">
<title>moment-generating function of functions of a random
variable</title>

<statement>
<p>
If <m>\displaystyle a</m> and <m>\displaystyle b</m> are constants, then

<ol>
<li><m>\displaystyle M_{X+a}(t) = E[e^{(X+a)t}] = e^{at} \cdot
M_X(t)</m>;</li>
<li><m>\displaystyle M_{bX}(t) = E[e^{bXt}] = M_X(bt)</m>;</li>
<li><m>\displaystyle M_{\frac{X+a}{b}}(t) =
E\left[e^{\left(\frac{X+a}{b}\right)t}\right] = e^{(a/b)t} \cdot
M_X\left(\frac{t}{b}\right)</m>;</li>
</ol>
</p>
</statement></theorem>

<p>The rules in <xref ref="thm-4-10"/> allow us to calculate
moment-generating functions of simple functions of a random
variable.</p>-->
</subsection>

<exercises>
 

 <exercise>
 <title>Problem 4.34</title>
 <statement>
<p>Find the moment-generating function of the continuous random
variable
<m>X</m> that has the probability density <me>f(x) =
\begin{cases}1, \amp \quad \text{ for }0 \lt x \lt 1\\0, \amp \quad \text{elsewhere}/end{cases}</me> and
use it to determine the values of <m>\mu_1'</m>, <m>\mu_2'</m>, and <m>\sigma^2</m>.
 </p>
 </statement>
 </exercise>
 
</exercises>


<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->
</section>