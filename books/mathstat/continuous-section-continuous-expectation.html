<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2020-04-06T00:04:49-05:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Expectation of continuous random variables</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://www.math.uco.edu" target="_blank"><img src="images/cover.png" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="sample-book.html"><span class="title">Mathematical Statistics I:</span> <span class="subtitle">Based heavily on Freund's Mathematical Statistics</span></a></h1>
<p class="byline">Sean M. Laverty</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-probability-densities.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap-continuous.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="section-special-densities.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-probability-densities.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap-continuous.html" title="Up">Up</a><a class="next-button button toolbar-item" href="section-special-densities.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="front-colophon.html" data-scroll="front-colophon">Colophon</a></li>
<li><a href="author-bio-SML.html" data-scroll="author-bio-SML">Author Biography</a></li>
<li><a href="dedication.html" data-scroll="dedication">Dedication</a></li>
<li><a href="acknowledgement.html" data-scroll="acknowledgement">Acknowledgements</a></li>
<li><a href="preface.html" data-scroll="preface">Preface</a></li>
<li><a href="contributors.html" data-scroll="contributors">Contributors to the 0\(^\mathrm{th}\) Edition</a></li>
</ul>
</li>
<li class="link">
<a href="chap-counting.html" data-scroll="chap-counting"><span class="codenumber">1</span> <span class="title">Counting</span></a><ul><li><a href="section-counting.html" data-scroll="section-counting">Counting</a></li></ul>
</li>
<li class="link">
<a href="chap-prob.html" data-scroll="chap-prob"><span class="codenumber">2</span> <span class="title">Probability</span></a><ul><li><a href="section-probability.html" data-scroll="section-probability">Probability</a></li></ul>
</li>
<li class="link">
<a href="chap-discrete.html" data-scroll="chap-discrete"><span class="codenumber">3</span> <span class="title">Discrete Random Variables</span></a><ul>
<li><a href="section-probability-distributions.html" data-scroll="section-probability-distributions">Probability distributions</a></li>
<li><a href="section-discrete-expectation.html" data-scroll="section-discrete-expectation">Mathematical expectation of discrete random variables</a></li>
<li><a href="section-special-distributions.html" data-scroll="section-special-distributions">Special probability distributions</a></li>
</ul>
</li>
<li class="link">
<a href="chap-continuous.html" data-scroll="chap-continuous"><span class="codenumber">4</span> <span class="title">Continuous Random Variables</span></a><ul>
<li><a href="section-probability-densities.html" data-scroll="section-probability-densities">Probability densities</a></li>
<li><a href="continuous-section-continuous-expectation.html" data-scroll="continuous-section-continuous-expectation" class="active">Expectation of continuous random variables</a></li>
<li><a href="section-special-densities.html" data-scroll="section-special-densities">Special probability densities</a></li>
</ul>
</li>
<li class="link"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-def.html" data-scroll="appendix-def"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thm.html" data-scroll="appendix-thm"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-ex.html" data-scroll="appendix-ex"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-gfdl.html" data-scroll="appendix-gfdl"><span class="codenumber">D</span> <span class="title">GNU Free Documentation License</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
<li class="link"><a href="back-colophon.html" data-scroll="back-colophon"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="continuous-section-continuous-expectation"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">4.2</span> <span class="title">Expectation of continuous random variables</span>
</h2>
<a href="continuous-section-continuous-expectation.html" class="permalink">¶</a><section class="introduction" id="introduction-21"><p id="p-220">This is the start of Chapter 4 in Freund's Mathematical Statistics. In the first pass we will study the major topics of this chapter with a focus on those applying to discrete random variables.</p></section><section class="subsection" id="sub-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.2.1</span> <span class="title">Expected value</span>
</h3>
<a href="continuous-section-continuous-expectation.html#sub-expectation" class="permalink">¶</a><p id="p-221">See section 4.1 - 4.2. Recommended problems: (pg 136) 7, 9, 10, 11, (pg 161) 60</p>
<article class="definition definition-like" id="continuous-expected-value-4-1"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.1</span><span class="period">.</span><span class="space"> </span><span class="title">expected value (continuous).</span>
</h6>
<p id="p-222">If \(X\) is a continuous random variable and \(f(x)\) is the value of its probability density at \(x\text{,}\) the <dfn class="terminology">expected value</dfn> of \(X\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[X] = \int x\cdot f(x)\,dx
\end{equation*}
</div></article><article class="theorem theorem-like" id="continuous-expected-value-function-random-variable-4-1"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.2</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of a function of a random variable (continuous).</span>
</h6>
<p id="p-223">If \(X\) is a continuous random variable and \(f(x)\) is the value of its probability density at \(x\text{,}\) the expected value of \(g(X)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X)] = \int g(x)\cdot f(x)\,dx
\end{equation*}
</div></article><article class="theorem theorem-like" id="continuous-expected-value-joint-random-variables-4-4"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.3</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of joint random variables (continuous).</span>
</h6>
<p id="p-224">If \(X\) and \(Y\) are continuous random variables and \(f(x,
y)\) is the value of their joint probability density at \((x, y)\text{,}\) the expected value of \(g(X, Y)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X, Y)] = \int\int g(x, y)\cdot f(x,y)\,dx\,dy
\end{equation*}
</div></article></section><section class="subsection" id="sub-moments-cont"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.2.2</span> <span class="title">Expected value</span>
</h3>
<a href="continuous-section-continuous-expectation.html#sub-moments-cont" class="permalink">¶</a><p id="p-225">See section 4.3. Recommended problems: 4.3 (pg 146) 20, 22, 23, 31, 33, 34, 40, (pg 162) 69, 73, 75</p>
<article class="definition definition-like" id="continuous-moments-about-origin-4-2"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.4</span><span class="period">.</span><span class="space"> </span><span class="title">moments about the origin (continuous).</span>
</h6>
<p id="p-226">The \(r^\text{th}\) <dfn class="terminology">moment about the origin</dfn> of a random variable \(X\text{,}\) denoted by \(\mu_r'\text{,}\) is the expected value of \((X)^r\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_r'=E[(X)^r] = \int x^r\cdot f(x)\,dx
\end{equation*}
</div>
<p data-braille="continuation">for \(r = 0,1,2,
\dots\) when \(X\) is continuous.</p></article><article class="definition definition-like" id="product-moments-origin-4-7"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.5</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the origin.</span>
</h6>
<p id="p-227">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the origin</dfn> of the random variables \(\displaystyle X\) and \(\displaystyle Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}\text{,}\) is the expected value of \(\displaystyle X^rY^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}'=E[X^rY^s] = \sum_x\sum_y x^r y^s\cdot f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2,
\dots\) when \(\displaystyle X\) and Y are discrete.</p></article><p id="p-228">Special cases of product moments are \(\displaystyle \mu_{1,0}' =
E[X^1Y^0] = E[X] = \mu_X\) and \(\displaystyle \mu_{0,1}' = E[X^0Y^1]
= E[Y] = \mu_Y\text{.}\)</p>
<p id="p-229">As complicated as the definitions of the product moments may be, they lead to a way to define and calculate the very important concept of covariance.</p>
<ul class="disc">
<li id="li-45">If we have a high probability of large \(\displaystyle X\) paired with large \(\displaystyle Y\) and small \(\displaystyle
X\) paired with small \(\displaystyle Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \gt 0\)</li>
<li id="li-46">If we have a high probability of large \(\displaystyle X\) paired with small \(\displaystyle Y\) and small \(\displaystyle
X\) paired with large \(\displaystyle Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \lt 0\)</li>
</ul>
<article class="definition definition-like" id="product-moments-mean-4-8"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.6</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the mean.</span>
</h6>
<p id="p-230">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the mean</dfn> of the random variables \(\displaystyle X\) and \(\displaystyle Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}'\text{,}\) is the expected value of \(\displaystyle (X-\mu_X)^r(Y-\mu_Y)^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}=E[(X-\mu_X)^r(Y-\mu_Y)^s] = \sum_x\sum_y (x-\mu_X)^r
(y-\mu_Y)^s\cdot f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2, \dots\) when \(\displaystyle X\) and Y are discrete.</p></article><article class="definition definition-like" id="covariance-4-9"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.7</span><span class="period">.</span><span class="space"> </span><span class="title">covariance.</span>
</h6>
<p id="p-231">\(\displaystyle \mu_{1,1}\) is called the <dfn class="terminology">covariance</dfn> of \(\displaystyle X\) and \(\displaystyle Y\text{,}\) and it is denoted by \(\displaystyle \sigma_{XY}\) or \(\displaystyle
\operatorname{cov}(X, Y)\text{,}\) or \(\displaystyle C(X, Y)\text{.}\)</p></article><article class="theorem theorem-like" id="thm-4-11"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.8</span><span class="period">.</span><span class="space"> </span><span class="title">covariance from moments about the origin.</span>
</h6>
<p id="p-232">\(\displaystyle \sigma_{XY} = \mu_{1,1}' - \mu_X \mu_Y\)</p></article><article class="theorem theorem-like" id="thm-4-12"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.9</span><span class="period">.</span><span class="space"> </span><span class="title">independence and covariance.</span>
</h6>
<p id="p-233">If \(\displaystyle X\) and \(\displaystyle Y\) are independent, then</p>
<div class="displaymath">
\begin{equation*}
\displaystyle E[XY] = E[X]\cdot E[Y]
\end{equation*}
</div>
<p data-braille="continuation">and \(\displaystyle \sigma_{XY} = 0\text{.}\)</p></article><article class="remark remark-like" id="rmrk-4-12"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">4.2.10</span><span class="period">.</span>
</h6>In terms of moments,<div class="displaymath">
\begin{equation*}
\displaystyle\mu_{1,1}' = \mu_{1,0}'\cdot \mu_{0,1}'
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-13"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.11</span><span class="period">.</span><span class="space"> </span><span class="title">product moments of independent random variables.</span>
</h6>
<p id="p-234">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent, then \(\displaystyle E[X_1X_2\cdots X_n] = E[X_1]\cdot
E[X_2]\cdot\cdots\cdot E[X_n]\text{.}\)</p></article><p id="p-235">Independence means covariance is zero, but covariances of zero does not mean independence.</p></section><section class="exercises" id="exercises-14"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.2.3</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-29"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.41.</span>
</h6>
<p id="p-236">If \(X\) and \(Y\) have the joint probability distribution \(f(x, y) = \dfrac{1}{4}\) for \((-3, -5)\text{,}\)  \((-1, -1)\text{,}\) \((1, 1)\text{,}\)  \((3, 5)\text{,}\) find \(\operatorname{cov}(X, Y)\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-30"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.45.</span>
</h6>
<p id="p-237">If \(X\) and \(Y\) have the joint probability distribution \(f(-1, 0) = 0\text{,}\)  \(f(-1, 1) = \dfrac{1}{4}\text{,}\) \(f(0, 0) = \dfrac{1}{6}\text{,}\) \(f(1, 0) = \dfrac{1}{12}\text{,}\) \(f(1,
1) = \dfrac{1}{2}\) show that</p>
<ol class="lower-alpha">
<li id="li-47"><p id="p-238">\(\operatorname{cov}(X, Y) = 0\text{;}\)</p></li>
<li id="li-48"><p id="p-239">the two random variables are not independent.</p></li>
</ol></article></section><section class="subsection" id="sub-moments-linear-combinations"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.2.4</span> <span class="title">Moments of linear combinations of random variables</span>
</h3>
<a href="continuous-section-continuous-expectation.html#sub-moments-linear-combinations" class="permalink">¶</a><section class="introduction" id="introduction-22"><p id="p-240">See section 4.7. Recommended problems: 4.7-8 (pg 158) 48, 49, 57</p></section><article class="theorem theorem-like" id="thm-4-14"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.12</span><span class="period">.</span><span class="space"> </span><span class="title">variance.</span>
</h6>
<p id="p-241">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle
a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E[Y] = \sum_{i=1}^n
a_iE[X_i]
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n a_i^2
\operatorname{var}[X_i] + 2 \mathop{\sum \sum}_{i \lt j} a_i
a_j\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.2.13</span><span class="period">.</span><span class="space"> </span><span class="title">variance of independent random variables.</span>
</h6>
<p id="p-242">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n
a_i^2\operatorname{var}[X_i]
\end{equation*}
</div></article><article class="example example-like" id="ex-var-covar"><a data-knowl="" class="id-ref original" data-refid="hk-ex-var-covar"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.2.14</span><span class="period">.</span><span class="space"> </span><span class="title">covariances of linear combinations.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-var-covar"><article class="example example-like"><p id="p-243">Consider three random variables \(X\text{,}\) \(Y\text{,}\) and \(Z\) with \(\mu_X = 2\text{,}\) with \(\mu_Y = -3\text{,}\) with \(\mu_Z = 4\text{;}\) with \(\sigma_X^2 = 1\text{,}\) \(\sigma_Y^2 = 5\text{,}\) \(\sigma_Z^2 =
2\text{;}\) and \(\operatorname{cov}(X, Y) = -2\text{,}\) \(\operatorname{cov}(X, Z) =
-1\text{,}\) and \(\operatorname{cov}(Y, Z) = 1\text{.}\)</p>
<p id="p-244">Find \(\mu_W\) and \(\operatorname{var}(W) = \sigma_W^2\) for \(W = 3X-Y+2Z\text{.}\)</p>
<a data-knowl="" class="id-ref original" data-refid="hk-solution-13" id="solution-13"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-13"><div class="solution solution-like">
<p id="p-245">First, \(\mu_W = (3)\mu_X + (-1)\mu_Y + (2)\mu_Z = 17\text{.}\)</p>
<p id="p-246">We could apply the theorem directly, but we can do this more directly with linear algebra. The idea is that we can picture the linear combination \(W = 3X-Y+2Z\) as</p>
<div class="displaymath">
\begin{equation*}
W =
\left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}X \\Y\\
Z\end{array}\right] = (3)X + (-1)Y + (2)Z
\end{equation*}
</div>
<p id="p-247">Let \(a\) be the row vector \(a = \left[\begin{array}{ccc}3 \amp -1
\amp 2\end{array}\right]\text{,}\) its transpose be the column vector \(a^T\text{,}\) and the matrix \(\Sigma\) be defined as follows,</p>
<div class="displaymath">
\begin{equation*}
\Sigma = \left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]
\end{equation*}
</div>
<p id="p-248">This approach can be justified by expanding the sums in <a class="xref" data-knowl="./knowl/thm-4-14.html" title="Theorem 3.2.24: variance">Theorem 3.2.24</a> with a sum of 2 random variables.</p>
<p id="p-249">We can calculate the variance of \(W\) by \(\operatorname{var}(W)
= a\Sigma a^T\text{.}\) Specifically,</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]\cdot\left[\begin{array}{c}3 \\-1\\
2\end{array}\right]
\end{equation*}
</div>
<p id="p-250">Notice \(Sigma\) is symmetric and that the covariances lie in order along the main diagonal and the variances off-diagonal.</p>
<p id="p-251">Multiplying the square matrix and column vector first, we have</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}3 \\-9\\
0\end{array}\right]
\end{equation*}
</div>
<p id="p-252">And finally, \(a \Sigma a^T = 18\text{.}\)</p>
</div></div></article></div>
<article class="theorem theorem-like" id="thm-4-15"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.2.15</span><span class="period">.</span><span class="space"> </span><span class="title">covariance of two linear combinations.</span>
</h6>
<p id="p-253">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_i X_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n,
b_1, b_2, \dots, b_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1,
Y_2] = \sum_{i=1}^n a_i b_i\operatorname{var}[X_i] + \mathop{\sum
\sum}_{i \lt j} (a_ib_j + a_jb_i)\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3b"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.2.16</span><span class="period">.</span>
</h6>
<p id="p-254">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_iX_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1, Y_2] =
\sum_{i=1}^n a_i b_i\operatorname{var}[X_i]
\end{equation*}
</div></article> The same logic used in <a class="xref" data-knowl="./knowl/ex-var-covar.html" title="Example 3.2.26: covariances of linear combinations">Example 3.2.26</a> allows us to compute the covariance between two linear combinations of random variables directly also. Instead of calculating \(a\Sigma a^T\) we will calculate \(a \Sigma b^T\) where \(b\) is the vector of coefficients of the second linear combination.</section><section class="exercises" id="exercises-15"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">4.2.5</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-31"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.48.</span>
</h6>
<p id="p-255">If \(X_1\text{,}\) \(X_2\text{,}\) \(X_3\) are independent and have the means \(4, 9, 3\) and the variances \(3, 7, 5\text{,}\) find the mean and variance of  show that</p>
<ol class="lower-alpha">
<li id="li-49"><p id="p-256">\(Y = 2X_1 - 3X_2 + 4X_3\text{;}\)</p></li>
<li id="li-50"><p id="p-257">\(Z = X_1 + 2X_2 -X_3\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-32"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.49.</span>
</h6>
<p id="p-258">Repeat both parts of the previous exercise after dropping the assumption of independence and using instead that \(\operatorname{cov}(X_1, X_2) = 1\text{,}\) \(\operatorname{cov}(X_2, X_3)
= -2\text{,}\) \(\operatorname{cov}(X_1, X_3) = -3\text{.}\)</p></article></section><section class="subsection" id="sub-conditional-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.2.6</span> <span class="title">Conditional expectation</span>
</h3>
<a href="continuous-section-continuous-expectation.html#sub-conditional-expectation" class="permalink">¶</a><p id="p-259">See section 4.8.</p>
<article class="definition definition-like" id="conditional-expectation-4-10"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.17</span><span class="period">.</span><span class="space"> </span><span class="title">conditional expectation.</span>
</h6>
<p id="p-260">If \(\displaystyle X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(\displaystyle X\) given \(\displaystyle Y = y\) at \(\displaystyle x\text{,}\) the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle u(X)\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
E[u(X)|y] = \sum_x u(x)\cdot f(x|y)
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle v(Y)\) given \(\displaystyle X
= x\) is</p>
<div class="displaymath">
\begin{equation*}
E[v(Y)|x] = \sum_y v(y)\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="conditional-mean"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.18</span><span class="period">.</span><span class="space"> </span><span class="title">conditional mean.</span>
</h6>
<p id="p-261">If \(\displaystyle X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(\displaystyle X\) given \(\displaystyle Y = y\) at \(\displaystyle x\text{,}\) the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle u(X) = X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\mu_{X|y} = E[X|y] = \sum_x x\cdot f(x|y)
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle v(Y) = Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle \mu_{Y|x} = E[Y|x] = \sum_y y\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="conditional-variance"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.2.19</span><span class="period">.</span><span class="space"> </span><span class="title">conditional variance.</span>
</h6>
<p id="p-262">If \(\displaystyle X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(\displaystyle X\) given \(\displaystyle Y = y\) at \(\displaystyle x\text{,}\) the <dfn class="terminology">conditional variance</dfn> of \(\displaystyle X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\sigma^2_{X|y} = E[(X-\mu_{X|y})^2|y] = E[X^2]-\mu^2_{X|y}
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle\sigma^2_{Y|x} = E[(Y-\mu_{Y|x})^2|y] =
E[Y^2]-\mu^2_{Y|x}
\end{equation*}
</div></article></section></section></div></main>
</div>
</body>
</html>
