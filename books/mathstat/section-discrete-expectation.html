<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2020-04-20T14:07:10-05:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Mathematical expectation of discrete random variables</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
/* support for the sfrac command in MathJax (Beveled fraction) */
/* see: https://github.com/mathjax/MathJax-docs/wiki/Beveled-fraction-like-sfrac,-nicefrac-bfrac */
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  var MML = MathJax.ElementJax.mml,
      TEX = MathJax.InputJax.TeX;
  TEX.Definitions.macros.sfrac = "myBevelFraction";
  TEX.Parse.Augment({
    myBevelFraction: function (name) {
      var num = this.ParseArg(name),
          den = this.ParseArg(name);
      this.Push(MML.mfrac(num,den).With({bevelled: true}));
    }
  });
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://www.math.uco.edu" target="_blank"><img src="images/cover.png" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="mathstat.html"><span class="title">Mathematical Statistics I:</span> <span class="subtitle">Based heavily on Freund's Mathematical Statistics</span></a></h1>
<p class="byline">Sean M. Laverty</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-probability-distributions.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap-discrete.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="section-special-distributions.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-probability-distributions.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap-discrete.html" title="Up">Up</a><a class="next-button button toolbar-item" href="section-special-distributions.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="front-colophon.html" data-scroll="front-colophon">Colophon</a></li>
<li><a href="author-bio-SML.html" data-scroll="author-bio-SML">Author Biography</a></li>
<li><a href="dedication.html" data-scroll="dedication">Dedication</a></li>
<li><a href="acknowledgement.html" data-scroll="acknowledgement">Acknowledgements</a></li>
<li><a href="preface.html" data-scroll="preface">Preface</a></li>
<li><a href="contributors.html" data-scroll="contributors">Contributors to the 0\(^\mathrm{th}\) Edition</a></li>
</ul>
</li>
<li class="link">
<a href="chap-counting.html" data-scroll="chap-counting"><span class="codenumber">1</span> <span class="title">Counting</span></a><ul><li><a href="section-counting.html" data-scroll="section-counting">Counting</a></li></ul>
</li>
<li class="link">
<a href="chap-prob.html" data-scroll="chap-prob"><span class="codenumber">2</span> <span class="title">Probability</span></a><ul><li><a href="section-probability.html" data-scroll="section-probability">Probability</a></li></ul>
</li>
<li class="link">
<a href="chap-discrete.html" data-scroll="chap-discrete"><span class="codenumber">3</span> <span class="title">Discrete Random Variables</span></a><ul>
<li><a href="section-probability-distributions.html" data-scroll="section-probability-distributions">Probability distributions</a></li>
<li><a href="section-discrete-expectation.html" data-scroll="section-discrete-expectation" class="active">Mathematical expectation of discrete random variables</a></li>
<li><a href="section-special-distributions.html" data-scroll="section-special-distributions">Special probability distributions</a></li>
</ul>
</li>
<li class="link">
<a href="chap-continuous.html" data-scroll="chap-continuous"><span class="codenumber">4</span> <span class="title">Continuous Random Variables</span></a><ul>
<li><a href="section-probability-densities.html" data-scroll="section-probability-densities">Probability densities</a></li>
<li><a href="section-continuous-expectation.html" data-scroll="section-continuous-expectation">Expectation of continuous random variables</a></li>
<li><a href="section-special-densities.html" data-scroll="section-special-densities">Special probability densities</a></li>
<li><a href="sec-multi-cont.html" data-scroll="sec-multi-cont">Multivariate continuous densities</a></li>
</ul>
</li>
<li class="link"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-def.html" data-scroll="appendix-def"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thm.html" data-scroll="appendix-thm"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-ex.html" data-scroll="appendix-ex"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-gfdl.html" data-scroll="appendix-gfdl"><span class="codenumber">D</span> <span class="title">GNU Free Documentation License</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
<li class="link"><a href="back-colophon.html" data-scroll="back-colophon"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="section-discrete-expectation"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">3.2</span> <span class="title">Mathematical expectation of discrete random variables</span>
</h2>
<a href="section-discrete-expectation.html" class="permalink">¶</a><section class="introduction" id="introduction-9"><p id="p-60">This is the start of Chapter 4 in Freund's Mathematical Statistics. In the first pass we will study the major topics of this chapter with a focus on those applying to discrete random variables.</p></section><section class="subsection" id="sub-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.2.1</span> <span class="title">Expected value</span>
</h3>
<a href="section-discrete-expectation.html#sub-expectation" class="permalink">¶</a><p id="p-61">See section 4.1 - 4.2. Recommended problems: (pg 136) 7, 9, 10, 11, (pg 161) 60</p>
<article class="definition definition-like" id="def-expected-value-4-1"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.1</span><span class="period">.</span><span class="space"> </span><span class="title">expected value.</span>
</h6>
<p id="p-62">If \(X\) is a discrete random variable and \(\displaystyle f(x)\) is the value of its probability distribution at \(X\text{,}\) the <dfn class="terminology">expected value</dfn> of \(X\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[X] = \sum_x x\cdot f(x)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-expected-value-function-random-variable-4-1"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.2</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of a function of a random variable.</span>
</h6>
<p id="p-63">If \(X\) is a discrete random variable and \(\displaystyle f(x)\) is the value of its probability distribution at \(X\text{,}\) the expected value of \(\displaystyle
g(X)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X)] = \sum_x g(x)\cdot f(x)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-2"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.3</span><span class="period">.</span><span class="space"> </span><span class="title">expectation of a linear function.</span>
</h6>
<p id="p-64">If \(\displaystyle a\) and \(\displaystyle b\) are constants, then \(\displaystyle E[aX +b] = aE[X]+b\text{.}\)</p></article><article class="corollary theorem-like" id="cor-4-1"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">3.2.4</span><span class="period">.</span>
</h6>
<p id="p-65">If a is a constant, then \(\displaystyle E[aX] = aE[X]\text{.}\)</p></article><article class="corollary theorem-like" id="cor-4-2"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">3.2.5</span><span class="period">.</span>
</h6>
<p id="p-66">If b is a constant, then \(\displaystyle E[b] = b\text{.}\)</p></article><article class="theorem theorem-like" id="thm-expected-value-joint-random-variables-4-4"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.6</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of joint random variables.</span>
</h6>
<p id="p-67">If \(X\) and \(Y\) are discrete random variables and \(\displaystyle f(x, y)\) is the value of their joint probability distribution at \(\displaystyle (x, y)\text{,}\) the expected value of \(\displaystyle g(X, Y)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X, Y)] = \sum_x \sum_y g(x, y)\cdot f(x,y)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-5"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.7</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of a linear combination of random variables.</span>
</h6>
<p id="p-68">If \(\displaystyle c_1, c_2, \dots, c_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E\left[\sum_{i=1}^n c_i g_i(X_1, X_2, \dots, X_k)\right] =
\sum_{i=1}^n c_i E\left[g_i(X_1, X_2, \dots, X_k)\right]
\end{equation*}
</div></article></section><section class="exercises" id="exercises-3"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.2.2</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-13"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.9.</span>
</h6>
<p id="p-69">Suppose that \(X\) takes on values \(0, 1, 2, 3\) with probabilities \(\dfrac{1}{125}, \dfrac{12}{125}, \dfrac{48}{125},
\dfrac{64}{125}\)</p>
<ol class="lower-alpha">
<li id="li-24"><p id="p-70">Find \(E[X]\) and \(E[X^2]\)</p></li>
<li id="li-25"><p id="p-71">Determine the value of \(E[(3X + 2)^2]\)</p></li>
</ol></article></section><section class="subsection" id="sub-moments"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.2.3</span> <span class="title">Expected value</span>
</h3>
<a href="section-discrete-expectation.html#sub-moments" class="permalink">¶</a><p id="p-72">See section 4.3. Recommended problems: 4.3 (pg 146) 20, 22, 23, 31, 33, 34, 40, (pg 162) 69, 73, 75</p>
<article class="definition definition-like" id="def-moments-about-origin-4-2"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.8</span><span class="period">.</span><span class="space"> </span><span class="title">moments about the origin.</span>
</h6>
<p id="p-73">The \(\displaystyle r^\text{th}\) <dfn class="terminology">moment about the origin</dfn> of a random variable \(X\text{,}\) denoted by \(\displaystyle \mu_r'\text{,}\) is the expected value of \(\displaystyle
(X)^r\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_r'=E[(X)^r] = \sum_x x^r\cdot f(x)
\end{equation*}
</div>
<p data-braille="continuation">for \(\displaystyle r =
0,1,2, \dots\) when \(X\) is discrete.</p></article><article class="definition definition-like" id="def-mean-4-3"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.9</span><span class="period">.</span><span class="space"> </span><span class="title">mean of a discrete random variable.</span>
</h6>\(\displaystyle \mu_1'\) is called the <dfn class="terminology">mean</dfn> of the distribution of \(X\text{,}\) or simply the <dfn class="terminology">mean</dfn> of \(X\text{;}\) and it is denoted by \(\displaystyle
\mu\text{.}\)</article><article class="definition definition-like" id="def-moments-about-mean-4-4"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.10</span><span class="period">.</span><span class="space"> </span><span class="title">moments about the mean.</span>
</h6>
<p id="p-74">The \(\displaystyle r^\text{th}\) <dfn class="terminology">moment about the mean</dfn> of a random variable \(X\text{,}\) denoted by \(\displaystyle \mu_r\text{,}\) is the expected value of \(\displaystyle
(X-\mu)^r\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_r=E[(X-\mu)^r] = \sum_x (x-\mu)^r\cdot f(x)
\end{equation*}
</div>
<p data-braille="continuation">for \(\displaystyle r =
0,1,2, \dots\) when \(X\) is discrete.</p></article><p id="p-75">Now, you could imagine in some cases the moments being difficult to calculate as sums. We sometimes take the approach of building what are called moment-generating functions. These are mathematical functions whose purpose is to generate the moments of a distribution that we might need.</p></section><section class="exercises" id="exercises-4"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.2.4</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-14"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.18.</span>
</h6>
<p id="p-76">Find \(\mu\text{,}\) \(\mu_2'\text{,}\) and \(\sigma^2\) for the random variable \(X\) that has probability distribution \(f(x) = 0.5 \text{
for } x= \pm 2\text{.}\)</p></article></section><section class="subsection" id="sub-mgf"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.2.5</span> <span class="title">Moment-generating functions</span>
</h3>
<a href="section-discrete-expectation.html#sub-mgf" class="permalink">¶</a><p id="p-77">See section 4.5.</p>
<article class="definition definition-like" id="def-mgf-4-6"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.11</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function.</span>
</h6>
<p id="p-78">The <dfn class="terminology">moment-generating function</dfn> of a random variable \(X\text{,}\) where it exists, is given by</p>
<div class="displaymath">
\begin{equation*}
\displaystyle
M_X(t) = E[e^{tX}] = \sum_x e^{tx}\cdot f(x)
\end{equation*}
</div>
<p data-braille="continuation">when \(X\) is discrete.</p></article><p id="p-79">Notce that a moment-generating function \(\displaystyle M_X(t)\) itself is a function of the variable \(\displaystyle t\) not \(X\text{.}\) As it turns out, we are most interested in values of the function at or near \(\displaystyle t=0\text{.}\)</p>
<article class="example example-like" id="ex-maclaurin"><a data-knowl="" class="id-ref original" data-refid="hk-ex-maclaurin"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.2.12</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function via Taylor series.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-maclaurin"><article class="example example-like"><p id="p-80">Recall that the Maclaurin series (Taylor series around zero) for \(\displaystyle e^{tx}\) is</p>
<div class="displaymath">
\begin{equation*}
e^{tx} = 1 + tx +
\frac{1}{2!}\left(tx\right)^2 + \frac{1}{3!}\left(tx\right)^3 + \cdots +
\frac{1}{r!}\left(tx\right)^r + \cdots
\end{equation*}
</div>
<p data-braille="continuation">This means (in the discrete case), that</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
M_X(t) \amp = \sum_x e^{tx} f(x)\\
\amp = \sum_x \left(1 + tx + \frac{1}{2!}(tx)^2 +
\frac{1}{3!}\left(tx\right)^3 + \cdots + \frac{1}{r!}\left(tx\right)^r +
\cdots\right) f(x)\\
\amp = \sum_x (f(x) + txf(x) + \frac{t^2}{2!}x^2f(x) +
\frac{t^3}{3!}x^3f(x) + \cdots + \frac{t^r}{r!}x^rf(x) + \cdots)\\
\amp = \sum_xf(x) + t\sum_x xf(x) + \frac{t^2}{2!}\sum_x x^2f(x) +
\frac{t^3}{3!}\sum_x x^3f(x) + \cdots + \frac{t^r}{r!}\sum_x x^rf(x) +
\cdots
\end{aligned}
\end{equation*}
</div>
<p data-braille="continuation">Looking closely, at</p>
<div class="displaymath">
\begin{equation*}
M_X(t) = \sum_xf(x) + \left(\sum_x xf(x)\right)t
+ \left(\sum_x x^2f(x)\right)\frac{t^2}{2!} + \left(\sum_x
x^3f(x)\right)\frac{t^3}{3!} + \cdots + \left(\sum_x
x^rf(x)\right)\frac{t^r}{r!} + \cdots
\end{equation*}
</div>
<p data-braille="continuation">coefficients of the terms \(\displaystyle \dfrac{t^r}{r!}\) are the moments about the origin \(\displaystyle \mu_r' = \sum_x x^r f(x)\)</p></article></div>
<article class="example example-like" id="ex-three-cards-mgf"><a data-knowl="" class="id-ref original" data-refid="hk-ex-three-cards-mgf"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.2.13</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function for three cards.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-three-cards-mgf"><article class="example example-like"><p id="p-81">Recall the probability distribution \(f(x) = P(X = x) =
\dfrac{{3\choose x}}{8} \text{ for } x = 0, 1, 2, 3\) (this was used earlier to determine the probabilities of \(x\) heads on three flips of a coin).</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
M_X(t) \amp = \sum_x e^{tx} f(x)\\
\amp = 1 \cdot \frac{1}{8} + e^{t} \cdot \frac{3}{8}+ e^{2t} \cdot
\frac{3}{8} + e^{3t} \cdot \frac{1}{8}\\
\amp = \frac{1}{8} \left(1 + 3e^{t} + 3e^{2t} + e^{3t}\right)\\
M_X(t) \amp = \frac{1}{8} (1+e^t)^3\\
\end{aligned}
\end{equation*}
</div></article></div>
<article class="theorem theorem-like" id="thm-4-9"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.14</span><span class="period">.</span><span class="space"> </span><span class="title">moments via differentiation.</span>
</h6>
<p id="p-82">The \(r^\text{th}\) moment about the origin, \(\mu_r'\text{,}\) can be written</p>
<div class="displaymath">
\begin{equation*}
\displaystyle \mu_r' = \dfrac{d^rM_X(t)}{dt^r}\Big|_{t=0}
\end{equation*}
</div></article><article class="example example-like" id="ex-three-cards-mgf2"><a data-knowl="" class="id-ref original" data-refid="hk-ex-three-cards-mgf2"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.2.15</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function for three cards, via differentiation.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-three-cards-mgf2"><article class="example example-like"><p id="p-83">Referencing Example <a class="xref" data-knowl="./knowl/ex-three-cards-mgf.html" title="Example 3.2.13: moment-generating function for three cards">Example 3.2.13</a>, the mean of the random variable, given by \(\mu_1'\text{,}\) whose MGF is \(M_X(t) =
\frac{1}{8} (1+e^t)^3\) is found as follows</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
\mu_1' \amp = \left(\dfrac{d}{dt}\left(\frac{1}{8}
(1+e^t)^3\right)\right)\Big|_{t=0}\\
\amp = \left(\frac{3}{8} (1+e^t)^2e^t\right)\Big|_{t=0}\\
\amp = \left(\frac{3}{8} (2)^2\right)\\
\mu_1' \amp = \frac{3}{2}
\end{aligned}
\end{equation*}
</div></article></div>
<p id="p-84">So, <em class="emphasis">given</em> a moment-generating function, a relatively simple application of calculus allows us to replace a more tedious calculation of the moment from its definition.</p>
<article class="theorem theorem-like" id="thm-4-10"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.16</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function of functions of a random variable.</span>
</h6>
<p id="p-85">If \(\displaystyle a\) and \(\displaystyle b\) are constants, then</p>
<ol class="decimal">
<li id="li-26">\(\displaystyle M_{X+a}(t) = E[e^{(X+a)t}] = e^{at} \cdot
M_X(t)\text{;}\)</li>
<li id="li-27">\(\displaystyle M_{bX}(t) = E[e^{bXt}] = M_X(bt)\text{;}\)</li>
<li id="li-28">\(\displaystyle M_{\frac{X+a}{b}}(t) =
E\left[e^{\left(\frac{X+a}{b}\right)t}\right] = e^{(a/b)t} \cdot
M_X\left(\frac{t}{b}\right)\text{;}\)</li>
</ol></article><p id="p-86">The rules in <a class="xref" data-knowl="./knowl/thm-4-10.html" title="Theorem 3.2.16: moment-generating function of functions of a random variable">Theorem 3.2.16</a> allow us to calculate moment-generating functions of simple functions of a random variable.</p></section><section class="exercises" id="exercises-5"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.2.6</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-15"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.33.</span>
</h6>
<p id="p-87">Find the moment-generating function of the discrete random variable \(X\) that has the probability distribution</p>
<div class="displaymath">
\begin{equation*}
f(x) =
2\left(\dfrac{1}{3}\right)^x \text{ for } x = 1, 2, 3, \dots
\end{equation*}
</div>
<p data-braille="continuation">and use it to determine the values of \(\mu_1'\) and \(\mu_2'\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-16"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.40.</span>
</h6>
<p id="p-88">Given the moment-generating function \(X_X(t) = e^{3t+8t^2}\text{,}\) find the moment generating function of the random variable \(Z =
\dfrac{1}{4}\left(X-3\right)\) and use it to determine the mean and variance of \(Z\text{.}\)</p></article></section><section class="subsection" id="sub-product-moments"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.2.7</span> <span class="title">Product moments</span>
</h3>
<a href="section-discrete-expectation.html#sub-product-moments" class="permalink">¶</a><p id="p-89">See section 4.6.</p>
<article class="definition definition-like" id="def-product-moments-origin-4-7"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.17</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the origin.</span>
</h6>
<p id="p-90">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the origin</dfn> of the random variables \(X\) and \(Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}\text{,}\) is the expected value of \(\displaystyle X^rY^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}'=E[X^rY^s] = \sum_x\sum_y x^r y^s\cdot f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2,
\dots\) when \(X\) and Y are discrete.</p></article><p id="p-91">Special cases of product moments are \(\displaystyle \mu_{1,0}' =
E[X^1Y^0] = E[X] = \mu_X\) and \(\displaystyle \mu_{0,1}' = E[X^0Y^1]
= E[Y] = \mu_Y\text{.}\)</p>
<p id="p-92">As complicated as the definitions of the product moments may be, they lead to a way to define and calculate the very important concept of covariance.</p>
<ul class="disc">
<li id="li-29">If we have a high probability of large \(X\) paired with large \(Y\) and small \(\displaystyle
X\) paired with small \(Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \gt 0\)</li>
<li id="li-30">If we have a high probability of large \(X\) paired with small \(Y\) and small \(\displaystyle
X\) paired with large \(Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \lt 0\)</li>
</ul>
<article class="definition definition-like" id="def-product-moments-mean-4-8"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.18</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the mean.</span>
</h6>
<p id="p-93">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the mean</dfn> of the random variables \(X\) and \(Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}'\text{,}\) is the expected value of \(\displaystyle (X-\mu_X)^r(Y-\mu_Y)^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}=E[(X-\mu_X)^r(Y-\mu_Y)^s] = \sum_x\sum_y (x-\mu_X)^r
(y-\mu_Y)^s\cdot f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2, \dots\) when \(X\) and Y are discrete.</p></article><article class="definition definition-like" id="def-covariance-4-9"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.19</span><span class="period">.</span><span class="space"> </span><span class="title">covariance.</span>
</h6>
<p id="p-94">\(\displaystyle \mu_{1,1}\) is called the <dfn class="terminology">covariance</dfn> of \(X\) and \(Y\text{,}\) and it is denoted by \(\displaystyle \sigma_{XY}\) or \(\displaystyle
\operatorname{cov}(X, Y)\text{,}\) or \(\displaystyle C(X, Y)\text{.}\)</p></article><article class="theorem theorem-like" id="thm-4-11"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.20</span><span class="period">.</span><span class="space"> </span><span class="title">covariance from moments about the origin.</span>
</h6>
<p id="p-95">\(\displaystyle \sigma_{XY} = \mu_{1,1}' - \mu_X \mu_Y\)</p></article><article class="theorem theorem-like" id="thm-4-12"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.21</span><span class="period">.</span><span class="space"> </span><span class="title">independence and covariance.</span>
</h6>
<p id="p-96">If \(X\) and \(Y\) are independent, then</p>
<div class="displaymath">
\begin{equation*}
\displaystyle E[XY] = E[X]\cdot E[Y]
\end{equation*}
</div>
<p data-braille="continuation">and \(\displaystyle \sigma_{XY} = 0\text{.}\)</p></article><article class="remark remark-like" id="rmrk-4-12"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">3.2.22</span><span class="period">.</span>
</h6>In terms of moments,<div class="displaymath">
\begin{equation*}
\displaystyle\mu_{1,1}' = \mu_{1,0}'\cdot \mu_{0,1}'
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-13"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.23</span><span class="period">.</span><span class="space"> </span><span class="title">product moments of independent random variables.</span>
</h6>
<p id="p-97">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent, then \(\displaystyle E[X_1X_2\cdots X_n] = E[X_1]\cdot
E[X_2]\cdot\cdots\cdot E[X_n]\text{.}\)</p></article><p id="p-98">Independence means covariance is zero, but covariances of zero does not mean independence.</p></section><section class="exercises" id="exercises-6"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.2.8</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-17"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.41.</span>
</h6>
<p id="p-99">If \(X\) and \(Y\) have the joint probability distribution \(f(x, y) = \dfrac{1}{4}\) for \((-3, -5)\text{,}\)  \((-1, -1)\text{,}\) \((1, 1)\text{,}\)  \((3, 5)\text{,}\) find \(\operatorname{cov}(X, Y)\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-18"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.45.</span>
</h6>
<p id="p-100">If \(X\) and \(Y\) have the joint probability distribution \(f(-1, 0) = 0\text{,}\)  \(f(-1, 1) = \dfrac{1}{4}\text{,}\) \(f(0, 0) = \dfrac{1}{6}\text{,}\) \(f(1, 0) = \dfrac{1}{12}\text{,}\) \(f(1,
1) = \dfrac{1}{2}\) show that</p>
<ol class="lower-alpha">
<li id="li-31"><p id="p-101">\(\operatorname{cov}(X, Y) = 0\text{;}\)</p></li>
<li id="li-32"><p id="p-102">the two random variables are not independent.</p></li>
</ol></article></section><section class="subsection" id="sub-moments-linear-combinations"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.2.9</span> <span class="title">Moments of linear combinations of random variables</span>
</h3>
<a href="section-discrete-expectation.html#sub-moments-linear-combinations" class="permalink">¶</a><section class="introduction" id="introduction-10"><p id="p-103">See section 4.7. Recommended problems: 4.7-8 (pg 158) 48, 49, 57</p></section><article class="theorem theorem-like" id="thm-4-14"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.24</span><span class="period">.</span><span class="space"> </span><span class="title">variance.</span>
</h6>
<p id="p-104">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle
a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E[Y] = \sum_{i=1}^n
a_iE[X_i]
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n a_i^2
\operatorname{var}[X_i] + 2 \mathop{\sum \sum}_{i \lt j} a_i
a_j\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">3.2.25</span><span class="period">.</span><span class="space"> </span><span class="title">variance of independent random variables.</span>
</h6>
<p id="p-105">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n
a_i^2\operatorname{var}[X_i]
\end{equation*}
</div></article><article class="example example-like" id="ex-var-covar"><a data-knowl="" class="id-ref original" data-refid="hk-ex-var-covar"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.2.26</span><span class="period">.</span><span class="space"> </span><span class="title">covariances of linear combinations.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-var-covar"><article class="example example-like"><p id="p-106">Consider three random variables \(X\text{,}\) \(Y\text{,}\) and \(Z\) with \(\mu_X = 2\text{,}\) with \(\mu_Y = -3\text{,}\) with \(\mu_Z = 4\text{;}\) with \(\sigma_X^2 = 1\text{,}\) \(\sigma_Y^2 = 5\text{,}\) \(\sigma_Z^2 =
2\text{;}\) and \(\operatorname{cov}(X, Y) = -2\text{,}\) \(\operatorname{cov}(X, Z) =
-1\text{,}\) and \(\operatorname{cov}(Y, Z) = 1\text{.}\)</p>
<p id="p-107">Find \(\mu_W\) and \(\operatorname{var}(W) = \sigma_W^2\) for \(W = 3X-Y+2Z\text{.}\)</p>
<a data-knowl="" class="id-ref original" data-refid="hk-solution-1" id="solution-1"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-1"><div class="solution solution-like">
<p id="p-108">First, \(\mu_W = (3)\mu_X + (-1)\mu_Y + (2)\mu_Z = 17\text{.}\)</p>
<p id="p-109">We could apply the theorem directly, but we can do this more directly with linear algebra. The idea is that we can picture the linear combination \(W = 3X-Y+2Z\) as</p>
<div class="displaymath">
\begin{equation*}
W =
\left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}X \\Y\\
Z\end{array}\right] = (3)X + (-1)Y + (2)Z
\end{equation*}
</div>
<p id="p-110">Let \(a\) be the row vector \(a = \left[\begin{array}{ccc}3 \amp -1
\amp 2\end{array}\right]\text{,}\) its transpose be the column vector \(a^T\text{,}\) and the matrix \(\Sigma\) be defined as follows,</p>
<div class="displaymath">
\begin{equation*}
\Sigma = \left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]
\end{equation*}
</div>
<p id="p-111">This approach can be justified by expanding the sums in <a class="xref" data-knowl="./knowl/thm-4-14.html" title="Theorem 3.2.24: variance">Theorem 3.2.24</a> with a sum of 2 random variables.</p>
<p id="p-112">We can calculate the variance of \(W\) by \(\operatorname{var}(W)
= a\Sigma a^T\text{.}\) Specifically,</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]\cdot\left[\begin{array}{c}3 \\-1\\
2\end{array}\right]
\end{equation*}
</div>
<p id="p-113">Notice \(Sigma\) is symmetric and that the covariances lie in order along the main diagonal and the variances off-diagonal.</p>
<p id="p-114">Multiplying the square matrix and column vector first, we have</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}3 \\-9\\
0\end{array}\right]
\end{equation*}
</div>
<p id="p-115">And finally, \(a \Sigma a^T = 18\text{.}\)</p>
</div></div></article></div>
<article class="theorem theorem-like" id="thm-4-15"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.2.27</span><span class="period">.</span><span class="space"> </span><span class="title">covariance of two linear combinations.</span>
</h6>
<p id="p-116">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_i X_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n,
b_1, b_2, \dots, b_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1,
Y_2] = \sum_{i=1}^n a_i b_i\operatorname{var}[X_i] + \mathop{\sum
\sum}_{i \lt j} (a_ib_j + a_jb_i)\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3b"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">3.2.28</span><span class="period">.</span>
</h6>
<p id="p-117">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_iX_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1, Y_2] =
\sum_{i=1}^n a_i b_i\operatorname{var}[X_i]
\end{equation*}
</div></article> The same logic used in <a class="xref" data-knowl="./knowl/ex-var-covar.html" title="Example 3.2.26: covariances of linear combinations">Example 3.2.26</a> allows us to compute the covariance between two linear combinations of random variables directly also. Instead of calculating \(a\Sigma a^T\) we will calculate \(a \Sigma b^T\) where \(b\) is the vector of coefficients of the second linear combination.</section><section class="exercises" id="exercises-7"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.2.10</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-19"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.48.</span>
</h6>
<p id="p-118">If \(X_1\text{,}\) \(X_2\text{,}\) \(X_3\) are independent and have the means \(4, 9, 3\) and the variances \(3, 7, 5\text{,}\) find the mean and variance of  show that</p>
<ol class="lower-alpha">
<li id="li-33"><p id="p-119">\(Y = 2X_1 - 3X_2 + 4X_3\text{;}\)</p></li>
<li id="li-34"><p id="p-120">\(Z = X_1 + 2X_2 -X_3\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-20"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.49.</span>
</h6>
<p id="p-121">Repeat both parts of the previous exercise after dropping the assumption of independence and using instead that \(\operatorname{cov}(X_1, X_2) = 1\text{,}\) \(\operatorname{cov}(X_2, X_3)
= -2\text{,}\) \(\operatorname{cov}(X_1, X_3) = -3\text{.}\)</p></article></section><section class="subsection" id="sub-conditional-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.2.11</span> <span class="title">Conditional expectation</span>
</h3>
<a href="section-discrete-expectation.html#sub-conditional-expectation" class="permalink">¶</a><p id="p-122">See section 4.8.</p>
<article class="definition definition-like" id="def-conditional-expectation-4-10"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.29</span><span class="period">.</span><span class="space"> </span><span class="title">conditional expectation.</span>
</h6>
<p id="p-123">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle u(X)\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
E[u(X)|y] = \sum_x u(x)\cdot f(x|y)
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle v(Y)\) given \(\displaystyle X
= x\) is</p>
<div class="displaymath">
\begin{equation*}
E[v(Y)|x] = \sum_y v(y)\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="def-conditional-mean"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.30</span><span class="period">.</span><span class="space"> </span><span class="title">conditional mean.</span>
</h6>
<p id="p-124">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle u(X) = X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\mu_{X|y} = E[X|y] = \sum_x x\cdot f(x|y)
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle v(Y) = Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle \mu_{Y|x} = E[Y|x] = \sum_y y\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="def-conditional-variance"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.2.31</span><span class="period">.</span><span class="space"> </span><span class="title">conditional variance.</span>
</h6>
<p id="p-125">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional variance</dfn> of \(X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\sigma^2_{X|y} = E[(X-\mu_{X|y})^2|y] = E[X^2]-\mu^2_{X|y}
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle\sigma^2_{Y|x} = E[(Y-\mu_{Y|x})^2|y] =
E[Y^2]-\mu^2_{Y|x}
\end{equation*}
</div></article></section></section></div></main>
</div>
</body>
</html>
