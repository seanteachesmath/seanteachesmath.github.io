<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-04-13T16:24:12-05:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Multivariate discrete random variables</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
/* support for the sfrac command in MathJax (Beveled fraction) */
/* see: https://github.com/mathjax/MathJax-docs/wiki/Beveled-fraction-like-sfrac,-nicefrac-bfrac */
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  var MML = MathJax.ElementJax.mml,
      TEX = MathJax.InputJax.TeX;
  TEX.Definitions.macros.sfrac = "myBevelFraction";
  TEX.Parse.Augment({
    myBevelFraction: function (name) {
      var num = this.ParseArg(name),
          den = this.ParseArg(name);
      this.Push(MML.mfrac(num,den).With({bevelled: true}));
    }
  });
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" class="hidden-content" style="display:none">\(
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://www.math.uco.edu" target="_blank"><img src="images/cover.png" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="mathstat.html"><span class="title">Mathematical Statistics I:</span> <span class="subtitle">Based on course notes developed using Freund's Mathematical Statistics</span></a></h1>
<p class="byline">Sean M. Laverty</p>
</div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap-multi.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap-multi.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="sec-multi-cont.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap-multi.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap-multi.html" title="Up">Up</a><a class="next-button button toolbar-item" href="sec-multi-cont.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="front-colophon.html" data-scroll="front-colophon">Colophon</a></li>
<li><a href="author-bio-SML.html" data-scroll="author-bio-SML">Author Biography</a></li>
<li><a href="dedication.html" data-scroll="dedication">Dedication</a></li>
<li><a href="acknowledgement.html" data-scroll="acknowledgement">Acknowledgements</a></li>
<li><a href="preface.html" data-scroll="preface">Preface</a></li>
<li><a href="contributors.html" data-scroll="contributors">Contributors to the 0\(^\mathrm{th}\) Edition</a></li>
</ul>
</li>
<li class="link">
<a href="chap-counting.html" data-scroll="chap-counting"><span class="codenumber">1</span> <span class="title">Counting</span></a><ul><li><a href="section-counting.html" data-scroll="section-counting">Counting</a></li></ul>
</li>
<li class="link">
<a href="chap-prob.html" data-scroll="chap-prob"><span class="codenumber">2</span> <span class="title">Probability</span></a><ul><li><a href="section-probability.html" data-scroll="section-probability">Probability</a></li></ul>
</li>
<li class="link">
<a href="chap-discrete.html" data-scroll="chap-discrete"><span class="codenumber">3</span> <span class="title">Discrete Random Variables</span></a><ul>
<li><a href="section-probability-distributions.html" data-scroll="section-probability-distributions">Probability distributions</a></li>
<li><a href="section-discrete-expectation.html" data-scroll="section-discrete-expectation">Mathematical expectation of discrete random variables</a></li>
<li><a href="section-special-distributions.html" data-scroll="section-special-distributions">Special probability distributions</a></li>
</ul>
</li>
<li class="link">
<a href="chap-continuous.html" data-scroll="chap-continuous"><span class="codenumber">4</span> <span class="title">Continuous Random Variables</span></a><ul>
<li><a href="section-probability-densities.html" data-scroll="section-probability-densities">Probability densities</a></li>
<li><a href="section-continuous-expectation.html" data-scroll="section-continuous-expectation">Expectation of continuous random variables</a></li>
<li><a href="section-special-densities.html" data-scroll="section-special-densities">Special probability densities</a></li>
</ul>
</li>
<li class="link">
<a href="chap-multi.html" data-scroll="chap-multi"><span class="codenumber">5</span> <span class="title">Multivariate Probability Distributions and Densities</span></a><ul>
<li><a href="section-multivariate-discrete-probability.html" data-scroll="section-multivariate-discrete-probability" class="active">Multivariate discrete random variables</a></li>
<li><a href="sec-multi-cont.html" data-scroll="sec-multi-cont">Multivariate continuous random variables</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-def.html" data-scroll="appendix-def"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thm.html" data-scroll="appendix-thm"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-ex.html" data-scroll="appendix-ex"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-gfdl.html" data-scroll="appendix-gfdl"><span class="codenumber">D</span> <span class="title">GNU Free Documentation License</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
<li class="link"><a href="back-colophon.html" data-scroll="back-colophon"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="section-multivariate-discrete-probability"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">5.1</span> <span class="title">Multivariate discrete random variables</span>
</h2>
<section class="introduction" id="introduction-24"><p id="p-304">Some words.</p></section><section class="subsection" id="sub-multi-variable"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.1</span> <span class="title">Multivariate, marginal, and conditional distributions</span>
</h3>
<section class="introduction" id="introduction-25"><p id="p-305">See sections 3.5, 3.6, 3.7. Recommended problems: (pg 101) 42, 44, 45, 49, 53, 54</p></section><article class="definition definition-like" id="def-joint-probability-distribution-3-6"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.1</span><span class="period">.</span><span class="space"> </span><span class="title">joint probability distribution.</span>
</h6>
<p id="p-306">If \(\displaystyle X\) and \(\displaystyle Y\) are discrete random variables, the function given by \(\displaystyle f(x,y) = P(X=x,
Y=y)\) for each pair of values \(\displaystyle (x,y)\) within the range of \(\displaystyle X\) and \(\displaystyle Y\) is called the <dfn class="terminology">joint probability distribution</dfn> of \(\displaystyle X\) and \(\displaystyle Y\text{.}\)</p></article><article class="theorem theorem-like" id="thm-3-7"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.2</span><span class="period">.</span><span class="space"> </span><span class="title">conditions for a joint probability distribution.</span>
</h6>
<p id="p-307">A bivariate function can serve as a joint probability distribution for a pair of discrete random variables \(\displaystyle X\) and \(\displaystyle Y\) if and only if its values, \(\displaystyle f(x,
y)\text{,}\) satisfy the conditions</p>
<ol class="decimal">
<li id="li-34">\(\displaystyle f(x, y) \ge 0\) for each pair of values \(\displaystyle (x, y)\) within its domain;</li>
<li id="li-35">\(\displaystyle \sum_x\sum_y f(x, y) = 1\) where the double summation extends over all possible pairs \(\displaystyle (x,
y)\text{.}\)</li>
</ol></article><article class="definition definition-like" id="def-joint-distribution-function-3-7"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.3</span><span class="period">.</span><span class="space"> </span><span class="title">joint distribution function.</span>
</h6>
<p id="p-308">If \(\displaystyle X\) and \(\displaystyle Y\) are discrete random variables, the function given by</p>
<div class="displaymath">
\begin{equation*}
F(x, y) = P(X \le x, Y \le y) =
\sum_{s\le x} \sum_{t\le y} f(s, t) \text{ for } \infty \lt x, y \lt
\infty
\end{equation*}
</div>
<p data-braille="continuation">where \(\displaystyle f(s, t)\) is the value of the joint probability distribution of \(\displaystyle X\) and \(\displaystyle
Y\) at \(\displaystyle (s, t)\text{,}\) is called the <dfn class="terminology">joint distribution function</dfn> or <dfn class="terminology">joint cumulative distribution</dfn> of \(\displaystyle X\) and \(\displaystyle
Y\text{.}\)</p></article><article class="definition definition-like" id="def-marginal-distribution-3-10"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.4</span><span class="period">.</span><span class="space"> </span><span class="title">marginal distribution.</span>
</h6>
<p id="p-309">If \(\displaystyle X\) and \(\displaystyle Y\) are discrete random variables and \(\displaystyle f(x, y)\) is the value of their joint probability distribution at \(\displaystyle (x, y)\text{,}\) the function given by</p>
<div class="displaymath">
\begin{equation*}
g(x) = \sum_y f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">for each \(\displaystyle x\) within the range of \(\displaystyle X\) is called the <dfn class="terminology">marginal distribution</dfn> of \(\displaystyle X\text{.}\) Correspondingly, the function given by</p>
<div class="displaymath">
\begin{equation*}
h(y) = \sum_x f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">for each \(\displaystyle y\) within the range of \(\displaystyle Y\) is called the <dfn class="terminology">marginal distribution</dfn> of \(\displaystyle Y\text{.}\)</p></article><article class="definition definition-like" id="def-conditional-distribution-3-10"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.5</span><span class="period">.</span><span class="space"> </span><span class="title">conditional distribution.</span>
</h6>
<p id="p-310"><dfn class="terminology">conditional distribution</dfn></p>
<div class="displaymath">
\begin{equation*}
f(x|y) = \dfrac{f(x, y)}{h(y)}, h(y)\ne 0
\end{equation*}
</div>
<div class="displaymath">
\begin{equation*}
w(y|x) = \dfrac{f(x, y)}{g(x)}, g(x)\ne 0
\end{equation*}
</div></article></section><section class="exercises" id="exercises-20"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">5.1.2</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-40"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 3.42.</span>
</h6>
<p id="p-311">If the values of the joint probability distribution of \​(X\) and \​(Y\) are shown below,</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
P[x=0, y=0] \amp = \frac{1}{12}\\
P[x=1, y=0] \amp = \frac{1}{6}\\
P[x=2, y=0] \amp = \frac{1}{24}\\
P[x=0, y=1] \amp = \frac{1}{4}\\
P[x=1, y=1] \amp = \frac{1}{4}\\
P[x=2, y=1] \amp = \frac{1}{40}\\
P[x=0, y=2] \amp = \frac{1}{8}\\
P[x=1, y=2] \amp = \frac{1}{20}\\
P[x=0, y=3] \amp = \frac{1}{120}\\
\end{aligned}
\end{equation*}
</div>
<p data-braille="continuation">find</p>
<ol class="lower-alpha">
<li id="li-36"><p id="p-312">(b) \(P(X = 0, 1\le Y \lt 3)\)</p></li>
<li id="li-37"><p id="p-313">(c) \(P(X + Y \le 1)\)</p></li>
<li id="li-38"><p id="p-314">(d) \(P(X \gt Y)\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-41"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 3.44.</span>
</h6>
<p id="p-315">If the joint probability distribution of \(X\) and \(Y\) is given by</p>
<div class="displaymath">
\begin{equation*}
f(x, y) = c(x^2+y^2) \text{ for } x=0, 3; y=0, 1, 2
\end{equation*}
</div>
<p data-braille="continuation">find the value of \(c\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-42"><h6 class="heading">
<span class="codenumber">3<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 3.45.</span>
</h6>
<p id="p-316">With references to the previous problem find</p>
<ol class="lower-alpha">
<li id="li-39"><p id="p-317">\(\displaystyle P(X\le 1, Y \gt 2)\)</p></li>
<li id="li-40"><p id="p-318">\(\displaystyle P(X=0, Y\le 2)\)</p></li>
<li id="li-41"><p id="p-319">\(\displaystyle P(X +Y \gt 2)\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-43"><h6 class="heading">
<span class="codenumber">4<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 3.70.</span>
</h6>
<p id="p-320">With reference to 3.42, find</p>
<ol class="lower-alpha">
<li id="li-42"><p id="p-321">the marginal distribution of \(X\)</p></li>
<li id="li-43"><p id="p-322">the marginal distribution of \(X\)</p></li>
<li id="li-44"><p id="p-323">the conditional distribution of \(X\) given \(Y=1\)</p></li>
<li id="li-45"><p id="p-324">the conditional distribution of \(Y\) given \(X=0\)</p></li>
</ol></article></section><section class="subsection" id="sub-expectation-multi"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.3</span> <span class="title"></span>
</h3>
<article class="theorem theorem-like" id="thm-expected-value-joint-random-variables-4-4"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.6</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of joint random variables.</span>
</h6>
<p id="p-325">If \(X\) and \(Y\) are discrete random variables and \(\displaystyle f(x, y)\) is the value of their joint probability distribution at \(\displaystyle (x, y)\text{,}\) the expected value of \(\displaystyle g(X, Y)\) is given by</p>
<div class="displaymath">
\begin{equation*}
E[g(X, Y)] = \sum_x \sum_y g(x, y)\cdot f(x,y)
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-5"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.7</span><span class="period">.</span><span class="space"> </span><span class="title">expected value of a linear combination of random variables.</span>
</h6>
<p id="p-326">If \(\displaystyle c_1, c_2, \dots, c_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E\left[\sum_{i=1}^n c_i g_i(X_1, X_2, \dots, X_k)\right] =
\sum_{i=1}^n c_i E\left[g_i(X_1, X_2, \dots, X_k)\right]
\end{equation*}
</div></article></section><section class="subsection" id="sub-product-moments"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.4</span> <span class="title">Product moments</span>
</h3>
<p id="p-327">See section 4.6.</p>
<article class="definition definition-like" id="def-product-moments-origin-4-7"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.8</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the origin.</span>
</h6>
<p id="p-328">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the origin</dfn> of the random variables \(X\) and \(Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}\text{,}\) is the expected value of \(\displaystyle X^rY^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}'=E[X^rY^s] = \sum_x\sum_y x^r y^s\cdot f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2,
\dots\) when \(X\) and Y are discrete.</p></article><p id="p-329">Special cases of product moments are \(\displaystyle \mu_{1,0}' =
E[X^1Y^0] = E[X] = \mu_X\) and \(\displaystyle \mu_{0,1}' = E[X^0Y^1]
= E[Y] = \mu_Y\text{.}\)</p>
<p id="p-330">As complicated as the definitions of the product moments may be, they lead to a way to define and calculate the very important concept of covariance.</p>
<ul class="disc">
<li id="li-46">If we have a high probability of large \(X\) paired with large \(Y\) and small \(\displaystyle
X\) paired with small \(Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \gt 0\)</li>
<li id="li-47">If we have a high probability of large \(X\) paired with small \(Y\) and small \(\displaystyle
X\) paired with large \(Y\text{,}\) \(\displaystyle
\operatorname{cov}(X,Y) \lt 0\)</li>
</ul>
<article class="definition definition-like" id="def-product-moments-mean-4-8"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.9</span><span class="period">.</span><span class="space"> </span><span class="title">product moments about the mean.</span>
</h6>
<p id="p-331">The <dfn class="terminology">\(\displaystyle r^\text{th}\) and \(\displaystyle
s^\text{th}\) product moment about the mean</dfn> of the random variables \(X\) and \(Y\text{,}\) denoted by \(\displaystyle \mu_{r,s}'\text{,}\) is the expected value of \(\displaystyle (X-\mu_X)^r(Y-\mu_Y)^s\text{;}\) symbolically</p>
<div class="displaymath">
\begin{equation*}
\mu_{r,s}=E[(X-\mu_X)^r(Y-\mu_Y)^s] = \sum_x\sum_y (x-\mu_X)^r
(y-\mu_Y)^s\cdot f(x, y)
\end{equation*}
</div>
<p data-braille="continuation">\(\displaystyle r = 0,1,2, \dots\) and \(\displaystyle s = 0,1,2, \dots\) when \(X\) and Y are discrete.</p></article><article class="definition definition-like" id="def-covariance-4-9"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.10</span><span class="period">.</span><span class="space"> </span><span class="title">covariance.</span>
</h6>
<p id="p-332">\(\displaystyle \mu_{1,1}\) is called the <dfn class="terminology">covariance</dfn> of \(X\) and \(Y\text{,}\) and it is denoted by \(\displaystyle \sigma_{XY}\) or \(\displaystyle
\operatorname{cov}(X, Y)\text{,}\) or \(\displaystyle C(X, Y)\text{.}\)</p></article><article class="theorem theorem-like" id="thm-4-11"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.11</span><span class="period">.</span><span class="space"> </span><span class="title">covariance from moments about the origin.</span>
</h6>
<p id="p-333">\(\displaystyle \sigma_{XY} = \mu_{1,1}' - \mu_X \mu_Y\)</p></article><article class="theorem theorem-like" id="thm-4-12"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.12</span><span class="period">.</span><span class="space"> </span><span class="title">independence and covariance.</span>
</h6>
<p id="p-334">If \(X\) and \(Y\) are independent, then</p>
<div class="displaymath">
\begin{equation*}
\displaystyle E[XY] = E[X]\cdot E[Y]
\end{equation*}
</div>
<p data-braille="continuation">and \(\displaystyle \sigma_{XY} = 0\text{.}\)</p></article><article class="remark remark-like" id="rmrk-4-12"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">5.1.13</span><span class="period">.</span>
</h6>In terms of moments,<div class="displaymath">
\begin{equation*}
\displaystyle\mu_{1,1}' = \mu_{1,0}'\cdot \mu_{0,1}'
\end{equation*}
</div></article><article class="theorem theorem-like" id="thm-4-13"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.14</span><span class="period">.</span><span class="space"> </span><span class="title">product moments of independent random variables.</span>
</h6>
<p id="p-335">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent, then \(\displaystyle E[X_1X_2\cdots X_n] = E[X_1]\cdot
E[X_2]\cdot\cdots\cdot E[X_n]\text{.}\)</p></article><p id="p-336">Independence means covariance is zero, but covariances of zero does not mean independence.</p></section><section class="exercises" id="exercises-21"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">5.1.5</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-44"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.41.</span>
</h6>
<p id="p-337">If \(X\) and \(Y\) have the joint probability distribution \(f(x, y) = \dfrac{1}{4}\) for \((-3, -5)\text{,}\)  \((-1, -1)\text{,}\) \((1, 1)\text{,}\)  \((3, 5)\text{,}\) find \(\operatorname{cov}(X, Y)\text{.}\)</p></article><article class="exercise exercise-like" id="exercise-45"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.45.</span>
</h6>
<p id="p-338">If \(X\) and \(Y\) have the joint probability distribution \(f(-1, 0) = 0\text{,}\)  \(f(-1, 1) = \dfrac{1}{4}\text{,}\) \(f(0, 0) = \dfrac{1}{6}\text{,}\) \(f(1, 0) = \dfrac{1}{12}\text{,}\) \(f(1,
1) = \dfrac{1}{2}\) show that</p>
<ol class="lower-alpha">
<li id="li-48"><p id="p-339">\(\operatorname{cov}(X, Y) = 0\text{;}\)</p></li>
<li id="li-49"><p id="p-340">the two random variables are not independent.</p></li>
</ol></article></section><section class="subsection" id="sub-moments-linear-combinations"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.6</span> <span class="title">Moments of linear combinations of random variables</span>
</h3>
<section class="introduction" id="introduction-26"><p id="p-341">See section 4.7. Recommended problems: 4.7-8 (pg 158) 48, 49, 57</p></section><article class="theorem theorem-like" id="thm-4-14"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.15</span><span class="period">.</span><span class="space"> </span><span class="title">variance.</span>
</h6>
<p id="p-342">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle
a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
E[Y] = \sum_{i=1}^n
a_iE[X_i]
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n a_i^2
\operatorname{var}[X_i] + 2 \mathop{\sum \sum}_{i \lt j} a_i
a_j\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">5.1.16</span><span class="period">.</span><span class="space"> </span><span class="title">variance of independent random variables.</span>
</h6>
<p id="p-343">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y = \sum_{i=1}^n a_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{var}[Y] = \sum_{i=1}^n
a_i^2\operatorname{var}[X_i]
\end{equation*}
</div></article><article class="example example-like" id="ex-var-covar"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-var-covar"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">5.1.17</span><span class="period">.</span><span class="space"> </span><span class="title">covariances of linear combinations.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-var-covar"><article class="example example-like"><p id="p-344">Consider three random variables \(X\text{,}\) \(Y\text{,}\) and \(Z\) with \(\mu_X = 2\text{,}\) with \(\mu_Y = -3\text{,}\) with \(\mu_Z = 4\text{;}\) with \(\sigma_X^2 = 1\text{,}\) \(\sigma_Y^2 = 5\text{,}\) \(\sigma_Z^2 =
2\text{;}\) and \(\operatorname{cov}(X, Y) = -2\text{,}\) \(\operatorname{cov}(X, Z) =
-1\text{,}\) and \(\operatorname{cov}(Y, Z) = 1\text{.}\)</p>
<p id="p-345">Find \(\mu_W\) and \(\operatorname{var}(W) = \sigma_W^2\) for \(W = 3X-Y+2Z\text{.}\)</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-25" id="solution-25"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-25"><div class="solution solution-like">
<p id="p-346">First, \(\mu_W = (3)\mu_X + (-1)\mu_Y + (2)\mu_Z = 17\text{.}\)</p>
<p id="p-347">We could apply the theorem directly, but we can do this more directly with linear algebra. The idea is that we can picture the linear combination \(W = 3X-Y+2Z\) as</p>
<div class="displaymath">
\begin{equation*}
W =
\left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}X \\Y\\
Z\end{array}\right] = (3)X + (-1)Y + (2)Z
\end{equation*}
</div>
<p id="p-348">Let \(a\) be the row vector \(a = \left[\begin{array}{ccc}3 \amp -1
\amp 2\end{array}\right]\text{,}\) its transpose be the column vector \(a^T\text{,}\) and the matrix \(\Sigma\) be defined as follows,</p>
<div class="displaymath">
\begin{equation*}
\Sigma = \left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]
\end{equation*}
</div>
<p id="p-349">This approach can be justified by expanding the sums in <a class="xref" data-knowl="./knowl/thm-4-14.html" title="Theorem 5.1.15: variance">Theorem 5.1.15</a> with a sum of 2 random variables.</p>
<p id="p-350">We can calculate the variance of \(W\) by \(\operatorname{var}(W)
= a\Sigma a^T\text{.}\) Specifically,</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{ccc}
1 \amp -2 \amp -1\\
-2 \amp 5 \amp 1\\
-1 \amp 1 \amp 2
\end{array}\right]\cdot\left[\begin{array}{c}3 \\-1\\
2\end{array}\right]
\end{equation*}
</div>
<p id="p-351">Notice \(Sigma\) is symmetric and that the covariances lie in order along the main diagonal and the variances off-diagonal.</p>
<p id="p-352">Multiplying the square matrix and column vector first, we have</p>
<div class="displaymath">
\begin{equation*}
a \Sigma a^T = \left[\begin{array}{ccc}3 \amp -1 \amp
2\end{array}\right]\cdot\left[\begin{array}{c}3 \\-9\\
0\end{array}\right]
\end{equation*}
</div>
<p id="p-353">And finally, \(a \Sigma a^T = 18\text{.}\)</p>
</div></div></article></div>
<article class="theorem theorem-like" id="thm-4-15"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.1.18</span><span class="period">.</span><span class="space"> </span><span class="title">covariance of two linear combinations.</span>
</h6>
<p id="p-354">If \(\displaystyle X_1, X_2, \dots, X_n\) are random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_i X_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\) where \(\displaystyle a_1, a_2, \dots, a_n,
b_1, b_2, \dots, b_n\) are constants, then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1,
Y_2] = \sum_{i=1}^n a_i b_i\operatorname{var}[X_i] + \mathop{\sum
\sum}_{i \lt j} (a_ib_j + a_jb_i)\operatorname{cov}[X_i,X_j]\text{.}
\end{equation*}
</div></article><article class="corollary theorem-like" id="cor-4-3b"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">5.1.19</span><span class="period">.</span>
</h6>
<p id="p-355">If \(\displaystyle X_1, X_2, \dots, X_n\) are independent random variables and \(\displaystyle Y_1 = \sum_{i=1}^n a_iX_i \text{ and } Y_2 =
\sum_{i=1}^n b_iX_i\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
\operatorname{cov}[Y_1, Y_2] =
\sum_{i=1}^n a_i b_i\operatorname{var}[X_i]
\end{equation*}
</div></article> The same logic used in <a class="xref" data-knowl="./knowl/ex-var-covar.html" title="Example 5.1.17: covariances of linear combinations">Example 5.1.17</a> allows us to compute the covariance between two linear combinations of random variables directly also. Instead of calculating \(a\Sigma a^T\) we will calculate \(a \Sigma b^T\) where \(b\) is the vector of coefficients of the second linear combination.</section><section class="exercises" id="exercises-22"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">5.1.7</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-46"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.48.</span>
</h6>
<p id="p-356">If \(X_1\text{,}\) \(X_2\text{,}\) \(X_3\) are independent and have the means \(4, 9, 3\) and the variances \(3, 7, 5\text{,}\) find the mean and variance of  show that</p>
<ol class="lower-alpha">
<li id="li-50"><p id="p-357">\(Y = 2X_1 - 3X_2 + 4X_3\text{;}\)</p></li>
<li id="li-51"><p id="p-358">\(Z = X_1 + 2X_2 -X_3\text{.}\)</p></li>
</ol></article><article class="exercise exercise-like" id="exercise-47"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 4.49.</span>
</h6>
<p id="p-359">Repeat both parts of the previous exercise after dropping the assumption of independence and using instead that \(\operatorname{cov}(X_1, X_2) = 1\text{,}\) \(\operatorname{cov}(X_2, X_3)
= -2\text{,}\) \(\operatorname{cov}(X_1, X_3) = -3\text{.}\)</p></article></section><section class="subsection" id="sub-conditional-expectation"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.8</span> <span class="title">Conditional expectation</span>
</h3>
<p id="p-360">See section 4.8.</p>
<article class="definition definition-like" id="def-conditional-expectation-4-10"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.20</span><span class="period">.</span><span class="space"> </span><span class="title">conditional expectation.</span>
</h6>
<p id="p-361">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle u(X)\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
E[u(X)|y] = \sum_x u(x)\cdot f(x|y)
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(\displaystyle v(Y)\) given \(\displaystyle X
= x\) is</p>
<div class="displaymath">
\begin{equation*}
E[v(Y)|x] = \sum_y v(y)\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="def-conditional-mean"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.21</span><span class="period">.</span><span class="space"> </span><span class="title">conditional mean.</span>
</h6>
<p id="p-362">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle u(X) = X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\mu_{X|y} = E[X|y] = \sum_x x\cdot f(x|y)
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional mean</dfn> of \(\displaystyle v(Y) = Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle \mu_{Y|x} = E[Y|x] = \sum_y y\cdot w(y|x)
\end{equation*}
</div></article><article class="definition definition-like" id="def-conditional-variance"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.1.22</span><span class="period">.</span><span class="space"> </span><span class="title">conditional variance.</span>
</h6>
<p id="p-363">If \(X\) is a discrete random variable and \(\displaystyle f(x|y)\) is the value of the conditional probability distribution of \(X\) given \(\displaystyle Y = y\) at \(X\text{,}\) the <dfn class="terminology">conditional variance</dfn> of \(X\) given \(\displaystyle Y = y\) is</p>
<div class="displaymath">
\begin{equation*}
\sigma^2_{X|y} = E[(X-\mu_{X|y})^2|y] = E[X^2]-\mu^2_{X|y}
\end{equation*}
</div>
<p data-braille="continuation">and the <dfn class="terminology">conditional expectation</dfn> of \(Y\) given \(\displaystyle X = x\) is</p>
<div class="displaymath">
\begin{equation*}
\displaystyle\sigma^2_{Y|x} = E[(Y-\mu_{Y|x})^2|y] =
E[Y^2]-\mu^2_{Y|x}
\end{equation*}
</div></article></section><section class="subsection" id="sub-discrete-multivariate"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">5.1.9</span> <span class="title">Multivariate distributions</span>
</h3>
<p id="p-364">See Sec. 5.8, 5.9</p>
<section class="introduction" id="introduction-27"><p id="p-365">The multinomial distribution is an extension of the binomial distribution that tracks the occurrence in number of multiple types of outcomes.</p>
<p id="p-366">The multivariate hypergeometric distribution is an extension of the hypergeometric distribution that tracks the occurrence in number of multiple types of outcomes.</p></section></section><section class="exercises" id="exercises-23"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">5.1.10</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-48"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">4.xx.</span>
</h6>
<p id="p-367">xx</p></article></section></section></div></main>
</div>
</body>
</html>
