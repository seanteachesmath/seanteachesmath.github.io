<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-04-13T16:24:10-05:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Special probability distributions</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
/* support for the sfrac command in MathJax (Beveled fraction) */
/* see: https://github.com/mathjax/MathJax-docs/wiki/Beveled-fraction-like-sfrac,-nicefrac-bfrac */
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  var MML = MathJax.ElementJax.mml,
      TEX = MathJax.InputJax.TeX;
  TEX.Definitions.macros.sfrac = "myBevelFraction";
  TEX.Parse.Augment({
    myBevelFraction: function (name) {
      var num = this.ParseArg(name),
          den = this.ParseArg(name);
      this.Push(MML.mfrac(num,den).With({bevelled: true}));
    }
  });
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" class="hidden-content" style="display:none">\(
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href="http://www.math.uco.edu" target="_blank"><img src="images/cover.png" alt="Logo image"></a><div class="title-container">
<h1 class="heading"><a href="mathstat.html"><span class="title">Mathematical Statistics I:</span> <span class="subtitle">Based on course notes developed using Freund's Mathematical Statistics</span></a></h1>
<p class="byline">Sean M. Laverty</p>
</div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-discrete-expectation.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap-discrete.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="chap-continuous.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-discrete-expectation.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap-discrete.html" title="Up">Up</a><a class="next-button button toolbar-item" href="chap-continuous.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter"><span class="title">Front Matter</span></a><ul>
<li><a href="front-colophon.html" data-scroll="front-colophon">Colophon</a></li>
<li><a href="author-bio-SML.html" data-scroll="author-bio-SML">Author Biography</a></li>
<li><a href="dedication.html" data-scroll="dedication">Dedication</a></li>
<li><a href="acknowledgement.html" data-scroll="acknowledgement">Acknowledgements</a></li>
<li><a href="preface.html" data-scroll="preface">Preface</a></li>
<li><a href="contributors.html" data-scroll="contributors">Contributors to the 0\(^\mathrm{th}\) Edition</a></li>
</ul>
</li>
<li class="link">
<a href="chap-counting.html" data-scroll="chap-counting"><span class="codenumber">1</span> <span class="title">Counting</span></a><ul><li><a href="section-counting.html" data-scroll="section-counting">Counting</a></li></ul>
</li>
<li class="link">
<a href="chap-prob.html" data-scroll="chap-prob"><span class="codenumber">2</span> <span class="title">Probability</span></a><ul><li><a href="section-probability.html" data-scroll="section-probability">Probability</a></li></ul>
</li>
<li class="link">
<a href="chap-discrete.html" data-scroll="chap-discrete"><span class="codenumber">3</span> <span class="title">Discrete Random Variables</span></a><ul>
<li><a href="section-probability-distributions.html" data-scroll="section-probability-distributions">Probability distributions</a></li>
<li><a href="section-discrete-expectation.html" data-scroll="section-discrete-expectation">Mathematical expectation of discrete random variables</a></li>
<li><a href="section-special-distributions.html" data-scroll="section-special-distributions" class="active">Special probability distributions</a></li>
</ul>
</li>
<li class="link">
<a href="chap-continuous.html" data-scroll="chap-continuous"><span class="codenumber">4</span> <span class="title">Continuous Random Variables</span></a><ul>
<li><a href="section-probability-densities.html" data-scroll="section-probability-densities">Probability densities</a></li>
<li><a href="section-continuous-expectation.html" data-scroll="section-continuous-expectation">Expectation of continuous random variables</a></li>
<li><a href="section-special-densities.html" data-scroll="section-special-densities">Special probability densities</a></li>
</ul>
</li>
<li class="link">
<a href="chap-multi.html" data-scroll="chap-multi"><span class="codenumber">5</span> <span class="title">Multivariate Probability Distributions and Densities</span></a><ul>
<li><a href="section-multivariate-discrete-probability.html" data-scroll="section-multivariate-discrete-probability">Multivariate discrete random variables</a></li>
<li><a href="sec-multi-cont.html" data-scroll="sec-multi-cont">Multivariate continuous random variables</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-def.html" data-scroll="appendix-def"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thm.html" data-scroll="appendix-thm"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-ex.html" data-scroll="appendix-ex"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="appendix-gfdl.html" data-scroll="appendix-gfdl"><span class="codenumber">D</span> <span class="title">GNU Free Documentation License</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
<li class="link"><a href="back-colophon.html" data-scroll="back-colophon"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="section-special-distributions"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">3.3</span> <span class="title">Special probability distributions</span>
</h2>
<section class="introduction" id="introduction-9"><p id="p-72">Now we look at a collection of formulas for probability distributions. These describe how probabilities are assigned across a range of discrete values of a random variable. It turns out to be the case that values of random variables associated with certain kinds of experiments follow certain formulas. In this chapter, we will look at the formulas that describe certain discrete random variables.</p>
<p id="p-73">For the purposes of generality we will emphasize that certain formulas include one or more parameters, these are symbols to which we will associate particular numerical values, much like we do \(m\) and \(b\) in the formula \(y = f(x) = mx+b\text{.}\) We should have a general understanding of something that follows this relationship and an understanding of the roles of both \(m\) and \(b\) in specifying that relationship.</p></section><section class="subsection" id="sub-discrete-unif"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.3.1</span> <span class="title">Discrete uniform</span>
</h3>
<article class="definition definition-like" id="def-discrete-unif"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.1</span><span class="period">.</span><span class="space"> </span><span class="title">discrete uniform distribution.</span>
</h6>
<p id="p-74">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">discrete uniform distribution</dfn> and is referred to as a <dfn class="terminology">discrete uniform random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{equation*}
f(x) = \frac{1}{k} \text{ for }
x = x_1, x_2, \dots, x_k \text{ where }x_i\ne x_j \text{ for } i \ne
j\text{.}
\end{equation*}
</div></article><p id="p-75">This is sometimes written as \(\operatorname{DU}(k)\text{,}\) where \(k\) is the parameter of the distribution.</p>
<article class="example example-like" id="ex-disc-unif-mean"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-disc-unif-mean"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.2</span><span class="period">.</span><span class="space"> </span><span class="title">mean of a discrete uniform distribution.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-disc-unif-mean"><article class="example example-like"><p id="p-76">Find the mean of a discrete uniform random variable (or of the discrete uniform distribution).</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-5" id="solution-5"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-5"><div class="solution solution-like">
<p id="p-77">Recall that</p>
<div class="displaymath">
\begin{equation*}
f(x) = \frac{1}{k} \text{ for } x = x_1, x_2, \dots,
x_k \text{ where }x_i\ne x_j \text{ for } i \ne j
\end{equation*}
</div>
<p data-braille="continuation">and that</p>
<div class="displaymath">
\begin{equation*}
\mu
= \sum_x x\cdot f(x)\text{.}
\end{equation*}
</div>
<p data-braille="continuation">This becomes</p>
<div class="displaymath">
\begin{equation*}
\mu = \sum_{i=1}^k
x_i\left(\frac{1}{k}\right)\text{.}
\end{equation*}
</div>
<p data-braille="continuation">This is about as far as we can go in general without knowing more about \(k\) or the values of \(x_i\text{.}\)</p>
</div></div></article></div>
<p id="p-78">Similar to the previous "Example", the variance of a discrete uniform random variable (or of the discrete uniform distribution) is given by</p>
<div class="displaymath">
\begin{equation*}
\sigma^2 = \sum_x (x-\mu)^2\cdot f(x)= \sum_x (x-\mu)^2
\left(\frac{1}{k}\right)\text{.}
\end{equation*}
</div>
<p data-braille="continuation">Once again, this is about as far as we can go in general.</p>
<article class="example example-like" id="ex-disc-unif-simple"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-disc-unif-simple"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.3</span><span class="period">.</span><span class="space"> </span><span class="title">simple discrete uniform distribution.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-disc-unif-simple"><article class="example example-like"><p id="p-79">Find the mean of a discrete uniform random variable (or of the discrete uniform distribution) for which \(x_i = i\text{.}\)</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-6" id="solution-6"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-6"><div class="solution solution-like">
<p id="p-80">Recall that</p>
<div class="displaymath">
\begin{equation*}
f(x) = \frac{1}{k} \text{ for } x = 1, 2, \dots,
k
\end{equation*}
</div>
<p data-braille="continuation">and that</p>
<div class="displaymath">
\begin{equation*}
\mu = \sum_x x\cdot f(x)\text{.}
\end{equation*}
</div>
<p data-braille="continuation">This becomes</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
\mu \amp = \sum_{i=1}^k i\left(\frac{1}{k}\right)\\
\amp = \frac{1}{k}\left(\sum_{i=1}^k i\right)\\
\amp = \frac{1}{k}\left(\dfrac{k(k+1)}{2}\right)\\
\mu	\amp = \dfrac{k+1}{2}
\end{aligned}\text{.}
\end{equation*}
</div>
<p data-braille="continuation">Now, verify our earlier work on the mean number of dots to show on a balanced, 6-sided die.</p>
</div></div></article></div></section><section class="exercises" id="exercises-5"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.3.2</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-13"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">5.2.</span>
</h6>
<p id="p-81">If \(X\) has a discrete uniform distribution \(f(x) =
\dfrac{1}{k}\) for \(x = 1, 2, \dots, k\text{,}\) show that its moment-generating function is given by</p>
<div class="displaymath">
\begin{equation*}
M_x(t) =
\dfrac{e^t(1-e^{kt})}{k(1-e^t)}\text{.}
\end{equation*}
</div></article></section><section class="subsection" id="sub-discrete-bern-bin"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.3.3</span> <span class="title">Bernoulli and Binomial distributions</span>
</h3>
<section class="introduction" id="introduction-10"><p id="p-82">Consider an experiment with two possible outcomes, flipping a coin for example. We consider one of those outcomes, say 'heads' as a success, and the other of those outcomes, in this case 'tails` as a failure. We allow these outcomes to each occur with a given probability. Here \(\theta\) gives the probability of 'success' and \(1-\theta\) is the corresponding probability of 'failure'.</p>
<p id="p-83">Though the outcome of a single event may be useful, the Bernoulli distribution is perhaps most useful as a building block to describe the results of more complex experiments.</p>
<p id="p-84">You will find tools for visualization at the following <a class="external" href="https://buddy.uco.edu/shiny/slaverty/mathstat/Binomial/" target="_blank">link</a>.</p></section><article class="definition definition-like" id="def-bern"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.4</span><span class="period">.</span><span class="space"> </span><span class="title">Bernoulli distribution.</span>
</h6>
<p id="p-85">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">Bernoulli distribution</dfn> and is referred to as a <dfn class="terminology">Bernoulli random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{equation*}
f(x; \theta) = \theta^x(1-\theta)^{1-x}
\text{ for }x = 0, 1\text{.}
\end{equation*}
</div></article><article class="example example-like" id="ex-bern-mean-var"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bern-mean-var"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.5</span><span class="period">.</span><span class="space"> </span><span class="title">mean and variance of a Bernoulli distribution.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bern-mean-var"><article class="example example-like"><p id="p-86">Find the mean and variance of a Bernoulli random variable (or of the Bernoulli distribution).</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-7" id="solution-7"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-7"><div class="solution solution-like">
<p id="p-87">Generally we calculate the mean as</p>
<div class="displaymath">
\begin{equation*}
\mu = \sum_x x\cdot f(x)\text{.}
\end{equation*}
</div>
<p data-braille="continuation">Specifically this becomes</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
\mu \amp = \sum_{i=0}^1 x f(x; \theta)\\
\amp = 0\left(\theta^0(1-\theta)^1\right) +
1\left(\theta^1(1-\theta)^0\right)\\
\mu \amp = \theta
\end{aligned}\text{.}
\end{equation*}
</div>
<p id="p-88">We calculate the variance as \(\sigma^2 =
E[X^2]-\left(E[X]\right)^2\text{.}\) Keep in mind that we now know that \(E[X] = \theta\text{,}\) so we know that \(\left(E[X]\right)^2 =
\theta^2\)</p>
<p id="p-89">Now</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
E[X^2] \amp = \sum_{i=0}^1 x^2 f(x; \theta)\\
\amp = 0^2\left(\theta^0(1-\theta)^1\right) +
1^2\left(\theta^1(1-\theta)^0\right)\\
E[X^2] \amp = \theta
\end{aligned}
\end{equation*}
</div>
<p data-braille="continuation">Given this,</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
\sigma^2 \amp = E[X^2]-\left(E[X]\right)^2\\
\amp = \theta - \theta^2 \\
\sigma^2 \amp = \theta(1-\theta)
\end{aligned}
\end{equation*}
</div>
</div></div></article></div>
<article class="example example-like" id="ex-bern-mgf"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bern-mgf"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.6</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function of a Bernoulli distribution.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bern-mgf"><article class="example example-like"><p id="p-90">Find the moment-generating function of a Bernoulli random variable (or of the Bernoulli distribution).</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-8" id="solution-8"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-8"><div class="solution solution-like">
<p id="p-91">Recall that \(\displaystyle M_X(t) = E[e^{tx}] = \sum_x e^{tx} f(x;
\theta)\text{.}\) This becomes</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
M_X(t) \amp = \sum_{x=0}^1 e^{tx} f(x; \theta)\\
\amp = e^{0\cdot t}\left(\theta^0(1-\theta)^1\right) + e^{1\cdot
t}\left(\theta^1(1-\theta)^0\right)\\
\amp = (1-\theta) + \theta e^t\\
M_X(t) \amp = 1 + \theta (e^t-1)
\end{aligned}\text{.}
\end{equation*}
</div>
</div></div></article></div>
<p id="p-92">You could use the moment-generating function and the earlier theorem to calculate moments about the origin used to find the mean and variance.</p>
<p id="p-93">Consider now a new random variable \(Y\) whose value is the sum of independent Bernoulli trials. This new variable has a 'binomial distribution' as defined below.</p>
<article class="definition definition-like" id="def-bin"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.7</span><span class="period">.</span><span class="space"> </span><span class="title">binomial distribution.</span>
</h6>
<p id="p-94">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">binomial distribution</dfn> and is referred to as a <dfn class="terminology">binomial random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{equation*}
b(x; n, \theta) = {n\choose
x}\theta^x(1-\theta)^{n-x} \text{ for }x = 0, 1, \dots, n\text{.}
\end{equation*}
</div></article><p id="p-95">In the definition, the term \(\displaystyle {n \choose x}\) gives the number of ways to select the order of the \(x\) successes in \(n\) trials. The values of \(b(x; n, \theta)\) give the coefficients of terms in the expansion of \(\displaystyle
\left((1-\theta) + \theta\right)^n\) (for more see <a class="xref" data-knowl="./knowl/rmk-connection-to-pascal.html" title="Remark 3.3.8: Connection to binomial theorem">Remark 3.3.8</a>).</p>
<article class="remark remark-like" id="rmk-connection-to-pascal"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">3.3.8</span><span class="period">.</span><span class="space"> </span><span class="title">Connection to binomial theorem.</span>
</h6>
<p id="p-96">You may be wondering, what does \(\displaystyle
\left((1-\theta) + \theta\right)^n\) have to do with anything?</p>
<p id="p-97">Recall that the binomial theorem suggests that</p>
<div class="displaymath">
\begin{equation*}
(x+y)^{n} = \sum\limits_{k=0}^n {n \choose k}x^{n-k}y^k = {n \choose 0} x^{n}y^{0} + {n \choose 1} x^{n-1}y^{1} + \dots + {n \choose n} x^{0}y^{n}.
\end{equation*}
</div>
<p data-braille="continuation">By connecting these, we see that with \(y = \theta\) and \(x = 1-\theta\text{,}\) we have,</p>
<div class="displaymath">
\begin{align*}
\sum\limits_{x=0}^n b(x; n, \theta) \amp = \sum\limits_{x=0}^n {n \choose x}\theta^{x}(1-\theta)^{n-x} \\
\ \amp = \sum\limits_{x=0}^n {n \choose x}(1-\theta)^{n-x}\theta^{x} \\
\amp = \left[(1-\theta) + \theta)\right]^{n} \\
\amp = \left[1\right]^{n} \\
\sum\limits_{x=0}^n b(x; n, \theta) \amp = 1 
\end{align*}
</div>
<p data-braille="continuation">This verifies that the nonnegative values of \(b(x; n, \theta)\) also sum to one and that the function meets the requirements of a probability mass function.</p></article><article class="example example-like" id="ex-bin-basketball-dumb"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bin-basketball-dumb"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.9</span><span class="period">.</span><span class="space"> </span><span class="title">Basketball free throws.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bin-basketball-dumb"><article class="example example-like"><p id="p-98">What is the probability that an \(86\%\) free throw-shooter makes \(2\) of \(5\) shots during a game?</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-9" id="solution-9"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-9"><div class="solution solution-like">
<p id="p-99">Let's try without invoking the binomial distribution.  Shots could occur as follows, <figure class="table table-like" id="tab-basketball"><figcaption><span class="type">Table</span><span class="space"> </span><span class="codenumber">3.3.10<span class="period">.</span></span><span class="space"> </span>Sequences of shots totalling 2 makes in 5 attempts.</figcaption><table>
<tr>
<td class="c m b2 r0 l0 t0 lines">shot sequence</td>
<td class="c m b2 r0 l0 t0 lines" data-braille="last-cell">\(\operatorname{Pr}(\text{shot sequence})\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(make)(make)(miss)(miss)(miss)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.86)(0.86)(0.14)(0.14)(0.14)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(make)(miss)(make)(miss)(miss)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.86)(0.14)(0.86)(0.14)(0.14)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(make)(miss)(miss)(make)(miss)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.86)(0.14)(0.14)(0.86)(0.14)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(make)(miss)(miss)(miss)(make)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.86)(0.14)(0.14)(0.14)(0.86)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(miss)(make)(make)(miss)(miss)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.14)(0.86)(0.86)(0.14)(0.14)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(miss)(make)(miss)(make)(miss)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.14)(0.86)(0.14)(0.86)(0.14)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(miss)(make)(miss)(miss)(make)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.14)(0.86)(0.14)(0.14)(0.86)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(miss)(miss)(make)(make)(miss)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.14)(0.14)(0.86)(0.86)(0.14)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(miss)(miss)(make)(miss)(make)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.14)(0.14)(0.86)(0.14)(0.86)\)</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines">(miss)(miss)(miss)(make)(make)</td>
<td class="l m b0 r0 l0 t0 lines" data-braille="last-cell">\((0.14)(0.14)(0.14)(0.86)(0.86)\)</td>
</tr>
</table></figure></p>
<p id="p-100">In all cases, the probability is given by \((0.86)^2(0.14)^3 \approx 0.00203\text{,}\) and since there are ten possible ways that this could happen,</p>
<div class="displaymath">
\begin{equation*}
\operatorname{Pr}(\text{making 2 of 5 shots}) = (10 \text{ ways})\cdot(0.00203 \text{ each way}) \approx 0.0203\text{.}
\end{equation*}
</div>
</div></div></article></div>
<article class="example example-like" id="ex-bin-bball"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bin-bball"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.11</span><span class="period">.</span><span class="space"> </span><span class="title">Free throws.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bin-bball"><article class="example example-like"><p id="p-101">Use the binomial distribution to find the probability of making \(2\) in \(5\) free throws if the probability of making one is \(0.86\text{?}\)</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-10" id="solution-10"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-10"><div class="solution solution-like">
<p id="p-102">We need to calculate</p>
<div class="displaymath">
\begin{equation*}
b(2; 5, 0.86) = {5\choose2}0.86^2(1-0.86)^{5-2}
= 10(0.86)^2(0.14)^3 \approx 0.0203\text{.}
\end{equation*}
</div>
<p data-braille="continuation">This calculation corresponds to the ten possible orderings of 2 makes in 5 shot attempts, the probabilities associated with 2 'makes', and the probabilities associated with 3 'misses'.  The ten orders shown in <a class="xref" data-knowl="./knowl/ex-bin-basketball-dumb.html" title="Example 3.3.9: Basketball free throws">Example 3.3.9</a> are reflected in the coefficient \({5 \choose 2}\) in the binomial distribution.</p>
</div></div></article></div>
<article class="example example-like" id="ex-bin-coins"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bin-coins"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.12</span><span class="period">.</span><span class="space"> </span><span class="title">Binomial coin flips.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bin-coins"><article class="example example-like"><p id="p-103">Find the probability of 5 heads in 9 coin flips under each of the following situations.</p>
<ol class="decimal">
<li id="li-15">The coin is balanced and \(P(\text{success}) = P(\text{failure}) =
0.5\)</li>
<li id="li-16">The coin is unbalanced and \(P(\text{success})\) is \(3\) times larger than \(P(\text{failure})\)</li>
</ol>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-11" id="solution-11"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-11"><div class="solution solution-like">This problem can be done by evaluating the binomial using 'Table 1' of the 'Statistical Tables Appendix' (see book), or by using the definition of probability distribution of the binomial random variable. <ol class="decimal">
<li id="li-17">
<p id="p-104">The coin is balanced and \(P(\text{success}) = P(\text{failure})
= 0.5\text{.}\) We are looking to evaluate the binomial probability, sometimes written \(\operatorname{Bin}(n=9, \theta=0.5)\text{,}\) specifically \(b(5; 9,
0.5)\text{.}\) This is,</p>
<div class="displaymath">
\begin{equation*}
b(5; 9, 0.5) = {9 \choose 5}0.5^5(1-0.5)^{9-5} \approx
0.2461 
\end{equation*}
</div>
</li>
<li id="li-18">
<p id="p-105">The coin is unbalanced and \(P(\text{success})\) is \(3\) times larger than \(P(\text{failure})\text{.}\) We first have to figure out the value for \(P(\text{success}) =
\theta\text{.}\) We have said that \(P(\text{success}) = 3\cdot P(\text{failure})\) or \(\theta = 3\cdot (1-\theta)\text{.}\) This gives \(\theta = 0.75\) and, as above,</p>
<div class="displaymath">
\begin{equation*}
b(5; 9, 0.75) = {9
\choose 5}0.75^5(1-0.75)^{9-5} \approx 0.1168 
\end{equation*}
</div>
</li>
</ol>
</div></div></article></div>
<p id="p-106">Switching focus from \(x\) successes in \(n\) trials with a probability of success \(\theta\text{,}\) we have \(n-x\) failures in \(n\) trials with a probability of failure \(1-\theta\text{.}\)</p>
<article class="theorem theorem-like" id="thm-bin-param"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.13</span><span class="period">.</span><span class="space"> </span><span class="title">reparameterizing a binomial.</span>
</h6>
<div class="displaymath" id="p-107">
\begin{equation*}
b(x; n, \theta) = b(n-x; n, 1-\theta)
\end{equation*}
</div></article><article class="example example-like" id="ex-bin-bball2"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bin-bball2"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.14</span><span class="period">.</span><span class="space"> </span><span class="title">Free throws - revisited.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bin-bball2"><article class="example example-like"><p id="p-108">Using theorem <a class="xref" data-knowl="./knowl/thm-bin-param.html" title="Theorem 3.3.13: reparameterizing a binomial">Theorem 3.3.13</a>, find the probability of making \(2\) in \(5\) free throws if the probability of making one is \(0.86\text{?}\)</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-12" id="solution-12"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-12"><div class="solution solution-like">
<p id="p-109">We need to calculate</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
b(5-2; 5, 1-0.86) \amp = b(3; 5, 0.14)\\
\amp = {5\choose3}0.14^3(1-0.14)^{5-3}\\
\amp = 10(0.14)^3(0.86)^2\\
\amp \approx 0.020
\end{aligned}\text{.}
\end{equation*}
</div>
<p data-braille="continuation">This calculation corresponds to the ten possible orderings of 2 makes in 5 shot attempts, the probabilities associated with 2 'makes', and the probabilities associated with 3 'misses'.</p>
</div></div></article></div>
<p id="p-110">For certain calculations, it is helpful to use the theorem to reduce the sizes of numbers used in the factorial or to find parameterizations for which the probabilities are known by a table.</p>
<article class="theorem theorem-like" id="thm-bin-mean-var"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.15</span><span class="period">.</span><span class="space"> </span><span class="title">Mean and variance of binomial distribution.</span>
</h6>
<p id="p-111">The mean and variance of a binomial random variable (or of the binomial distribution) are</p>
<div class="displaymath">
\begin{equation*}
\mu = n\theta \text{ and } \sigma^2 =
n\theta(1-\theta)\text{.}
\end{equation*}
</div>
<article class="hiddenproof" id="proof-1"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-1"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-1"><article class="hiddenproof"><p id="p-112">These can be proved using the expectation and some clever re-indexing in evaluating the sum.</p></article></div></article><article class="theorem theorem-like" id="thm-bin-successes"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.16</span><span class="period">.</span><span class="space"> </span><span class="title">Proportion of binomial successes.</span>
</h6>
<p id="p-113">If \(X\) is a binomially-distributed random variable with parameters \(n\text{,}\) \(\theta\text{,}\) and \(Y = \dfrac{X}{n}\) gives the proportion of successes,</p>
<div class="displaymath">
\begin{equation*}
E[Y] = \theta \text{ and } \sigma_Y^2 =
\dfrac{\theta(1-\theta)}{n}\text{.}
\end{equation*}
</div></article><article class="exercise exercise-like" id="exer-bin-dice"><a data-knowl="" class="id-ref exercise-knowl original" data-refid="hk-exer-bin-dice"><h6 class="heading">
<span class="type">Checkpoint</span><span class="space"> </span><span class="codenumber">3.3.17</span><span class="period">.</span><span class="space"> </span><span class="title">probabilities of dice rolls.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-exer-bin-dice"><article class="exercise exercise-like"><p id="p-114">Find the expected value of the number of times a 2 or 3 shows in 15 rolls of a standard 6-sided die.</p>
<a data-knowl="" class="id-ref hint-knowl original" data-refid="hk-hint-1" id="hint-1"><span class="type">Hint</span></a><div class="hidden-content tex2jax_ignore" id="hk-hint-1"><div class="hint solution-like">What is the probability of 'success'?</div></div></article></div>
<article class="example example-like" id="ex-bin-mgf"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-bin-mgf"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.18</span><span class="period">.</span><span class="space"> </span><span class="title">moment-generating function of a binomial distribution.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-bin-mgf"><article class="example example-like"><p id="p-115">Find the moment-generating function of a binomial random variable (or of the binomial distribution).</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-13" id="solution-13"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-13"><div class="solution solution-like">
<p id="p-116">Recall that \(\displaystyle M_X(t) = E[e^{tx}] = \sum_x e^{tx} f(x;
\theta)\text{.}\) This becomes</p>
<div class="displaymath">
\begin{equation*}
\begin{aligned}[t]
M_X(t) \amp = \sum_{x=0}^n e^{tx} b(x; n, \theta)\\
\amp = \sum_{x=0}^n e^{tx} {n \choose x} \theta^x (1-\theta)^{n-x}\\
\amp = \sum_{x=0}^n {n \choose x} \left(\theta e^t\right)^x
(1-\theta)^{n-x}\\
\amp = \sum_{x=0}^n {n \choose x} (1-\theta)^{n-x}\left(\theta
e^t\right)^x\\
M_X(t) \amp = \left((1-\theta) + \theta e^t\right)^n
\end{aligned}\text{.}
\end{equation*}
</div>
<p data-braille="continuation">That last step, though a big leap in print, comes from applying the binomial theorem in reverse, that is</p>
<div class="displaymath">
\begin{equation*}
\sum_{k=0}^n {n \choose
r}a^{n-r} b^r = (a+b)^n
\end{equation*}
</div>
<p data-braille="continuation">where \(a=1-\theta\) and \(b = \theta
e^t\text{.}\)</p>
</div></div></article></div>
<p id="p-117">You might recognize this as \(n\) factors of the moment-generating function of a Bernoulli random variable.</p>
<p id="p-118">You could use the moment-generating function and <a class="xref" data-knowl="./knowl/thm-4-9.html" title="Theorem 3.2.14: moments via differentiation">Theorem 3.2.14</a> to calculate moments about the origin used to find the mean and variance as a mechanism to prove <a class="xref" data-knowl="./knowl/thm-bin-mean-var.html" title="Theorem 3.3.15: Mean and variance of binomial distribution">Theorem 3.3.15</a>.</p></section><section class="exercises" id="exercises-6"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.3.4</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-15"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 5.7.</span>
</h6>
<p id="p-119">Verify <a class="xref" data-knowl="./knowl/thm-bin-successes.html" title="Theorem 3.3.16: Proportion of binomial successes">Theorem 3.3.16</a>.</p></article><article class="exercise exercise-like" id="exercise-16"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 5.10.</span>
</h6>
<p id="p-120">If \(X\) is a binomial random variable, for what value of \(\theta\) is the probability \(b(x; n, \theta)\) a maximum? In other words, maximize \(b(x; n, \theta)\) with respect to \(\theta\text{.}\)</p></article></section><section class="subsection" id="sub-discrete-negbin-geom"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.3.5</span> <span class="title">Negative Binomial and Geometric distributions</span>
</h3>
<section class="introduction" id="introduction-11"><p id="p-121">The binomial distribution describes the probability of a certain number of successes in a certain amount of trials. Sometimes, instead, we are interested in the trial on which a particular success occurs.  This is described by the negative binomial distribution.</p>
<p id="p-122">This situation requires obtaining \(k-1\) successes across the first \(x-1\) trials, with the \(k^{\text{th}}\) and final success to occur on the \(x^{\text{th}}\) trial.</p>
<p id="p-123">You will find tools for visualization at the following <a class="external" href="https://buddy.uco.edu/shiny/slaverty/mathstat/NegBin/" target="_blank">link</a>.</p></section><article class="definition definition-like" id="def-neg-bin"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.19</span><span class="period">.</span><span class="space"> </span><span class="title">negative binomial distribution.</span>
</h6>
<p id="p-124">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">negative binomial distribution</dfn> and is referred to as a <dfn class="terminology">negative binomial random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{align*}
b^*(x; k, \theta) \amp = {x-1\choose
k-1}\theta^k(1-\theta)^{(x-1)-(k-1)}s\\
\amp = {x-1\choose
k-1}\theta^k(1-\theta)^{x-k}
\end{align*}
</div>
<p data-braille="continuation">for \(x = k, k+1, \dots\text{.}\)</p></article><p id="p-125">Sometimes we refer to the random variable as being distributed according to \(\operatorname{NegBin}(k, \theta)\) or \(\operatorname{NB}(k, \theta)\text{.}\)   The values of the random variable describe <em class="emphasis">binomial waiting-times</em>, since the result is the number of trials until arriving at a particular outcome of interest.</p>
<article class="theorem theorem-like" id="thm-bin-negbin"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.20</span><span class="period">.</span><span class="space"> </span><span class="title">negative binomial probability as a binomial probability.</span>
</h6>
<div class="displaymath" id="p-126">
\begin{equation*}
b^*(x; k, \theta) = \frac{k}{x}b(k; x, \theta)\text{.}
\end{equation*}
</div></article><article class="hiddenproof" id="proof-2"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-2"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-2"><article class="hiddenproof"><p id="p-127">This can be shown, relatively quickly by relatively simple manipulation of the definitions. Notice that we are equating on the left the negative binomial and on the right the binomial.</p></article></div>
<article class="exercise exercise-like" id="exer-negbin-bball"><a data-knowl="" class="id-ref exercise-knowl original" data-refid="hk-exer-negbin-bball"><h6 class="heading">
<span class="type">Checkpoint</span><span class="space"> </span><span class="codenumber">3.3.21</span><span class="period">.</span><span class="space"> </span><span class="title">Free throws with the negative binomial.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-exer-negbin-bball"><article class="exercise exercise-like"><p id="p-128">A player makes a free throw with probability \(0.86\text{.}\) What is the probability that the shooter makes her \(3^{\text{rd}}\) shot on her \(5^{\text{th}}\) attempt?</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-14" id="solution-14"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-14"><div class="solution solution-like">
<p id="p-129">We need to calculate</p>
<div class="displaymath">
\begin{equation*}
b^*(5; 3, 0.86) =
{5-1\choose3-1}0.86^3(1-0.86)^{5-3}
= 6(0.86)^3(0.14)^2\text{.}
\end{equation*}
</div>
<p data-braille="continuation">This means \(2\) shots were made in the first \(4\) attempts, followed by the \(3^{\text{rd}}\) make on the \(5^{\text{th}}\) attempt.</p>
</div></div></article></div>
<article class="theorem theorem-like" id="theorem-11"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.22</span><span class="period">.</span><span class="space"> </span><span class="title">Mean and variance of the negative binomial distribution.</span>
</h6>
<p id="p-130">The mean and variance of the negative binomial distribution are</p>
<div class="displaymath">
\begin{equation*}
\mu
= \frac{k}{\theta}
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\sigma^2 =
\frac{k}{\theta}\left(\frac{1}{\theta}-1\right)\text{.}
\end{equation*}
</div></article><article class="example example-like" id="ex-negbin-mgf"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-negbin-mgf"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.23</span><span class="period">.</span><span class="space"> </span><span class="title">moment generating function of the negative binomial distribution.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-negbin-mgf"><article class="example example-like"><p id="p-131">Find the moment-generating function of a negative binomial random variable (or of the negative binomial distribution).</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-15" id="solution-15"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-15"><div class="solution solution-like">
<p id="p-132">Recall that \(\displaystyle M_X(t) = E[e^{tx}] = \sum_x e^{tx} f(x;
\theta)\text{.}\) This becomes</p>
<div class="displaymath">
\begin{equation*}
M_X(t) = \left(\dfrac{\theta e^t}{1-(1-\theta)e^t}\right)^k\text{.}
\end{equation*}
</div>
</div></div></article></div>
<p id="p-133">The question about when the first success occurs is a common one. So common, in fact, that it gets its own name, the <em class="emphasis">geometric distribution</em>.  Now it should first be noted that we sometimes view 'success' strangely. Often we are instead thinking of a particular 'outcome of interest' rather than the traditional interpretation of 'success' as 'a good thing'.</p>
<article class="definition definition-like" id="def-geom"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.24</span><span class="period">.</span><span class="space"> </span><span class="title">geometric distribution.</span>
</h6>
<p id="p-134">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">geometric distribution</dfn> and is referred to as a <dfn class="terminology">geometric random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{equation*}
g(x; \theta) = \theta(1-\theta)^{x-1}
\text{ for }x = 1, 2, \dots\text{.}
\end{equation*}
</div></article><p id="p-135">The geometric distribution answers the age-old question "if at first you don't succeed, how many times did you fail?"</p>
<article class="theorem theorem-like" id="theorem-12"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.25</span><span class="period">.</span><span class="space"> </span><span class="title">Mean and variance of the geometric distribution.</span>
</h6>
<p id="p-136">The mean and variance of the geometric distribution are</p>
<div class="displaymath">
\begin{equation*}
\mu =
\frac{1}{\theta}
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\sigma^2 =
\frac{1}{\theta}\left(\frac{1}{\theta}-1\right)\text{.}
\end{equation*}
</div></article><p id="p-137">Sometimes we refer to the random variable as being distributed according to \(\operatorname{Geom}(\theta)\text{.}\)</p></section><section class="exercises" id="exercises-7"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.3.6</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-18"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 5.18.</span>
</h6>
<p id="p-138">Prove <a class="xref" data-knowl="./knowl/thm-bin-negbin.html" title="Theorem 3.3.20: negative binomial probability as a binomial probability">Theorem 3.3.20</a>.</p></article><article class="exercise exercise-like" id="exercise-19"><h6 class="heading">
<span class="codenumber">2<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 5.20.</span>
</h6>
<p id="p-139">Show that the moment-generating function of the geometric distribution is given by</p>
<div class="displaymath">
\begin{equation*}
M_X(t) = \dfrac{\theta
e^t}{1-e^t(1-\theta)}\text{.}
\end{equation*}
</div></article><article class="exercise exercise-like" id="exercise-20"><h6 class="heading">
<span class="codenumber">3<span class="period">.</span></span><span class="space"> </span><span class="title">Problem 5.21.</span>
</h6>
<p id="p-140">Use <a class="xref" data-knowl="./knowl/thm-4-9.html" title="Theorem 3.2.14: moments via differentiation">Theorem 3.2.14</a> and</p>
<div class="displaymath">
\begin{equation*}
M_X(t) = \dfrac{\theta
e^t}{1-e^t(1-\theta)}
\end{equation*}
</div>
<p data-braille="continuation">to find \(\mu\) and \(\sigma^2\) by differentiation.</p></article><article class="exercise exercise-like" id="exercise-21"><h6 class="heading">
<span class="codenumber">4<span class="period">.</span></span><span class="space"> </span><span class="title">Negative Binomial expansions.</span>
</h6>
<p id="p-141">Expand the negative binomial \((x+1)^{-n}\) by Taylor Series around \(x_{0} = 0\) to orders second- and third-orders.</p></article></section><section class="subsection" id="sub-discrete-hypergeom"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.3.7</span> <span class="title">Hypergeometric distribution</span>
</h3>
<section class="introduction" id="introduction-12"><p id="p-142">The Hypergeometric distribution describes the probabilities of sampling from a finite population without replacement.  Unlike the binomial where it is assumed that the probability of success is a constant, with the hypergeometric distribution, the probability of success changes with the selection process.</p>
<p id="p-143">You will find tools for visualization at the following <a class="external" href="https://buddy.uco.edu/shiny/slaverty/mathstat/Hyper/" target="_blank">link</a>.</p></section><article class="definition definition-like" id="def-hyper"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.26</span><span class="period">.</span><span class="space"> </span><span class="title">hypergeometric distribution.</span>
</h6>
<p id="p-144">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">hypergeometric distribution</dfn> and is referred to as a <dfn class="terminology">hypergeometric random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{equation*}
h(x; n, N, M) = \dfrac{{M \choose x}{N-M
\choose{n-x}}}{{N \choose n}}
\text{ for }x = 1, 2, \dots, n; x \le M \text{ and } n-x \le N-M\text{.}
\end{equation*}
</div></article><article class="theorem theorem-like" id="theorem-13"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.27</span><span class="period">.</span><span class="space"> </span><span class="title">Mean and variance of the hypergeometric distribution.</span>
</h6>
<p id="p-145">The mean and variance of the hypergeometric distribution are</p>
<div class="displaymath">
\begin{equation*}
\mu =
\frac{nM}{N}
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\sigma^2 =
\frac{nM(N-M)(N-n)}{N^2(N-1)}\text{.}
\end{equation*}
</div></article><article class="remark remark-like" id="typo-hyper"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">3.3.28</span><span class="period">.</span>
</h6>
<p id="p-146">Depending on your printing of the recommended resources, there may be a typo in the printed book in the second term of the numerator.</p></article><article class="example example-like" id="ex-hyper"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-ex-hyper"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">3.3.29</span><span class="period">.</span><span class="space"> </span><span class="title">hypergeometric distribution for truck inspections.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-ex-hyper"><article class="example example-like"><p id="p-147">You randomly choose 6 out of 24 trucks for a new fleet of work trucks.  It is known that 4 of the 24 trucks have failed a recent emissions inspection. What is the probability that none of the trucks in your fleet are "polluters"?</p>
<a data-knowl="" class="id-ref solution-knowl original" data-refid="hk-solution-16" id="solution-16"><span class="type">Solution</span></a><div class="hidden-content tex2jax_ignore" id="hk-solution-16"><div class="solution solution-like">
<p id="p-148">Let \(x = \text{# of polluters selected}\text{.}\)  Then, we are looking for</p>
<div class="displaymath">
\begin{equation*}
h(0; 6, 24, 4) = \dfrac{{4 \choose 0}{24-4 \choose{6-0}}}{{24 \choose
6}}\text{.}
\end{equation*}
</div>
<p data-braille="continuation">Notice the denominator is the total number of ways that we can choose \(6\) of \(24\) trucks.  The first term in the numerator is the number of ways that we can choose \(0\) trucks from the collection of \(4\) defective trucks.   The second term in the numerator is the number of ways that we can choose \(6-0\) trucks from the \(24-4\) non-defective trucks.</p>
</div></div></article></div>
<article class="remark remark-like" id="re-mean-var-hyper"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">3.3.30</span><span class="period">.</span>
</h6>
<p id="p-149">Below let \(\theta = \dfrac{M}{N}\) (think through this). The mean and variance of the hypergeometric distribution can be written</p>
<div class="displaymath">
\begin{equation*}
\mu =
\frac{nM}{N} = n\theta
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\sigma^2 =
\frac{nM(N-M)(N-n)}{N^2(N-1)} =
n\theta(1-\theta)\left(\dfrac{N-n}{N-1}\right)\text{.}
\end{equation*}
</div>
<p data-braille="continuation">Above, the term \(\left(\dfrac{N-n}{N-1}\right)\) is called the "finite population correction factor".</p>
<p id="p-150">With this we might interpret the fraction \(\theta = \dfrac{M}{N}\) as the "probability of success", given that we are choosing \(M\) successes from \(N\) objects.</p></article><p id="p-151">As indicated at the beginning of this section, the binomial and hypergeometric are related. The binomial describes a situation where sampling is done with replacement, while the hypergeometric describes a situation where sampling is done without replacement.</p>
<p id="p-152">You will find tools for visualization at the following <a class="external" href="https://buddy.uco.edu/shiny/slaverty/mathstat/HyperBin/" target="_blank">link</a>.</p></section><section class="exercises" id="exercises-8"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.3.8</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-22"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">4.xx.</span>
</h6>
<p id="p-153">xx</p></article></section><section class="subsection" id="sub-discrete-poisson"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">3.3.9</span> <span class="title">Poisson distribution</span>
</h3>
<section class="introduction" id="introduction-13"><p id="p-154">The Poisson distribution describes the occurrence of events taking place at a constant rate in time or over space.  For example, if car accidents take place at a rate of 3 per 100 miles, we would use a Poisson distribution to describe the probabilities of certain numbers of accidents occurring along a known distance of highway.</p>
<p id="p-155">You will find tools for visualization at the following <a class="external" href="https://buddy.uco.edu/shiny/slaverty/mathstat/Poisson/" target="_blank">link</a>.</p></section><article class="definition definition-like" id="def-poiss"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">3.3.31</span><span class="period">.</span><span class="space"> </span><span class="title">Poisson distribution.</span>
</h6>
<p id="p-156">A discrete random variable \(\displaystyle X\) has a <dfn class="terminology">Poisson distribution</dfn> and is referred to as a <dfn class="terminology">Poisson random variable</dfn> if and only if its probability distribution is given by</p>
<div class="displaymath">
\begin{equation*}
p(x; \lambda) = \dfrac{\lambda^x
e^{-\lambda}}{x!}\text{.}
\end{equation*}
</div></article><article class="theorem theorem-like" id="theorem-14"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">3.3.32</span><span class="period">.</span><span class="space"> </span><span class="title">Mean, variance, and MGF of the Poisson distribution.</span>
</h6>
<p id="p-157">The mean and variance of the Poisson distribution are</p>
<div class="displaymath">
\begin{equation*}
\mu =
\lambda
\end{equation*}
</div>
<p data-braille="continuation">and</p>
<div class="displaymath">
\begin{equation*}
\sigma^2 = \lambda\text{.}
\end{equation*}
</div>
<p id="p-158">The moment-generating function of the Poisson distribution is</p>
<div class="displaymath">
\begin{equation*}
M_X(t)
= e^{\lambda(t-1)}
\end{equation*}
</div></article><p id="p-159">The Poisson distribution is derived from the binomial distribution (see pgs. 81-82 in Hogg, Tanis, Zimmerman, 10th ed. for a very good derivation).  Though this is the case, and though the Poisson can be used to approximate the binomial under certain circumstances when the binomial probabilities would be numerically challenging to calculate, many applications of the Poisson distribute have absolutely nothing to do with an underlying binomial process.</p>
<p id="p-160">You will find tools for visualization at the following <a class="external" href="https://buddy.uco.edu/shiny/slaverty/mathstat/BinPoi/" target="_blank">link</a>.</p>
<p id="p-161">As described above, it is generally regarded as safe to use the Poisson as a means of approximating binomial probabilities when \(n \ge
20\) and \(\theta \le 0.05\) or if \(n \gt 100\) and \(\theta
\lt 0.10\text{.}\)  In some cases the approximation will work quite well even in violation of these bounds. Approximation may be slightly less important in modern times than it was in the past due to the ubiquity of computers and software, though the connection is still worth remembering.</p>
<p id="p-162">Beyond its use in approximation, the Poisson distribution has numerous applications for calculation probabilities of events occurring over time or across space.</p></section><section class="exercises" id="exercises-9"><h3 class="heading hide-type">
<span class="type">Exercises</span> <span class="codenumber">3.3.10</span> <span class="title">Exercises</span>
</h3>
<article class="exercise exercise-like" id="exercise-23"><h6 class="heading">
<span class="codenumber">1<span class="period">.</span></span><span class="space"> </span><span class="title">4.xx.</span>
</h6>
<p id="p-163">xx</p></article></section></section></div></main>
</div>
</body>
</html>
