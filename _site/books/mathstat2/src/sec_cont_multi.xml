<?xml version="1.0" encoding="UTF-8" ?>
<section xml:id="sec-multi-cont">
<title>Multivariate continuous random variables</title>

<introduction>
<p> Though we will not specifically look at special jointly-distributed
continuous random variables, we certainly could.  Instead here we will
focus on the fundamental definitions and properties.  This will be handy
for example if we someday encountered a situation with a special
multivariable probability density (e.g., the bivariate normal). </p>
</introduction>

<subsection xml:id="sub-cont-multi-variable">
<title>Multivariate distributions</title>
<introduction>
<p> See Sec. 3.5 through 3.7.</p>
</introduction>

<definition xml:id="def-joint-probability-density-3-6-cont">
<title>joint probability density</title>
<statement>
<p>
If <m>X</m> and <m>Y</m> are continuous random variables, the function
given by <m>f(x,y) = P(X=x, Y=y)</m> for each point in the <m>xy</m>-plane <m>X</m> and <m>Y</m> is called the
<term>joint probability density</term> of <m>X</m> and <m>Y</m>.
</p>
</statement>
</definition>

<example xml:id="ex-joint">
<title>jointly-distributed random variables</title>
<statement>
<p>
Consider  <me>f(x) = \begin{cases}\dfrac{3}{5}\Big(x\Big(x+y\Big)\Big),\amp \quad 0 \lt x \lt 1; 0 \lt y \lt 2\\
0,\amp \quad \text{otherwise}\end{cases}</me> and use it to find <m>P(0 \lt x \lt 0.5, 1 \lt y \lt 1.5)</m>.</p>

<figure xml:id="fig-ex-joint">

	<caption>Graph of a probability density function.</caption>
		<image xml:id="image-ex-joint" source="./images/little_f.png"
width='50%'>
		</image>
</figure>

<hint>
<p>Set up and evaluate a double integral. Here instead of area under a curve, we are looking for the volume under a surface that has been accumulated over the region of interest.
</p></hint>
</statement>
</example>



<theorem xml:id="thm-continuous-th-3-7-cont">
<title>conditions for a joint probability density</title>
<statement>
<p> A bivariate function can serve as a joint probability density for a
pair of continuous random variables <m>X</m> and <m>Y</m> if and only if
its values, <m>f(x, y)</m>, satisfy the conditions
<ol>
<li><p><m>f(x, y) \ge 0</m>  <m>-\infty \lt x, y\lt \infty</m>;</p></li>
<li><p><m> \displaystyle\int_{-\infty}^\infty\int_{-\infty}^\infty f(x, y) \,dy\,dx=1</m>.</p></li>
</ol>
</p>
</statement>
</theorem>

<definition xml:id="def-joint-density-function-3-7-cont">
<title>joint distribution function (continuous)</title>
<statement>
<p>
If <m>X</m> and <m>Y</m> are continuous random variables, the function
given by <me>F(x, y) = P(X \le x, Y \le y) = \int_{-\infty}^y\int_{-\infty}^x f(s, t)
\,ds\,dt\text{ for } -\infty \lt x, y \lt \infty</me> where <m>f(s,
t)</m> is the value of the joint probability density of <m>X</m> and
<m>Y</m> at <m>(s, t)</m>, is called the <term>joint density
function</term> or <term>joint cumulative density</term> of <m>X</m> and
<m>Y</m>.
</p>
</statement>
</definition>

<example xml:id="ex-joint-cdf">
<title>joint CDF</title>
<statement>
<p>
Consider  <me>f(x) = \begin{cases}\dfrac{3}{5}\Big(x\Big(x+y\Big)\Big),\amp \quad 0 \lt x \lt 1; 0 \lt y \lt 2\\
0,\amp \quad \text{otherwise}\end{cases}</me> and use it to find <m>F(x, y) = P(X \lt x, Y \lt y)</m>.</p>

<hint>
<p>Set up and evaluate a double integral to an arbitrary point in the plane.</p></hint>

<solution> Here instead of area under a curve, we are looking for the volume under a surface that has been accumulated up to the arbitrary point <m>(x, y)</m>.
<me>
\begin{aligned}[t]
F(x,y) = 
\begin{cases}
0 \amp \quad x\le 0\text{ or }y\le 0\\
\dfrac{3x^2y^2}{20} + \dfrac{x^3y}{5} \amp \quad 0 \lt x \lt 1, 0 \lt y \lt 2\\
\dfrac{3y^2}{20}+\dfrac{y}{5} \amp \quad  1\le x, 0 \lt y \lt 2\\
\dfrac{12x^2}{20} +\dfrac{6x^3}{15} \amp \quad 0 \lt x \lt 1, 2\le y\\
1 \amp \quad 1\le x, 2\le y
\end{cases}
\end{aligned}
</me>

Notice that for <m>x \ge 1</m> and <m> 0 \lt y \lt 2</m>, the result depends only on the <m>y</m>-value, we are no longer accumulating probability in the <m>x</m>-direction (lower right region in <xref ref="fig-ex-joint-cdf"/>).  Reversing the roles of the variables, the is true for the upper left region where <m>0 \lt x \lt 1</m> and <m>y \ge 2</m>.

<figure xml:id="fig-ex-joint-cdf">

	<caption>Graph of the joint cumulative distribution function.</caption>
		<image xml:id="image-ex-joint-cdf" source="./images/big_F.png"
width='50%'>
		</image>
</figure>
</solution>



</statement>
</example>



<remark xml:id="rmk-jpdf-jcdf">
<statement>
<p>Motivated by <m>f(x) = \dfrac{dF}{dx}</m> we have <me>f(x, y) = \dfrac{\partial^2}{\partial x \partial y}\Big(F(x, y)\Big)</me>
</p>
</statement>
</remark>

<definition xml:id="def-marginal-density-3-10-cont">
<title>marginal density</title>
<statement>
<p>
If <m>X</m> and <m>Y</m> are continuous random variables and <m>f(x,
y)</m> is the value of their joint probability density at <m>(x, y)</m>,
the function given by <me>g(x) = \int f(x, y)\, dy</me> for each
<m>x</m> within the range of <m>X</m> is called the <term>marginal
density</term> of <m>X</m>. Correspondingly, the function given by
<me>h(y) = \int f(x, y)\,dx</me> for each <m>y</m> within the range of
<m>Y</m> is called the <term>marginal density</term> of <m>Y</m>. 
</p>
</statement>
</definition>

<exercise>
<title>marginal densities</title>
<statement>
<p>Calculate the marginal densities of the joint probability distribution used in <xref ref="ex-joint"/> and <xref ref="ex-joint-cdf"/>
</p>
</statement>
</exercise>

<definition xml:id="def-conditional-density-3-10-cont">
<title>conditional density</title>
<statement>
<p>
conditional density
<me>f(x|y) = \dfrac{f(x, y)}{h(y)}, h(y)\ne 0</me>
<me>w(y|x) = \dfrac{f(x, y)}{g(x)}, g(x)\ne 0</me>
</p>
</statement>
</definition>
</subsection>


<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->


<subsection xml:id="sub-product-moments-cont">
<title>Product moments</title>
<p>See section 4.6.</p>


<definition xml:id="def-product-moments-origin-4-7-cont">
<title>product moments about the origin</title>
<statement>
<p> The <term><m>\displaystyle r^\text{th}</m> and <m>\displaystyle
s^\text{th}</m> product moment about the origin</term> of the random
variables
<m>X</m> and <m>Y</m>, denoted by
<m>\displaystyle \mu_{r,s}</m>, is the expected value of
<m>\displaystyle X^rY^s</m>; symbolically 
<me>\mu_{r,s}'=E[X^rY^s] = \int\int x^r y^s\cdot f(x, y)\,dy\,dx</me>
<m>\displaystyle r = 0,1,2, \dots</m> and <m>\displaystyle s = 0,1,2,
\dots</m>
when <m>X</m> and Y are discrete.</p>
</statement>
</definition>

<p>
Special cases of product moments are <m>\displaystyle \mu_{1,0}' =
E[X^1Y^0] = E[X] = \mu_X</m> and <m>\displaystyle \mu_{0,1}' = E[X^0Y^1]
= E[Y] = \mu_Y</m>.
</p>

<definition xml:id="def-product-moments-mean-4-8-cont">
<title>product moments about the mean</title>
<statement>
<p> The <term><m>\displaystyle r^\text{th}</m> and <m>\displaystyle
s^\text{th}</m> product moment about the mean</term> of the random
variables
<m>X</m> and <m>Y</m>, denoted by
<m>\displaystyle \mu_{r,s}'</m>, is the expected value of
<m>\displaystyle (X-\mu_X)^r(Y-\mu_Y)^s</m>; symbolically 
<me>\mu_{r,s}=E[(X-\mu_X)^r(Y-\mu_Y)^s] = \int\int (x-\mu_X)^r
(y-\mu_Y)^s\cdot f(x, y)\,dy\,dx</me> <m>\displaystyle r = 0,1,2, \dots</m> and
<m>\displaystyle s = 0,1,2, \dots</m>
when <m>X</m> and <m>Y</m> are continuous.
</p>
</statement>
</definition>

<p>
Now would be a good time to review <xref ref="def-covariance-4-9"/>, <xref
ref="thm-4-11"/>, <xref ref="thm-4-12"/>, <xref ref="rmrk-4-12"/>, and
<xref ref="thm-4-13"/> all of which apply here as well.
</p>

<p>Independence means covariance is zero, but covariances of zero does
not mean independence.
</p>

</subsection>

<exercises>
<exercise>
<title>Problem 4.41</title>
<statement>
<p>If <m>X</m> and <m>Y</m> have the joint probability distribution
<m>f(x, y) = \dfrac{1}{4}</m> for <m>(-3, -5)</m>,  <m>(-1, -1)</m>, 
<m>(1, 1)</m>,  <m>(3, 5)</m>, find <m>\operatorname{cov}(X, Y)</m>.
</p>
</statement>
</exercise>

<exercise>
<title>Problem 4.45</title>
<statement>
<p>If <m>X</m> and <m>Y</m> have the joint probability distribution
<m>f(-1, 0) = 0</m>,  <m>f(-1, 1) = \dfrac{1}{4}</m>, 
<m>f(0, 0) = \dfrac{1}{6}</m>, <m>f(1, 0) = \dfrac{1}{12}</m>, <m>f(1,
1) = \dfrac{1}{2}</m> show that
<ol>
<li><p> <m>\operatorname{cov}(X, Y) = 0</m>;</p></li>
<li><p> the two random variables are not independent.</p></li>
</ol>
</p>
</statement>
</exercise>

</exercises>


<!-- *********************************** -->
<!-- *********************************** -->
<!-- *********************************** -->

<subsection xml:id="sub-conditional-expectation">
<title>Conditional expectation</title>
<p>See section 4.8.</p>


<definition xml:id="def-conditional-expectation-4-10">
<title>conditional expectation</title>
<statement>
<p>If <m>X</m> is a discrete random variable and
<m>\displaystyle f(x|y)</m> is the value of the conditional probability
distribution of <m>X</m> given <m>\displaystyle Y = y</m>
at <m>X</m>, the <term>conditional expectation</term> of
<m>\displaystyle u(X)</m> given <m>\displaystyle Y = y</m> is 
<me>E[u(X)|y] = \int u(x)\cdot f(x|y)\,dx</me> and the <term>conditional
expectation</term> of <m>\displaystyle v(Y)</m> given <m>\displaystyle X
= x</m> is 
<me>E[v(Y)|x] = \int v(y)\cdot w(y|x)\,dy</me>
</p>
</statement></definition>

<definition xml:id="def-conditional-mean">
<title>conditional mean</title>
<statement>
<p> If <m>X</m> is a discrete random variable and
<m>\displaystyle f(x|y)</m> is the value of the conditional probability
distribution of <m>X</m> given <m>\displaystyle Y = y</m>
at <m>X</m>, the <term>conditional mean</term> of
<m>\displaystyle u(X) = X</m> given <m>\displaystyle Y = y</m> is 
<me>\mu_{X|y} = E[X|y] = \int x\cdot f(x|y)\,dx</me> and the
<term>conditional mean</term> of <m>\displaystyle v(Y) = Y</m> given
<m>\displaystyle X = x</m> is 
<me>\displaystyle \mu_{Y|x} = E[Y|x] = \int y\cdot w(y|x)\,dy</me>
</p>
</statement></definition>

<definition xml:id="def-conditional-variance">
<title>conditional variance</title>
<statement>
<p>If <m>X</m> is a discrete random variable and
<m>\displaystyle f(x|y)</m> is the value of the conditional probability
distribution of <m>X</m> given <m>\displaystyle Y = y</m>
at <m>X</m>, the <term>conditional variance</term> of
<m>X</m> given <m>\displaystyle Y = y</m> is 
<me>\sigma^2_{X|y} = E[(X-\mu_{X|y})^2|y] = E[X^2]-\mu^2_{X|y}</me> and
the <term>conditional expectation</term> of <m>Y</m> given
<m>\displaystyle X = x</m> is 
<me>\displaystyle\sigma^2_{Y|x} = E[(Y-\mu_{Y|x})^2|y] =
E[Y^2]-\mu^2_{Y|x}</me>
</p>
</statement>
</definition>


</subsection>


</section>

